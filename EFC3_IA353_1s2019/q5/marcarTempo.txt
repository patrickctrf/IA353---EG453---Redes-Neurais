_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                80        
_________________________________________________________________
activation_1 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_2 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_3 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 34        
_________________________________________________________________
activation_4 (Activation)    (None, 2)                 0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_2 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_5 (Dense)              (None, 2)                 10        
_________________________________________________________________
activation_5 (Activation)    (None, 2)                 0         
=================================================================
Total params: 10
Trainable params: 10
Non-trainable params: 0
_________________________________________________________________
None
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_3 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_6 (Dense)              (None, 16)                80        
_________________________________________________________________
activation_6 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_7 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_7 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_8 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_9 (Dense)              (None, 2)                 34        
_________________________________________________________________
activation_9 (Activation)    (None, 2)                 0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_4 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_10 (Dense)             (None, 16)                80        
_________________________________________________________________
activation_10 (Activation)   (None, 16)                0         
_________________________________________________________________
dense_11 (Dense)             (None, 16)                272       
_________________________________________________________________
activation_11 (Activation)   (None, 16)                0         
_________________________________________________________________
dense_12 (Dense)             (None, 16)                272       
_________________________________________________________________
activation_12 (Activation)   (None, 16)                0         
_________________________________________________________________
dense_13 (Dense)             (None, 2)                 34        
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
TEMPO:  1561719212.1217005
Training for 2900 steps ...
   39/2900: episode: 1, duration: 1.203s, episode steps: 39, steps per second: 32, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.153 [-0.990, 1.781], loss: 0.410294, mean_absolute_error: 0.561611, mean_q: 0.255399
   60/2900: episode: 2, duration: 0.064s, episode steps: 21, steps per second: 328, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.009 [-1.990, 1.412], loss: 0.190479, mean_absolute_error: 0.573803, mean_q: 0.588316
   97/2900: episode: 3, duration: 0.113s, episode steps: 37, steps per second: 328, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.622 [0.000, 1.000], mean observation: 0.007 [-2.507, 1.798], loss: 0.041500, mean_absolute_error: 0.681297, mean_q: 1.156990
  122/2900: episode: 4, duration: 0.078s, episode steps: 25, steps per second: 319, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-1.022, 2.040], loss: 0.017846, mean_absolute_error: 0.762648, mean_q: 1.429394
  132/2900: episode: 5, duration: 0.032s, episode steps: 10, steps per second: 313, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.108 [-2.030, 1.217], loss: 0.016656, mean_absolute_error: 0.833539, mean_q: 1.606080
  145/2900: episode: 6, duration: 0.041s, episode steps: 13, steps per second: 316, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.123 [-0.785, 1.465], loss: 0.030574, mean_absolute_error: 0.892253, mean_q: 1.700983
  177/2900: episode: 7, duration: 0.106s, episode steps: 32, steps per second: 301, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.738, 1.176], loss: 0.027439, mean_absolute_error: 0.977557, mean_q: 1.891467
  196/2900: episode: 8, duration: 0.064s, episode steps: 19, steps per second: 298, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.065 [-1.375, 2.183], loss: 0.037399, mean_absolute_error: 1.072848, mean_q: 2.070227
  211/2900: episode: 9, duration: 0.048s, episode steps: 15, steps per second: 313, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.071 [-1.725, 1.032], loss: 0.046464, mean_absolute_error: 1.124807, mean_q: 2.123569
  233/2900: episode: 10, duration: 0.067s, episode steps: 22, steps per second: 326, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.066 [-1.338, 0.772], loss: 0.043682, mean_absolute_error: 1.206221, mean_q: 2.328325
  246/2900: episode: 11, duration: 0.043s, episode steps: 13, steps per second: 300, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.118 [-2.881, 1.737], loss: 0.042635, mean_absolute_error: 1.253342, mean_q: 2.397304
  270/2900: episode: 12, duration: 0.078s, episode steps: 24, steps per second: 307, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.031 [-1.769, 1.211], loss: 0.049603, mean_absolute_error: 1.305188, mean_q: 2.479897
  297/2900: episode: 13, duration: 0.086s, episode steps: 27, steps per second: 313, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.370 [0.000, 1.000], mean observation: 0.060 [-1.334, 2.273], loss: 0.058949, mean_absolute_error: 1.415687, mean_q: 2.692725
  337/2900: episode: 14, duration: 0.127s, episode steps: 40, steps per second: 316, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.135 [-1.378, 0.637], loss: 0.069345, mean_absolute_error: 1.539384, mean_q: 2.938713
  360/2900: episode: 15, duration: 0.072s, episode steps: 23, steps per second: 319, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.081 [-2.329, 1.370], loss: 0.066254, mean_absolute_error: 1.646517, mean_q: 3.153914
  377/2900: episode: 16, duration: 0.053s, episode steps: 17, steps per second: 321, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.085 [-0.779, 1.348], loss: 0.068403, mean_absolute_error: 1.750117, mean_q: 3.376561
  413/2900: episode: 17, duration: 0.116s, episode steps: 36, steps per second: 311, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.135 [-2.656, 1.225], loss: 0.093316, mean_absolute_error: 1.849025, mean_q: 3.533205
  427/2900: episode: 18, duration: 0.044s, episode steps: 14, steps per second: 315, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.108 [-1.965, 3.078], loss: 0.078540, mean_absolute_error: 1.951480, mean_q: 3.772302
  449/2900: episode: 19, duration: 0.073s, episode steps: 22, steps per second: 303, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.111 [-0.780, 1.739], loss: 0.090680, mean_absolute_error: 2.048806, mean_q: 3.970209
  461/2900: episode: 20, duration: 0.047s, episode steps: 12, steps per second: 257, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-1.171, 1.899], loss: 0.110836, mean_absolute_error: 2.140228, mean_q: 4.106353
  472/2900: episode: 21, duration: 0.035s, episode steps: 11, steps per second: 316, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.134 [-0.966, 1.777], loss: 0.119258, mean_absolute_error: 2.213527, mean_q: 4.296750
  486/2900: episode: 22, duration: 0.047s, episode steps: 14, steps per second: 295, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.101 [-0.807, 1.542], loss: 0.128491, mean_absolute_error: 2.267986, mean_q: 4.339695
  512/2900: episode: 23, duration: 0.081s, episode steps: 26, steps per second: 320, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.373, 1.032], loss: 0.137032, mean_absolute_error: 2.348148, mean_q: 4.501188
  523/2900: episode: 24, duration: 0.036s, episode steps: 11, steps per second: 307, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.160, 1.837], loss: 0.119405, mean_absolute_error: 2.373686, mean_q: 4.589510
  553/2900: episode: 25, duration: 0.101s, episode steps: 30, steps per second: 297, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-0.990, 1.450], loss: 0.176353, mean_absolute_error: 2.497471, mean_q: 4.810105
  567/2900: episode: 26, duration: 0.046s, episode steps: 14, steps per second: 303, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.105 [-0.572, 1.095], loss: 0.223060, mean_absolute_error: 2.593964, mean_q: 4.966499
  589/2900: episode: 27, duration: 0.079s, episode steps: 22, steps per second: 280, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.040 [-1.191, 1.979], loss: 0.178596, mean_absolute_error: 2.667113, mean_q: 5.157636
  606/2900: episode: 28, duration: 0.053s, episode steps: 17, steps per second: 322, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.106 [-1.266, 0.736], loss: 0.229105, mean_absolute_error: 2.752719, mean_q: 5.297260
  666/2900: episode: 29, duration: 0.190s, episode steps: 60, steps per second: 316, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.168 [-1.148, 2.309], loss: 0.209198, mean_absolute_error: 2.925678, mean_q: 5.668427
  690/2900: episode: 30, duration: 0.075s, episode steps: 24, steps per second: 318, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.046 [-1.677, 1.029], loss: 0.258674, mean_absolute_error: 3.112750, mean_q: 6.031212
  741/2900: episode: 31, duration: 0.169s, episode steps: 51, steps per second: 302, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.022 [-1.204, 1.801], loss: 0.205335, mean_absolute_error: 3.276109, mean_q: 6.376498
  772/2900: episode: 32, duration: 0.108s, episode steps: 31, steps per second: 288, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.065 [-1.621, 1.121], loss: 0.306807, mean_absolute_error: 3.460107, mean_q: 6.680796
  809/2900: episode: 33, duration: 0.120s, episode steps: 37, steps per second: 309, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.065 [-0.757, 1.147], loss: 0.236953, mean_absolute_error: 3.619710, mean_q: 7.088407
  832/2900: episode: 34, duration: 0.072s, episode steps: 23, steps per second: 318, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.041 [-2.419, 1.614], loss: 0.336053, mean_absolute_error: 3.721982, mean_q: 7.216940
  845/2900: episode: 35, duration: 0.042s, episode steps: 13, steps per second: 312, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.101 [-0.614, 1.045], loss: 0.403363, mean_absolute_error: 3.835150, mean_q: 7.408442
  861/2900: episode: 36, duration: 0.057s, episode steps: 16, steps per second: 281, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.109 [-1.289, 0.581], loss: 0.330474, mean_absolute_error: 3.867088, mean_q: 7.480140
  874/2900: episode: 37, duration: 0.045s, episode steps: 13, steps per second: 286, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.393, 1.394], loss: 0.410793, mean_absolute_error: 3.934415, mean_q: 7.590953
  919/2900: episode: 38, duration: 0.139s, episode steps: 45, steps per second: 324, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.064 [-0.974, 0.610], loss: 0.324429, mean_absolute_error: 4.014733, mean_q: 7.817972
  938/2900: episode: 39, duration: 0.065s, episode steps: 19, steps per second: 294, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.083 [-1.311, 0.783], loss: 0.273061, mean_absolute_error: 4.138851, mean_q: 8.102255
  962/2900: episode: 40, duration: 0.079s, episode steps: 24, steps per second: 303, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.794, 1.198], loss: 0.334240, mean_absolute_error: 4.258055, mean_q: 8.325253
 1016/2900: episode: 41, duration: 0.175s, episode steps: 54, steps per second: 308, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.095 [-1.230, 1.333], loss: 0.408596, mean_absolute_error: 4.399060, mean_q: 8.557096
 1030/2900: episode: 42, duration: 0.044s, episode steps: 14, steps per second: 320, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.833, 1.282], loss: 0.355651, mean_absolute_error: 4.545843, mean_q: 8.884073
 1063/2900: episode: 43, duration: 0.112s, episode steps: 33, steps per second: 295, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.023 [-1.136, 0.819], loss: 0.377801, mean_absolute_error: 4.634138, mean_q: 9.103601
 1123/2900: episode: 44, duration: 0.185s, episode steps: 60, steps per second: 325, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.142 [-1.599, 0.763], loss: 0.399556, mean_absolute_error: 4.848400, mean_q: 9.492413
 1177/2900: episode: 45, duration: 0.170s, episode steps: 54, steps per second: 317, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-1.317, 1.126], loss: 0.454578, mean_absolute_error: 5.035510, mean_q: 9.854568
 1195/2900: episode: 46, duration: 0.059s, episode steps: 18, steps per second: 303, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.123 [-0.552, 0.968], loss: 0.345754, mean_absolute_error: 5.206004, mean_q: 10.262394
 1220/2900: episode: 47, duration: 0.081s, episode steps: 25, steps per second: 309, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.026 [-1.432, 0.831], loss: 0.322266, mean_absolute_error: 5.332867, mean_q: 10.530815
 1233/2900: episode: 48, duration: 0.044s, episode steps: 13, steps per second: 293, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.068 [-1.207, 1.830], loss: 0.453841, mean_absolute_error: 5.339791, mean_q: 10.497715
 1252/2900: episode: 49, duration: 0.064s, episode steps: 19, steps per second: 298, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.080 [-0.799, 1.428], loss: 0.359933, mean_absolute_error: 5.416567, mean_q: 10.717914
 1267/2900: episode: 50, duration: 0.049s, episode steps: 15, steps per second: 306, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.123 [-1.876, 0.970], loss: 0.728780, mean_absolute_error: 5.484786, mean_q: 10.688837
 1282/2900: episode: 51, duration: 0.048s, episode steps: 15, steps per second: 311, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.082 [-0.756, 1.393], loss: 0.455707, mean_absolute_error: 5.609832, mean_q: 10.985165
 1299/2900: episode: 52, duration: 0.055s, episode steps: 17, steps per second: 308, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.103 [-1.380, 0.573], loss: 0.386147, mean_absolute_error: 5.577353, mean_q: 10.994821
 1315/2900: episode: 53, duration: 0.052s, episode steps: 16, steps per second: 305, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.052, 0.551], loss: 0.553416, mean_absolute_error: 5.653914, mean_q: 11.125072
 1367/2900: episode: 54, duration: 0.166s, episode steps: 52, steps per second: 314, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.074 [-1.336, 1.277], loss: 0.471135, mean_absolute_error: 5.791492, mean_q: 11.422877
 1398/2900: episode: 55, duration: 0.113s, episode steps: 31, steps per second: 275, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.012 [-1.392, 1.174], loss: 0.515756, mean_absolute_error: 6.021237, mean_q: 11.839259
 1490/2900: episode: 56, duration: 0.291s, episode steps: 92, steps per second: 316, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.242 [-1.957, 2.423], loss: 0.509052, mean_absolute_error: 6.213333, mean_q: 12.293443
 1583/2900: episode: 57, duration: 0.292s, episode steps: 93, steps per second: 319, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.235 [-1.014, 1.305], loss: 0.574058, mean_absolute_error: 6.635026, mean_q: 13.197795
 1730/2900: episode: 58, duration: 0.482s, episode steps: 147, steps per second: 305, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.089 [-1.151, 1.944], loss: 0.637216, mean_absolute_error: 7.152466, mean_q: 14.361691
 1798/2900: episode: 59, duration: 0.223s, episode steps: 68, steps per second: 305, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.087 [-1.430, 1.515], loss: 0.703413, mean_absolute_error: 7.672200, mean_q: 15.477545
 1910/2900: episode: 60, duration: 0.353s, episode steps: 112, steps per second: 317, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.240 [-1.423, 1.583], loss: 0.826770, mean_absolute_error: 8.074517, mean_q: 16.291843
 2080/2900: episode: 61, duration: 0.545s, episode steps: 170, steps per second: 312, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.081 [-1.573, 1.552], loss: 0.784051, mean_absolute_error: 8.756348, mean_q: 17.779127
 2207/2900: episode: 62, duration: 0.413s, episode steps: 127, steps per second: 308, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.180 [-1.358, 1.244], loss: 0.908823, mean_absolute_error: 9.456221, mean_q: 19.252270
 2402/2900: episode: 63, duration: 0.617s, episode steps: 195, steps per second: 316, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.221 [-0.753, 1.869], loss: 1.105615, mean_absolute_error: 10.270044, mean_q: 20.892365
 2589/2900: episode: 64, duration: 0.597s, episode steps: 187, steps per second: 313, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.204 [-1.628, 1.176], loss: 1.223389, mean_absolute_error: 11.216369, mean_q: 22.882315
 2789/2900: episode: 65, duration: 0.642s, episode steps: 200, steps per second: 311, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.142 [-1.491, 1.016], loss: 1.438211, mean_absolute_error: 12.133466, mean_q: 24.758204
done, took 10.443 seconds
TEMPO:  1561719222.564834
Training for 3200 steps ...
   10/3200: episode: 1, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.036, 1.177], loss: --, mean_absolute_error: --, mean_q: --
   36/3200: episode: 2, duration: 0.668s, episode steps: 26, steps per second: 39, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.377, 0.853], loss: 0.418958, mean_absolute_error: 0.498587, mean_q: 0.134956
   63/3200: episode: 3, duration: 0.082s, episode steps: 27, steps per second: 329, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.071 [-0.789, 1.664], loss: 0.246309, mean_absolute_error: 0.526307, mean_q: 0.484435
   75/3200: episode: 4, duration: 0.041s, episode steps: 12, steps per second: 294, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.088 [-1.288, 0.800], loss: 0.135228, mean_absolute_error: 0.592664, mean_q: 0.805629
  104/3200: episode: 5, duration: 0.086s, episode steps: 29, steps per second: 339, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.014 [-1.537, 0.933], loss: 0.089936, mean_absolute_error: 0.659997, mean_q: 1.028115
  120/3200: episode: 6, duration: 0.050s, episode steps: 16, steps per second: 321, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.119 [-0.773, 1.736], loss: 0.032702, mean_absolute_error: 0.714605, mean_q: 1.309490
  145/3200: episode: 7, duration: 0.074s, episode steps: 25, steps per second: 338, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.034 [-1.419, 2.311], loss: 0.034908, mean_absolute_error: 0.782552, mean_q: 1.486060
  156/3200: episode: 8, duration: 0.044s, episode steps: 11, steps per second: 251, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.145 [-1.342, 2.371], loss: 0.041027, mean_absolute_error: 0.866599, mean_q: 1.679047
  183/3200: episode: 9, duration: 0.083s, episode steps: 27, steps per second: 324, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.090 [-0.595, 1.573], loss: 0.076036, mean_absolute_error: 0.953262, mean_q: 1.755706
  198/3200: episode: 10, duration: 0.048s, episode steps: 15, steps per second: 315, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.057 [-1.374, 0.833], loss: 0.062796, mean_absolute_error: 1.020321, mean_q: 1.941917
  209/3200: episode: 11, duration: 0.035s, episode steps: 11, steps per second: 317, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.106 [-1.343, 2.048], loss: 0.065286, mean_absolute_error: 1.063200, mean_q: 1.993404
  245/3200: episode: 12, duration: 0.112s, episode steps: 36, steps per second: 322, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-1.134, 1.899], loss: 0.058482, mean_absolute_error: 1.142170, mean_q: 2.187371
  259/3200: episode: 13, duration: 0.042s, episode steps: 14, steps per second: 331, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.095 [-1.144, 1.980], loss: 0.093250, mean_absolute_error: 1.289349, mean_q: 2.469982
  320/3200: episode: 14, duration: 0.186s, episode steps: 61, steps per second: 327, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.112 [-0.844, 1.790], loss: 0.090836, mean_absolute_error: 1.397358, mean_q: 2.652261
  340/3200: episode: 15, duration: 0.062s, episode steps: 20, steps per second: 324, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.094 [-0.978, 1.847], loss: 0.094076, mean_absolute_error: 1.548295, mean_q: 2.932606
  383/3200: episode: 16, duration: 0.132s, episode steps: 43, steps per second: 325, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.092 [-0.945, 0.637], loss: 0.099193, mean_absolute_error: 1.666738, mean_q: 3.167771
  407/3200: episode: 17, duration: 0.072s, episode steps: 24, steps per second: 332, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.026 [-1.196, 1.845], loss: 0.072662, mean_absolute_error: 1.785199, mean_q: 3.432917
  429/3200: episode: 18, duration: 0.069s, episode steps: 22, steps per second: 319, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.085 [-0.757, 1.651], loss: 0.097562, mean_absolute_error: 1.886640, mean_q: 3.600337
  452/3200: episode: 19, duration: 0.071s, episode steps: 23, steps per second: 324, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.061 [-1.376, 2.050], loss: 0.135128, mean_absolute_error: 1.954211, mean_q: 3.680189
  469/3200: episode: 20, duration: 0.054s, episode steps: 17, steps per second: 313, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.073 [-0.972, 1.612], loss: 0.094397, mean_absolute_error: 2.050302, mean_q: 3.909145
  493/3200: episode: 21, duration: 0.089s, episode steps: 24, steps per second: 269, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.079 [-0.944, 1.809], loss: 0.147146, mean_absolute_error: 2.109651, mean_q: 4.011567
  507/3200: episode: 22, duration: 0.045s, episode steps: 14, steps per second: 310, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.110 [-1.311, 0.752], loss: 0.150825, mean_absolute_error: 2.183289, mean_q: 4.147816
  545/3200: episode: 23, duration: 0.122s, episode steps: 38, steps per second: 312, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.068 [-1.298, 0.397], loss: 0.138530, mean_absolute_error: 2.280437, mean_q: 4.355490
  570/3200: episode: 24, duration: 0.077s, episode steps: 25, steps per second: 327, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.052 [-0.743, 1.208], loss: 0.135130, mean_absolute_error: 2.396324, mean_q: 4.615427
  640/3200: episode: 25, duration: 0.210s, episode steps: 70, steps per second: 333, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.002 [-1.643, 1.213], loss: 0.169066, mean_absolute_error: 2.586761, mean_q: 4.988426
  668/3200: episode: 26, duration: 0.088s, episode steps: 28, steps per second: 317, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.157, 0.575], loss: 0.134605, mean_absolute_error: 2.761669, mean_q: 5.436538
  711/3200: episode: 27, duration: 0.129s, episode steps: 43, steps per second: 334, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.026 [-1.491, 1.155], loss: 0.215423, mean_absolute_error: 2.913761, mean_q: 5.686997
  743/3200: episode: 28, duration: 0.096s, episode steps: 32, steps per second: 333, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.133 [-1.213, 0.760], loss: 0.183341, mean_absolute_error: 3.066769, mean_q: 6.058288
  758/3200: episode: 29, duration: 0.046s, episode steps: 15, steps per second: 327, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.129 [-0.566, 1.326], loss: 0.243165, mean_absolute_error: 3.178408, mean_q: 6.281656
  817/3200: episode: 30, duration: 0.183s, episode steps: 59, steps per second: 323, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.156 [-1.503, 1.356], loss: 0.261128, mean_absolute_error: 3.347094, mean_q: 6.580278
  826/3200: episode: 31, duration: 0.028s, episode steps: 9, steps per second: 317, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.134 [-1.836, 1.160], loss: 0.249629, mean_absolute_error: 3.543898, mean_q: 7.027866
  855/3200: episode: 32, duration: 0.089s, episode steps: 29, steps per second: 324, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.094 [-0.566, 1.254], loss: 0.362440, mean_absolute_error: 3.547475, mean_q: 6.910645
  879/3200: episode: 33, duration: 0.073s, episode steps: 24, steps per second: 330, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-1.517, 0.969], loss: 0.251871, mean_absolute_error: 3.657432, mean_q: 7.224598
  921/3200: episode: 34, duration: 0.133s, episode steps: 42, steps per second: 317, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.873, 1.011], loss: 0.307587, mean_absolute_error: 3.796335, mean_q: 7.494063
  977/3200: episode: 35, duration: 0.175s, episode steps: 56, steps per second: 319, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.185 [-1.171, 0.687], loss: 0.241249, mean_absolute_error: 4.026618, mean_q: 8.006187
  992/3200: episode: 36, duration: 0.047s, episode steps: 15, steps per second: 320, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.100 [-1.026, 0.580], loss: 0.432380, mean_absolute_error: 4.143077, mean_q: 8.180717
 1038/3200: episode: 37, duration: 0.138s, episode steps: 46, steps per second: 333, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.399, 0.616], loss: 0.406383, mean_absolute_error: 4.317586, mean_q: 8.556452
 1097/3200: episode: 38, duration: 0.182s, episode steps: 59, steps per second: 324, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.088 [-1.300, 1.516], loss: 0.250905, mean_absolute_error: 4.500399, mean_q: 9.017553
 1183/3200: episode: 39, duration: 0.281s, episode steps: 86, steps per second: 306, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.183 [-1.322, 0.816], loss: 0.490577, mean_absolute_error: 4.817359, mean_q: 9.575171
 1205/3200: episode: 40, duration: 0.068s, episode steps: 22, steps per second: 323, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.075 [-0.596, 1.149], loss: 0.514674, mean_absolute_error: 5.021838, mean_q: 10.048517
 1292/3200: episode: 41, duration: 0.271s, episode steps: 87, steps per second: 321, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.213 [-1.148, 0.788], loss: 0.461015, mean_absolute_error: 5.273443, mean_q: 10.573929
 1316/3200: episode: 42, duration: 0.074s, episode steps: 24, steps per second: 326, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.021 [-1.306, 0.902], loss: 0.485362, mean_absolute_error: 5.503348, mean_q: 11.019753
 1382/3200: episode: 43, duration: 0.199s, episode steps: 66, steps per second: 331, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.024 [-1.888, 1.395], loss: 0.609135, mean_absolute_error: 5.715723, mean_q: 11.423583
 1502/3200: episode: 44, duration: 0.367s, episode steps: 120, steps per second: 327, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-1.371, 1.337], loss: 0.570617, mean_absolute_error: 6.083657, mean_q: 12.232182
 1634/3200: episode: 45, duration: 0.394s, episode steps: 132, steps per second: 335, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.192 [-1.640, 1.398], loss: 0.617133, mean_absolute_error: 6.617765, mean_q: 13.333559
 1728/3200: episode: 46, duration: 0.282s, episode steps: 94, steps per second: 334, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.139 [-1.555, 1.907], loss: 0.706776, mean_absolute_error: 7.110295, mean_q: 14.324134
 1860/3200: episode: 47, duration: 0.401s, episode steps: 132, steps per second: 330, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.026 [-2.120, 1.758], loss: 0.650938, mean_absolute_error: 7.579319, mean_q: 15.336743
 2060/3200: episode: 48, duration: 0.605s, episode steps: 200, steps per second: 330, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.261 [-1.470, 1.735], loss: 0.570577, mean_absolute_error: 8.292412, mean_q: 16.842337
 2201/3200: episode: 49, duration: 0.434s, episode steps: 141, steps per second: 325, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.177 [-1.231, 1.340], loss: 0.579493, mean_absolute_error: 8.960632, mean_q: 18.263178
 2362/3200: episode: 50, duration: 0.490s, episode steps: 161, steps per second: 328, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.326 [-1.405, 1.994], loss: 0.893981, mean_absolute_error: 9.601273, mean_q: 19.529964
 2531/3200: episode: 51, duration: 0.505s, episode steps: 169, steps per second: 335, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.323 [-1.115, 2.224], loss: 0.926407, mean_absolute_error: 10.223015, mean_q: 20.802643
 2731/3200: episode: 52, duration: 0.612s, episode steps: 200, steps per second: 327, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.151 [-1.093, 1.448], loss: 1.013672, mean_absolute_error: 10.973313, mean_q: 22.321562
 2926/3200: episode: 53, duration: 0.598s, episode steps: 195, steps per second: 326, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.110 [-1.155, 1.226], loss: 1.078567, mean_absolute_error: 11.804296, mean_q: 24.067869
 3126/3200: episode: 54, duration: 0.609s, episode steps: 200, steps per second: 328, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.217 [-1.095, 1.720], loss: 1.192176, mean_absolute_error: 12.801095, mean_q: 25.974838
done, took 10.454 seconds
TEMPO:  1561719233.0191643
Training for 11000 steps ...
    19/11000: episode: 1, duration: 0.122s, episode steps: 19, steps per second: 155, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.059 [-2.147, 1.357], mean_best_reward: --
    35/11000: episode: 2, duration: 0.015s, episode steps: 16, steps per second: 1037, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.055 [-1.013, 1.428], mean_best_reward: --
    56/11000: episode: 3, duration: 0.021s, episode steps: 21, steps per second: 1012, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.060 [-1.900, 1.179], mean_best_reward: --
    72/11000: episode: 4, duration: 0.016s, episode steps: 16, steps per second: 993, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.084 [-1.186, 2.031], mean_best_reward: --
   100/11000: episode: 5, duration: 0.026s, episode steps: 28, steps per second: 1078, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.121 [-1.531, 0.458], mean_best_reward: --
   117/11000: episode: 6, duration: 0.016s, episode steps: 17, steps per second: 1089, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.083 [-1.750, 1.021], mean_best_reward: --
   132/11000: episode: 7, duration: 0.014s, episode steps: 15, steps per second: 1060, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.060 [-2.666, 1.802], mean_best_reward: --
   150/11000: episode: 8, duration: 0.017s, episode steps: 18, steps per second: 1078, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.559, 0.962], mean_best_reward: --
   161/11000: episode: 9, duration: 0.011s, episode steps: 11, steps per second: 971, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-1.009, 1.773], mean_best_reward: --
   181/11000: episode: 10, duration: 0.019s, episode steps: 20, steps per second: 1055, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.096 [-1.486, 0.775], mean_best_reward: --
   193/11000: episode: 11, duration: 0.013s, episode steps: 12, steps per second: 952, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.106 [-3.078, 1.970], mean_best_reward: --
   204/11000: episode: 12, duration: 0.011s, episode steps: 11, steps per second: 981, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.807, 1.792], mean_best_reward: --
   237/11000: episode: 13, duration: 0.029s, episode steps: 33, steps per second: 1151, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.124 [-1.202, 0.804], mean_best_reward: --
   255/11000: episode: 14, duration: 0.016s, episode steps: 18, steps per second: 1099, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.078 [-0.795, 1.134], mean_best_reward: --
   269/11000: episode: 15, duration: 0.015s, episode steps: 14, steps per second: 965, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.113 [-3.080, 1.913], mean_best_reward: --
   283/11000: episode: 16, duration: 0.015s, episode steps: 14, steps per second: 931, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.114 [-2.144, 1.189], mean_best_reward: --
   307/11000: episode: 17, duration: 0.024s, episode steps: 24, steps per second: 1019, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.045 [-0.958, 1.593], mean_best_reward: --
   324/11000: episode: 18, duration: 0.017s, episode steps: 17, steps per second: 976, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.827, 1.242], mean_best_reward: --
   344/11000: episode: 19, duration: 0.019s, episode steps: 20, steps per second: 1033, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.101 [-0.766, 0.401], mean_best_reward: --
   371/11000: episode: 20, duration: 0.026s, episode steps: 27, steps per second: 1057, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.089 [-1.024, 2.122], mean_best_reward: --
   396/11000: episode: 21, duration: 0.023s, episode steps: 25, steps per second: 1100, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.033 [-0.998, 1.694], mean_best_reward: --
   413/11000: episode: 22, duration: 0.017s, episode steps: 17, steps per second: 1003, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.123 [-0.945, 1.869], mean_best_reward: --
   454/11000: episode: 23, duration: 0.036s, episode steps: 41, steps per second: 1137, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.089 [-0.608, 1.575], mean_best_reward: --
   466/11000: episode: 24, duration: 0.012s, episode steps: 12, steps per second: 1005, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.772, 1.490], mean_best_reward: --
   479/11000: episode: 25, duration: 0.013s, episode steps: 13, steps per second: 969, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.077 [-1.581, 2.352], mean_best_reward: --
   488/11000: episode: 26, duration: 0.009s, episode steps: 9, steps per second: 957, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.143 [-1.899, 1.141], mean_best_reward: --
   498/11000: episode: 27, duration: 0.010s, episode steps: 10, steps per second: 959, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.981, 1.669], mean_best_reward: --
   508/11000: episode: 28, duration: 0.010s, episode steps: 10, steps per second: 1007, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.948, 2.980], mean_best_reward: --
   529/11000: episode: 29, duration: 0.022s, episode steps: 21, steps per second: 974, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.062 [-0.789, 1.358], mean_best_reward: --
   545/11000: episode: 30, duration: 0.014s, episode steps: 16, steps per second: 1131, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.075 [-1.887, 1.177], mean_best_reward: --
   565/11000: episode: 31, duration: 0.023s, episode steps: 20, steps per second: 879, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.613, 0.753], mean_best_reward: --
   575/11000: episode: 32, duration: 0.011s, episode steps: 10, steps per second: 894, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.522, 1.574], mean_best_reward: --
   590/11000: episode: 33, duration: 0.015s, episode steps: 15, steps per second: 976, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.073 [-1.197, 1.787], mean_best_reward: --
   610/11000: episode: 34, duration: 0.019s, episode steps: 20, steps per second: 1077, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.091 [-0.787, 1.587], mean_best_reward: --
   630/11000: episode: 35, duration: 0.020s, episode steps: 20, steps per second: 1018, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.045 [-1.966, 2.917], mean_best_reward: --
   645/11000: episode: 36, duration: 0.015s, episode steps: 15, steps per second: 996, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.867 [0.000, 1.000], mean observation: -0.096 [-3.220, 2.123], mean_best_reward: --
   685/11000: episode: 37, duration: 0.037s, episode steps: 40, steps per second: 1094, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.590, 1.158], mean_best_reward: --
   699/11000: episode: 38, duration: 0.014s, episode steps: 14, steps per second: 1031, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.118 [-0.769, 1.622], mean_best_reward: --
   714/11000: episode: 39, duration: 0.015s, episode steps: 15, steps per second: 1006, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.089 [-1.319, 2.175], mean_best_reward: --
   728/11000: episode: 40, duration: 0.014s, episode steps: 14, steps per second: 1009, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.118 [-0.796, 1.342], mean_best_reward: --
   738/11000: episode: 41, duration: 0.009s, episode steps: 10, steps per second: 1151, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.549, 2.617], mean_best_reward: --
   750/11000: episode: 42, duration: 0.013s, episode steps: 12, steps per second: 946, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.115 [-2.559, 1.529], mean_best_reward: --
   763/11000: episode: 43, duration: 0.013s, episode steps: 13, steps per second: 984, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.082 [-2.210, 1.395], mean_best_reward: --
   774/11000: episode: 44, duration: 0.010s, episode steps: 11, steps per second: 1049, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.760, 1.734], mean_best_reward: --
   797/11000: episode: 45, duration: 0.022s, episode steps: 23, steps per second: 1053, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.075 [-0.650, 1.246], mean_best_reward: --
   824/11000: episode: 46, duration: 0.025s, episode steps: 27, steps per second: 1062, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.047 [-1.336, 0.799], mean_best_reward: --
   840/11000: episode: 47, duration: 0.016s, episode steps: 16, steps per second: 1021, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.074 [-1.931, 1.219], mean_best_reward: --
   859/11000: episode: 48, duration: 0.017s, episode steps: 19, steps per second: 1109, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.119 [-2.062, 0.992], mean_best_reward: --
   868/11000: episode: 49, duration: 0.009s, episode steps: 9, steps per second: 987, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.127 [-2.449, 1.585], mean_best_reward: --
   877/11000: episode: 50, duration: 0.009s, episode steps: 9, steps per second: 955, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.784, 1.723], mean_best_reward: --
   887/11000: episode: 51, duration: 0.010s, episode steps: 10, steps per second: 1048, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.531, 2.571], mean_best_reward: --
   937/11000: episode: 52, duration: 0.045s, episode steps: 50, steps per second: 1119, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.009 [-1.319, 2.001], mean_best_reward: --
   948/11000: episode: 53, duration: 0.011s, episode steps: 11, steps per second: 969, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.089 [-1.031, 1.576], mean_best_reward: --
   977/11000: episode: 54, duration: 0.026s, episode steps: 29, steps per second: 1130, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.063 [-1.502, 0.638], mean_best_reward: --
   991/11000: episode: 55, duration: 0.013s, episode steps: 14, steps per second: 1064, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.075 [-1.197, 1.846], mean_best_reward: --
  1003/11000: episode: 56, duration: 0.011s, episode steps: 12, steps per second: 1089, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.123 [-1.353, 2.169], mean_best_reward: --
  1012/11000: episode: 57, duration: 0.010s, episode steps: 9, steps per second: 912, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.131 [-1.189, 1.872], mean_best_reward: --
  1025/11000: episode: 58, duration: 0.013s, episode steps: 13, steps per second: 972, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.099 [-2.231, 1.369], mean_best_reward: --
  1045/11000: episode: 59, duration: 0.018s, episode steps: 20, steps per second: 1086, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.095 [-1.673, 0.816], mean_best_reward: --
  1055/11000: episode: 60, duration: 0.009s, episode steps: 10, steps per second: 1076, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.212, 1.398], mean_best_reward: --
  1069/11000: episode: 61, duration: 0.015s, episode steps: 14, steps per second: 962, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.122 [-2.107, 1.149], mean_best_reward: --
  1084/11000: episode: 62, duration: 0.014s, episode steps: 15, steps per second: 1083, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.098 [-2.388, 1.362], mean_best_reward: --
  1109/11000: episode: 63, duration: 0.025s, episode steps: 25, steps per second: 999, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.105 [-1.257, 0.552], mean_best_reward: --
  1119/11000: episode: 64, duration: 0.010s, episode steps: 10, steps per second: 980, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.110 [-2.135, 1.385], mean_best_reward: --
  1130/11000: episode: 65, duration: 0.012s, episode steps: 11, steps per second: 946, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.378, 2.311], mean_best_reward: --
  1139/11000: episode: 66, duration: 0.010s, episode steps: 9, steps per second: 910, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.151 [-2.330, 1.382], mean_best_reward: --
  1167/11000: episode: 67, duration: 0.025s, episode steps: 28, steps per second: 1105, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.068 [-1.239, 0.739], mean_best_reward: --
  1211/11000: episode: 68, duration: 0.041s, episode steps: 44, steps per second: 1080, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.095 [-0.664, 1.128], mean_best_reward: --
  1222/11000: episode: 69, duration: 0.011s, episode steps: 11, steps per second: 1022, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.132 [-2.824, 1.785], mean_best_reward: --
  1234/11000: episode: 70, duration: 0.012s, episode steps: 12, steps per second: 1000, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.116 [-1.180, 2.046], mean_best_reward: --
  1247/11000: episode: 71, duration: 0.012s, episode steps: 13, steps per second: 1117, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.122 [-1.752, 2.835], mean_best_reward: --
  1259/11000: episode: 72, duration: 0.012s, episode steps: 12, steps per second: 983, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.736, 0.969], mean_best_reward: --
  1294/11000: episode: 73, duration: 0.030s, episode steps: 35, steps per second: 1181, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.686 [0.000, 1.000], mean observation: 0.030 [-3.055, 2.476], mean_best_reward: --
  1312/11000: episode: 74, duration: 0.017s, episode steps: 18, steps per second: 1043, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.086 [-1.671, 0.848], mean_best_reward: --
  1329/11000: episode: 75, duration: 0.016s, episode steps: 17, steps per second: 1060, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.065 [-2.754, 1.806], mean_best_reward: --
  1339/11000: episode: 76, duration: 0.011s, episode steps: 10, steps per second: 927, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.201, 1.952], mean_best_reward: --
  1351/11000: episode: 77, duration: 0.011s, episode steps: 12, steps per second: 1053, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.114 [-2.721, 1.731], mean_best_reward: --
  1364/11000: episode: 78, duration: 0.013s, episode steps: 13, steps per second: 980, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.118 [-1.351, 2.349], mean_best_reward: --
  1387/11000: episode: 79, duration: 0.021s, episode steps: 23, steps per second: 1086, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.060 [-1.330, 0.757], mean_best_reward: --
  1402/11000: episode: 80, duration: 0.014s, episode steps: 15, steps per second: 1056, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.072 [-2.208, 1.400], mean_best_reward: --
  1425/11000: episode: 81, duration: 0.021s, episode steps: 23, steps per second: 1086, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.065 [-2.310, 1.327], mean_best_reward: --
  1491/11000: episode: 82, duration: 0.057s, episode steps: 66, steps per second: 1167, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.039 [-1.103, 1.304], mean_best_reward: --
  1514/11000: episode: 83, duration: 0.022s, episode steps: 23, steps per second: 1044, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.021 [-1.870, 1.222], mean_best_reward: --
  1533/11000: episode: 84, duration: 0.017s, episode steps: 19, steps per second: 1111, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.087 [-0.818, 1.550], mean_best_reward: --
  1545/11000: episode: 85, duration: 0.012s, episode steps: 12, steps per second: 1027, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.077 [-2.447, 1.611], mean_best_reward: --
  1561/11000: episode: 86, duration: 0.016s, episode steps: 16, steps per second: 1001, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.107 [-0.803, 1.306], mean_best_reward: --
  1576/11000: episode: 87, duration: 0.014s, episode steps: 15, steps per second: 1037, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.107 [-1.768, 2.841], mean_best_reward: --
  1587/11000: episode: 88, duration: 0.016s, episode steps: 11, steps per second: 671, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.715, 2.741], mean_best_reward: --
  1606/11000: episode: 89, duration: 0.018s, episode steps: 19, steps per second: 1047, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.057 [-2.328, 1.512], mean_best_reward: --
  1618/11000: episode: 90, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.116 [-2.580, 1.533], mean_best_reward: --
  1634/11000: episode: 91, duration: 0.015s, episode steps: 16, steps per second: 1079, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.103 [-2.005, 1.139], mean_best_reward: --
  1693/11000: episode: 92, duration: 0.053s, episode steps: 59, steps per second: 1120, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.084 [-1.152, 0.867], mean_best_reward: --
  1708/11000: episode: 93, duration: 0.015s, episode steps: 15, steps per second: 1033, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.357, 2.197], mean_best_reward: --
  1724/11000: episode: 94, duration: 0.015s, episode steps: 16, steps per second: 1038, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.082 [-1.352, 0.818], mean_best_reward: --
  1735/11000: episode: 95, duration: 0.011s, episode steps: 11, steps per second: 975, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.113 [-2.190, 1.371], mean_best_reward: --
  1745/11000: episode: 96, duration: 0.009s, episode steps: 10, steps per second: 1059, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.198, 1.954], mean_best_reward: --
  1761/11000: episode: 97, duration: 0.015s, episode steps: 16, steps per second: 1043, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-1.220, 1.864], mean_best_reward: --
  1778/11000: episode: 98, duration: 0.016s, episode steps: 17, steps per second: 1037, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.107 [-1.643, 0.815], mean_best_reward: --
  1803/11000: episode: 99, duration: 0.024s, episode steps: 25, steps per second: 1046, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.022 [-1.356, 1.999], mean_best_reward: --
  1840/11000: episode: 100, duration: 0.034s, episode steps: 37, steps per second: 1087, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.061 [-0.745, 1.492], mean_best_reward: --
  1855/11000: episode: 101, duration: 0.014s, episode steps: 15, steps per second: 1062, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.616, 1.278], mean_best_reward: --
  1875/11000: episode: 102, duration: 0.019s, episode steps: 20, steps per second: 1053, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.078 [-1.132, 1.853], mean_best_reward: --
  1884/11000: episode: 103, duration: 0.010s, episode steps: 9, steps per second: 946, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.782, 2.784], mean_best_reward: --
  1898/11000: episode: 104, duration: 0.013s, episode steps: 14, steps per second: 1043, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.094 [-1.542, 0.844], mean_best_reward: --
  1908/11000: episode: 105, duration: 0.011s, episode steps: 10, steps per second: 937, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-0.985, 1.691], mean_best_reward: --
  1923/11000: episode: 106, duration: 0.015s, episode steps: 15, steps per second: 1025, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.889, 1.020], mean_best_reward: --
  1938/11000: episode: 107, duration: 0.014s, episode steps: 15, steps per second: 1107, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.094 [-2.866, 1.749], mean_best_reward: --
  2030/11000: episode: 108, duration: 0.082s, episode steps: 92, steps per second: 1125, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.336 [-2.171, 0.802], mean_best_reward: --
  2062/11000: episode: 109, duration: 0.030s, episode steps: 32, steps per second: 1058, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.051 [-1.073, 1.127], mean_best_reward: --
  2073/11000: episode: 110, duration: 0.011s, episode steps: 11, steps per second: 1025, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.107 [-1.612, 2.365], mean_best_reward: --
  2131/11000: episode: 111, duration: 0.052s, episode steps: 58, steps per second: 1109, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.002 [-1.776, 1.019], mean_best_reward: --
  2153/11000: episode: 112, duration: 0.022s, episode steps: 22, steps per second: 991, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.048 [-0.823, 1.184], mean_best_reward: --
  2195/11000: episode: 113, duration: 0.039s, episode steps: 42, steps per second: 1064, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.023 [-1.268, 0.772], mean_best_reward: --
  2224/11000: episode: 114, duration: 0.026s, episode steps: 29, steps per second: 1095, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.019 [-1.339, 1.869], mean_best_reward: --
  2236/11000: episode: 115, duration: 0.012s, episode steps: 12, steps per second: 1003, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.107 [-2.165, 1.321], mean_best_reward: --
  2272/11000: episode: 116, duration: 0.031s, episode steps: 36, steps per second: 1171, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.162 [-0.560, 1.087], mean_best_reward: --
  2300/11000: episode: 117, duration: 0.024s, episode steps: 28, steps per second: 1189, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: 0.042 [-2.722, 2.260], mean_best_reward: --
  2323/11000: episode: 118, duration: 0.019s, episode steps: 23, steps per second: 1184, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.045 [-1.401, 2.121], mean_best_reward: --
  2337/11000: episode: 119, duration: 0.013s, episode steps: 14, steps per second: 1108, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.109 [-1.657, 0.803], mean_best_reward: --
  2347/11000: episode: 120, duration: 0.009s, episode steps: 10, steps per second: 1136, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.108 [-2.523, 1.561], mean_best_reward: --
  2370/11000: episode: 121, duration: 0.021s, episode steps: 23, steps per second: 1070, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.103 [-1.940, 0.947], mean_best_reward: --
  2391/11000: episode: 122, duration: 0.019s, episode steps: 21, steps per second: 1130, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.543, 0.822], mean_best_reward: --
  2401/11000: episode: 123, duration: 0.009s, episode steps: 10, steps per second: 1103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.157 [-2.426, 1.526], mean_best_reward: --
  2418/11000: episode: 124, duration: 0.016s, episode steps: 17, steps per second: 1095, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.094 [-0.639, 1.063], mean_best_reward: --
  2451/11000: episode: 125, duration: 0.031s, episode steps: 33, steps per second: 1078, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.128 [-0.630, 1.216], mean_best_reward: --
  2467/11000: episode: 126, duration: 0.015s, episode steps: 16, steps per second: 1047, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.086 [-1.160, 2.039], mean_best_reward: --
  2497/11000: episode: 127, duration: 0.028s, episode steps: 30, steps per second: 1077, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.009 [-1.759, 1.128], mean_best_reward: --
  2513/11000: episode: 128, duration: 0.015s, episode steps: 16, steps per second: 1033, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.049 [-1.602, 2.445], mean_best_reward: --
  2522/11000: episode: 129, duration: 0.009s, episode steps: 9, steps per second: 971, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.117 [-2.272, 1.395], mean_best_reward: --
  2565/11000: episode: 130, duration: 0.039s, episode steps: 43, steps per second: 1104, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.163 [-1.157, 1.849], mean_best_reward: --
  2586/11000: episode: 131, duration: 0.019s, episode steps: 21, steps per second: 1092, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.070 [-1.889, 1.170], mean_best_reward: --
  2604/11000: episode: 132, duration: 0.018s, episode steps: 18, steps per second: 1028, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.056 [-1.216, 0.831], mean_best_reward: --
  2620/11000: episode: 133, duration: 0.016s, episode steps: 16, steps per second: 970, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.060 [-2.395, 1.557], mean_best_reward: --
  2633/11000: episode: 134, duration: 0.012s, episode steps: 13, steps per second: 1045, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.101 [-0.943, 1.494], mean_best_reward: --
  2643/11000: episode: 135, duration: 0.017s, episode steps: 10, steps per second: 606, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-1.997, 1.211], mean_best_reward: --
  2654/11000: episode: 136, duration: 0.011s, episode steps: 11, steps per second: 986, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-3.294, 2.183], mean_best_reward: --
  2682/11000: episode: 137, duration: 0.025s, episode steps: 28, steps per second: 1112, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.028 [-1.422, 0.992], mean_best_reward: --
  2717/11000: episode: 138, duration: 0.032s, episode steps: 35, steps per second: 1107, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.053 [-1.945, 0.986], mean_best_reward: --
  2765/11000: episode: 139, duration: 0.042s, episode steps: 48, steps per second: 1156, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.174 [-1.093, 0.814], mean_best_reward: --
  2779/11000: episode: 140, duration: 0.014s, episode steps: 14, steps per second: 1009, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.105 [-1.195, 1.979], mean_best_reward: --
  2795/11000: episode: 141, duration: 0.014s, episode steps: 16, steps per second: 1139, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-2.532, 1.540], mean_best_reward: --
  2816/11000: episode: 142, duration: 0.021s, episode steps: 21, steps per second: 1020, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.079 [-0.798, 1.458], mean_best_reward: --
  2833/11000: episode: 143, duration: 0.017s, episode steps: 17, steps per second: 1014, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.085 [-2.351, 1.357], mean_best_reward: --
  2855/11000: episode: 144, duration: 0.021s, episode steps: 22, steps per second: 1035, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.055 [-2.167, 1.321], mean_best_reward: --
  2868/11000: episode: 145, duration: 0.013s, episode steps: 13, steps per second: 992, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.102 [-0.982, 1.710], mean_best_reward: --
  2888/11000: episode: 146, duration: 0.020s, episode steps: 20, steps per second: 1009, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.113 [-1.693, 0.785], mean_best_reward: --
  2898/11000: episode: 147, duration: 0.010s, episode steps: 10, steps per second: 1045, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.057, 1.947], mean_best_reward: --
  2909/11000: episode: 148, duration: 0.012s, episode steps: 11, steps per second: 952, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.124 [-1.953, 1.156], mean_best_reward: --
  2917/11000: episode: 149, duration: 0.008s, episode steps: 8, steps per second: 1039, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.568, 2.535], mean_best_reward: --
  2929/11000: episode: 150, duration: 0.012s, episode steps: 12, steps per second: 999, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.101 [-2.520, 1.562], mean_best_reward: --
  2941/11000: episode: 151, duration: 0.012s, episode steps: 12, steps per second: 1012, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.160, 2.065], mean_best_reward: 45.500000
  2958/11000: episode: 152, duration: 0.016s, episode steps: 17, steps per second: 1044, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.097 [-1.337, 0.750], mean_best_reward: --
  3046/11000: episode: 153, duration: 0.082s, episode steps: 88, steps per second: 1071, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-1.234, 1.118], mean_best_reward: --
  3061/11000: episode: 154, duration: 0.015s, episode steps: 15, steps per second: 985, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.963, 1.873], mean_best_reward: --
  3073/11000: episode: 155, duration: 0.012s, episode steps: 12, steps per second: 986, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.126 [-2.200, 1.336], mean_best_reward: --
  3105/11000: episode: 156, duration: 0.028s, episode steps: 32, steps per second: 1155, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.079 [-0.843, 1.752], mean_best_reward: --
  3129/11000: episode: 157, duration: 0.023s, episode steps: 24, steps per second: 1059, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.039 [-1.933, 1.220], mean_best_reward: --
  3158/11000: episode: 158, duration: 0.027s, episode steps: 29, steps per second: 1056, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.089 [-1.358, 0.931], mean_best_reward: --
  3168/11000: episode: 159, duration: 0.009s, episode steps: 10, steps per second: 1098, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.138 [-1.356, 2.210], mean_best_reward: --
  3211/11000: episode: 160, duration: 0.037s, episode steps: 43, steps per second: 1170, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: -0.113 [-1.674, 1.869], mean_best_reward: --
  3233/11000: episode: 161, duration: 0.021s, episode steps: 22, steps per second: 1059, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.049 [-1.857, 1.156], mean_best_reward: --
  3260/11000: episode: 162, duration: 0.026s, episode steps: 27, steps per second: 1037, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.041 [-1.220, 1.810], mean_best_reward: --
  3278/11000: episode: 163, duration: 0.016s, episode steps: 18, steps per second: 1158, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.069 [-0.990, 1.496], mean_best_reward: --
  3297/11000: episode: 164, duration: 0.017s, episode steps: 19, steps per second: 1123, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.085 [-1.745, 2.809], mean_best_reward: --
  3314/11000: episode: 165, duration: 0.016s, episode steps: 17, steps per second: 1050, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.071 [-1.728, 1.132], mean_best_reward: --
  3348/11000: episode: 166, duration: 0.032s, episode steps: 34, steps per second: 1060, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.608, 1.048], mean_best_reward: --
  3397/11000: episode: 167, duration: 0.045s, episode steps: 49, steps per second: 1095, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.009 [-1.429, 0.957], mean_best_reward: --
  3445/11000: episode: 168, duration: 0.041s, episode steps: 48, steps per second: 1177, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.090 [-0.563, 1.486], mean_best_reward: --
  3507/11000: episode: 169, duration: 0.066s, episode steps: 62, steps per second: 944, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.153 [-1.706, 1.256], mean_best_reward: --
  3562/11000: episode: 170, duration: 0.052s, episode steps: 55, steps per second: 1053, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.003 [-1.317, 1.955], mean_best_reward: --
  3575/11000: episode: 171, duration: 0.011s, episode steps: 13, steps per second: 1176, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.118 [-1.992, 1.159], mean_best_reward: --
  3621/11000: episode: 172, duration: 0.042s, episode steps: 46, steps per second: 1099, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.746, 1.165], mean_best_reward: --
  3632/11000: episode: 173, duration: 0.012s, episode steps: 11, steps per second: 920, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.110 [-0.804, 1.294], mean_best_reward: --
  3676/11000: episode: 174, duration: 0.040s, episode steps: 44, steps per second: 1091, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.070 [-0.866, 2.041], mean_best_reward: --
  3727/11000: episode: 175, duration: 0.050s, episode steps: 51, steps per second: 1019, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.134 [-0.950, 0.554], mean_best_reward: --
  3747/11000: episode: 176, duration: 0.019s, episode steps: 20, steps per second: 1061, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.073 [-1.179, 0.775], mean_best_reward: --
  3773/11000: episode: 177, duration: 0.027s, episode steps: 26, steps per second: 976, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-1.120, 0.638], mean_best_reward: --
  3787/11000: episode: 178, duration: 0.014s, episode steps: 14, steps per second: 1005, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.091 [-2.547, 1.604], mean_best_reward: --
  3842/11000: episode: 179, duration: 0.050s, episode steps: 55, steps per second: 1106, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.121 [-1.471, 1.206], mean_best_reward: --
  3852/11000: episode: 180, duration: 0.009s, episode steps: 10, steps per second: 1135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.324, 2.189], mean_best_reward: --
  3866/11000: episode: 181, duration: 0.013s, episode steps: 14, steps per second: 1043, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-0.999, 1.695], mean_best_reward: --
  3879/11000: episode: 182, duration: 0.013s, episode steps: 13, steps per second: 1033, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.140 [-1.307, 0.736], mean_best_reward: --
  3915/11000: episode: 183, duration: 0.034s, episode steps: 36, steps per second: 1074, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.050 [-1.401, 0.649], mean_best_reward: --
  3928/11000: episode: 184, duration: 0.014s, episode steps: 13, steps per second: 904, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.095 [-1.199, 1.989], mean_best_reward: --
  3941/11000: episode: 185, duration: 0.013s, episode steps: 13, steps per second: 993, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.125 [-1.338, 2.286], mean_best_reward: --
  3969/11000: episode: 186, duration: 0.027s, episode steps: 28, steps per second: 1047, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-0.999, 0.639], mean_best_reward: --
  3982/11000: episode: 187, duration: 0.011s, episode steps: 13, steps per second: 1168, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.089 [-1.383, 2.039], mean_best_reward: --
  4004/11000: episode: 188, duration: 0.022s, episode steps: 22, steps per second: 986, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.048 [-1.251, 0.633], mean_best_reward: --
  4015/11000: episode: 189, duration: 0.011s, episode steps: 11, steps per second: 963, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.111 [-1.770, 2.756], mean_best_reward: --
  4057/11000: episode: 190, duration: 0.039s, episode steps: 42, steps per second: 1070, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.128 [-1.142, 0.902], mean_best_reward: --
  4071/11000: episode: 191, duration: 0.013s, episode steps: 14, steps per second: 1098, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-2.297, 1.419], mean_best_reward: --
  4091/11000: episode: 192, duration: 0.019s, episode steps: 20, steps per second: 1037, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.093 [-2.198, 1.185], mean_best_reward: --
  4110/11000: episode: 193, duration: 0.018s, episode steps: 19, steps per second: 1037, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.089 [-0.571, 1.360], mean_best_reward: --
  4148/11000: episode: 194, duration: 0.035s, episode steps: 38, steps per second: 1083, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.563, 1.083], mean_best_reward: --
  4166/11000: episode: 195, duration: 0.018s, episode steps: 18, steps per second: 1010, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.114 [-0.560, 1.269], mean_best_reward: --
  4203/11000: episode: 196, duration: 0.035s, episode steps: 37, steps per second: 1070, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.622 [0.000, 1.000], mean observation: -0.045 [-2.530, 1.714], mean_best_reward: --
  4216/11000: episode: 197, duration: 0.011s, episode steps: 13, steps per second: 1134, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-0.787, 1.429], mean_best_reward: --
  4279/11000: episode: 198, duration: 0.054s, episode steps: 63, steps per second: 1173, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.038 [-1.207, 0.998], mean_best_reward: --
  4302/11000: episode: 199, duration: 0.023s, episode steps: 23, steps per second: 1018, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.052 [-0.774, 1.167], mean_best_reward: --
  4329/11000: episode: 200, duration: 0.025s, episode steps: 27, steps per second: 1080, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.081 [-0.760, 1.530], mean_best_reward: --
  4341/11000: episode: 201, duration: 0.012s, episode steps: 12, steps per second: 995, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.128 [-1.135, 1.839], mean_best_reward: 71.500000
  4359/11000: episode: 202, duration: 0.017s, episode steps: 18, steps per second: 1056, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.078 [-0.785, 1.417], mean_best_reward: --
  4392/11000: episode: 203, duration: 0.028s, episode steps: 33, steps per second: 1183, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.095 [-0.419, 1.209], mean_best_reward: --
  4411/11000: episode: 204, duration: 0.017s, episode steps: 19, steps per second: 1095, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.103 [-0.623, 0.987], mean_best_reward: --
  4432/11000: episode: 205, duration: 0.019s, episode steps: 21, steps per second: 1095, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.082 [-0.746, 1.503], mean_best_reward: --
  4458/11000: episode: 206, duration: 0.022s, episode steps: 26, steps per second: 1194, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.125 [-0.589, 1.550], mean_best_reward: --
  4504/11000: episode: 207, duration: 0.041s, episode steps: 46, steps per second: 1125, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.090 [-1.719, 1.726], mean_best_reward: --
  4529/11000: episode: 208, duration: 0.025s, episode steps: 25, steps per second: 1000, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.093 [-0.587, 1.131], mean_best_reward: --
  4542/11000: episode: 209, duration: 0.013s, episode steps: 13, steps per second: 985, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.086 [-0.838, 1.444], mean_best_reward: --
  4595/11000: episode: 210, duration: 0.049s, episode steps: 53, steps per second: 1081, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.097 [-0.570, 1.140], mean_best_reward: --
  4620/11000: episode: 211, duration: 0.021s, episode steps: 25, steps per second: 1171, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.030 [-0.998, 1.390], mean_best_reward: --
  4687/11000: episode: 212, duration: 0.062s, episode steps: 67, steps per second: 1082, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.126 [-1.073, 1.752], mean_best_reward: --
  4723/11000: episode: 213, duration: 0.034s, episode steps: 36, steps per second: 1074, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.808, 1.044], mean_best_reward: --
  4781/11000: episode: 214, duration: 0.056s, episode steps: 58, steps per second: 1044, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.768, 1.067], mean_best_reward: --
  4807/11000: episode: 215, duration: 0.024s, episode steps: 26, steps per second: 1062, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.101 [-0.638, 0.972], mean_best_reward: --
  4825/11000: episode: 216, duration: 0.017s, episode steps: 18, steps per second: 1043, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.121 [-0.787, 1.828], mean_best_reward: --
  4840/11000: episode: 217, duration: 0.016s, episode steps: 15, steps per second: 950, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.111 [-0.794, 1.447], mean_best_reward: --
  4939/11000: episode: 218, duration: 0.087s, episode steps: 99, steps per second: 1137, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.138 [-0.967, 1.323], mean_best_reward: --
  4960/11000: episode: 219, duration: 0.018s, episode steps: 21, steps per second: 1150, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.134 [-0.745, 1.261], mean_best_reward: --
  4998/11000: episode: 220, duration: 0.033s, episode steps: 38, steps per second: 1148, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.608, 1.361], mean_best_reward: --
  5079/11000: episode: 221, duration: 0.072s, episode steps: 81, steps per second: 1132, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.114 [-2.104, 2.008], mean_best_reward: --
  5152/11000: episode: 222, duration: 0.065s, episode steps: 73, steps per second: 1116, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.029 [-1.014, 1.707], mean_best_reward: --
  5198/11000: episode: 223, duration: 0.041s, episode steps: 46, steps per second: 1111, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.069 [-0.443, 0.930], mean_best_reward: --
  5220/11000: episode: 224, duration: 0.020s, episode steps: 22, steps per second: 1083, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.085 [-0.582, 1.078], mean_best_reward: --
  5243/11000: episode: 225, duration: 0.020s, episode steps: 23, steps per second: 1138, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.063 [-1.398, 0.830], mean_best_reward: --
  5340/11000: episode: 226, duration: 0.082s, episode steps: 97, steps per second: 1176, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.198 [-1.158, 1.525], mean_best_reward: --
  5414/11000: episode: 227, duration: 0.064s, episode steps: 74, steps per second: 1161, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: 0.245 [-2.266, 2.694], mean_best_reward: --
  5454/11000: episode: 228, duration: 0.039s, episode steps: 40, steps per second: 1031, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.573, 1.001], mean_best_reward: --
  5526/11000: episode: 229, duration: 0.064s, episode steps: 72, steps per second: 1125, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.224 [-2.109, 1.553], mean_best_reward: --
  5556/11000: episode: 230, duration: 0.028s, episode steps: 30, steps per second: 1087, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.068 [-0.790, 1.373], mean_best_reward: --
  5594/11000: episode: 231, duration: 0.034s, episode steps: 38, steps per second: 1108, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.015 [-0.985, 1.631], mean_best_reward: --
  5612/11000: episode: 232, duration: 0.016s, episode steps: 18, steps per second: 1161, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.811, 1.329], mean_best_reward: --
  5636/11000: episode: 233, duration: 0.021s, episode steps: 24, steps per second: 1152, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.094 [-0.357, 0.839], mean_best_reward: --
  5693/11000: episode: 234, duration: 0.054s, episode steps: 57, steps per second: 1060, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.061 [-0.975, 1.270], mean_best_reward: --
  5716/11000: episode: 235, duration: 0.020s, episode steps: 23, steps per second: 1132, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.049 [-0.576, 1.055], mean_best_reward: --
  5745/11000: episode: 236, duration: 0.028s, episode steps: 29, steps per second: 1051, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.097 [-0.545, 1.001], mean_best_reward: --
  5769/11000: episode: 237, duration: 0.022s, episode steps: 24, steps per second: 1081, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.051 [-0.644, 1.323], mean_best_reward: --
  5798/11000: episode: 238, duration: 0.027s, episode steps: 29, steps per second: 1080, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.088 [-0.374, 0.860], mean_best_reward: --
  5815/11000: episode: 239, duration: 0.015s, episode steps: 17, steps per second: 1101, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.194, 0.643], mean_best_reward: --
  5832/11000: episode: 240, duration: 0.015s, episode steps: 17, steps per second: 1098, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.087 [-1.477, 0.766], mean_best_reward: --
  5866/11000: episode: 241, duration: 0.036s, episode steps: 34, steps per second: 955, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.026 [-0.628, 1.008], mean_best_reward: --
  5988/11000: episode: 242, duration: 0.112s, episode steps: 122, steps per second: 1087, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.173 [-2.469, 1.784], mean_best_reward: --
  6068/11000: episode: 243, duration: 0.073s, episode steps: 80, steps per second: 1097, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.931, 1.323], mean_best_reward: --
  6121/11000: episode: 244, duration: 0.048s, episode steps: 53, steps per second: 1098, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.007 [-1.450, 0.837], mean_best_reward: --
  6208/11000: episode: 245, duration: 0.078s, episode steps: 87, steps per second: 1122, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.270 [-0.595, 1.850], mean_best_reward: --
  6243/11000: episode: 246, duration: 0.034s, episode steps: 35, steps per second: 1039, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.105 [-0.551, 1.117], mean_best_reward: --
  6272/11000: episode: 247, duration: 0.026s, episode steps: 29, steps per second: 1104, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.057 [-0.779, 1.121], mean_best_reward: --
  6307/11000: episode: 248, duration: 0.030s, episode steps: 35, steps per second: 1166, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.117 [-1.124, 1.142], mean_best_reward: --
  6365/11000: episode: 249, duration: 0.052s, episode steps: 58, steps per second: 1116, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.176 [-0.808, 1.177], mean_best_reward: --
  6419/11000: episode: 250, duration: 0.046s, episode steps: 54, steps per second: 1169, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.049 [-0.394, 0.907], mean_best_reward: --
  6434/11000: episode: 251, duration: 0.014s, episode steps: 15, steps per second: 1049, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.090 [-0.798, 1.309], mean_best_reward: 65.000000
  6452/11000: episode: 252, duration: 0.017s, episode steps: 18, steps per second: 1030, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-1.165, 2.158], mean_best_reward: --
  6516/11000: episode: 253, duration: 0.058s, episode steps: 64, steps per second: 1102, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.118 [-0.540, 1.311], mean_best_reward: --
  6586/11000: episode: 254, duration: 0.061s, episode steps: 70, steps per second: 1139, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.082 [-0.998, 0.987], mean_best_reward: --
  6613/11000: episode: 255, duration: 0.025s, episode steps: 27, steps per second: 1083, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.101 [-1.190, 0.736], mean_best_reward: --
  6631/11000: episode: 256, duration: 0.016s, episode steps: 18, steps per second: 1110, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.098 [-0.456, 1.198], mean_best_reward: --
  6648/11000: episode: 257, duration: 0.015s, episode steps: 17, steps per second: 1110, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.112 [-0.606, 1.145], mean_best_reward: --
  6735/11000: episode: 258, duration: 0.075s, episode steps: 87, steps per second: 1165, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.563 [0.000, 1.000], mean observation: 0.270 [-1.754, 2.082], mean_best_reward: --
  6758/11000: episode: 259, duration: 0.020s, episode steps: 23, steps per second: 1145, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.051 [-2.517, 1.562], mean_best_reward: --
  6843/11000: episode: 260, duration: 0.080s, episode steps: 85, steps per second: 1060, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.164 [-1.478, 1.293], mean_best_reward: --
  6904/11000: episode: 261, duration: 0.058s, episode steps: 61, steps per second: 1056, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.171 [-1.680, 0.500], mean_best_reward: --
  6921/11000: episode: 262, duration: 0.017s, episode steps: 17, steps per second: 1026, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.127 [-0.971, 0.580], mean_best_reward: --
  6939/11000: episode: 263, duration: 0.021s, episode steps: 18, steps per second: 838, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.637, 1.072], mean_best_reward: --
  6977/11000: episode: 264, duration: 0.034s, episode steps: 38, steps per second: 1134, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.128 [-1.658, 0.620], mean_best_reward: --
  7048/11000: episode: 265, duration: 0.062s, episode steps: 71, steps per second: 1144, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.563 [0.000, 1.000], mean observation: 0.116 [-2.119, 1.953], mean_best_reward: --
  7059/11000: episode: 266, duration: 0.010s, episode steps: 11, steps per second: 1112, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.101 [-0.789, 1.285], mean_best_reward: --
  7074/11000: episode: 267, duration: 0.013s, episode steps: 15, steps per second: 1121, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.101 [-0.650, 1.253], mean_best_reward: --
  7152/11000: episode: 268, duration: 0.070s, episode steps: 78, steps per second: 1120, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.206 [-2.289, 1.833], mean_best_reward: --
  7222/11000: episode: 269, duration: 0.064s, episode steps: 70, steps per second: 1091, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.135 [-1.742, 1.520], mean_best_reward: --
  7241/11000: episode: 270, duration: 0.019s, episode steps: 19, steps per second: 1026, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.071 [-1.599, 1.008], mean_best_reward: --
  7258/11000: episode: 271, duration: 0.017s, episode steps: 17, steps per second: 1018, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.068 [-1.737, 1.203], mean_best_reward: --
  7292/11000: episode: 272, duration: 0.032s, episode steps: 34, steps per second: 1068, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.197, 0.961], mean_best_reward: --
  7345/11000: episode: 273, duration: 0.049s, episode steps: 53, steps per second: 1085, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.054 [-1.390, 1.824], mean_best_reward: --
  7398/11000: episode: 274, duration: 0.047s, episode steps: 53, steps per second: 1125, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.106 [-1.530, 1.366], mean_best_reward: --
  7436/11000: episode: 275, duration: 0.034s, episode steps: 38, steps per second: 1116, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.148 [-0.672, 1.051], mean_best_reward: --
  7459/11000: episode: 276, duration: 0.020s, episode steps: 23, steps per second: 1173, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.095 [-0.566, 1.266], mean_best_reward: --
  7507/11000: episode: 277, duration: 0.041s, episode steps: 48, steps per second: 1177, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.132 [-1.098, 0.603], mean_best_reward: --
  7616/11000: episode: 278, duration: 0.094s, episode steps: 109, steps per second: 1163, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.251 [-2.856, 2.130], mean_best_reward: --
  7631/11000: episode: 279, duration: 0.015s, episode steps: 15, steps per second: 1006, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.094 [-0.771, 1.361], mean_best_reward: --
  7645/11000: episode: 280, duration: 0.013s, episode steps: 14, steps per second: 1066, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.830, 1.285], mean_best_reward: --
  7679/11000: episode: 281, duration: 0.030s, episode steps: 34, steps per second: 1148, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.092 [-0.426, 0.867], mean_best_reward: --
  7744/11000: episode: 282, duration: 0.058s, episode steps: 65, steps per second: 1129, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: 0.239 [-1.890, 2.644], mean_best_reward: --
  7763/11000: episode: 283, duration: 0.018s, episode steps: 19, steps per second: 1041, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.092 [-0.807, 1.198], mean_best_reward: --
  7782/11000: episode: 284, duration: 0.017s, episode steps: 19, steps per second: 1147, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.084 [-0.770, 1.228], mean_best_reward: --
  7800/11000: episode: 285, duration: 0.017s, episode steps: 18, steps per second: 1065, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.191, 0.599], mean_best_reward: --
  7821/11000: episode: 286, duration: 0.020s, episode steps: 21, steps per second: 1036, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.044 [-1.009, 1.466], mean_best_reward: --
  7837/11000: episode: 287, duration: 0.015s, episode steps: 16, steps per second: 1042, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.093 [-1.605, 0.787], mean_best_reward: --
  7873/11000: episode: 288, duration: 0.032s, episode steps: 36, steps per second: 1123, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.033 [-1.073, 1.534], mean_best_reward: --
  7890/11000: episode: 289, duration: 0.016s, episode steps: 17, steps per second: 1046, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.044 [-1.197, 1.715], mean_best_reward: --
  7920/11000: episode: 290, duration: 0.027s, episode steps: 30, steps per second: 1100, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.739, 1.198], mean_best_reward: --
  7942/11000: episode: 291, duration: 0.021s, episode steps: 22, steps per second: 1038, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.062 [-1.601, 0.825], mean_best_reward: --
  7998/11000: episode: 292, duration: 0.053s, episode steps: 56, steps per second: 1060, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.162 [-0.803, 0.986], mean_best_reward: --
  8041/11000: episode: 293, duration: 0.044s, episode steps: 43, steps per second: 973, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.093 [-0.553, 0.824], mean_best_reward: --
  8063/11000: episode: 294, duration: 0.021s, episode steps: 22, steps per second: 1054, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.092 [-0.606, 1.036], mean_best_reward: --
  8079/11000: episode: 295, duration: 0.016s, episode steps: 16, steps per second: 1027, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.078 [-1.345, 2.053], mean_best_reward: --
  8169/11000: episode: 296, duration: 0.081s, episode steps: 90, steps per second: 1111, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.220 [-1.258, 1.895], mean_best_reward: --
  8225/11000: episode: 297, duration: 0.052s, episode steps: 56, steps per second: 1080, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.045 [-1.827, 1.560], mean_best_reward: --
  8250/11000: episode: 298, duration: 0.023s, episode steps: 25, steps per second: 1070, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.099 [-0.561, 1.409], mean_best_reward: --
  8306/11000: episode: 299, duration: 0.054s, episode steps: 56, steps per second: 1031, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.067 [-2.005, 0.967], mean_best_reward: --
  8348/11000: episode: 300, duration: 0.038s, episode steps: 42, steps per second: 1102, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.042 [-1.516, 2.063], mean_best_reward: --
  8367/11000: episode: 301, duration: 0.018s, episode steps: 19, steps per second: 1032, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.105 [-0.749, 1.432], mean_best_reward: 87.000000
  8382/11000: episode: 302, duration: 0.015s, episode steps: 15, steps per second: 1022, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.116 [-1.426, 0.786], mean_best_reward: --
  8411/11000: episode: 303, duration: 0.028s, episode steps: 29, steps per second: 1021, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.089 [-0.590, 0.937], mean_best_reward: --
  8429/11000: episode: 304, duration: 0.017s, episode steps: 18, steps per second: 1070, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.086 [-1.437, 0.789], mean_best_reward: --
  8461/11000: episode: 305, duration: 0.029s, episode steps: 32, steps per second: 1090, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.123 [-0.949, 0.572], mean_best_reward: --
  8492/11000: episode: 306, duration: 0.027s, episode steps: 31, steps per second: 1147, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.049 [-0.766, 1.429], mean_best_reward: --
  8578/11000: episode: 307, duration: 0.078s, episode steps: 86, steps per second: 1109, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.322 [-1.338, 2.067], mean_best_reward: --
  8719/11000: episode: 308, duration: 0.121s, episode steps: 141, steps per second: 1163, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.085 [-2.608, 1.757], mean_best_reward: --
  8742/11000: episode: 309, duration: 0.020s, episode steps: 23, steps per second: 1129, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.097 [-1.394, 0.602], mean_best_reward: --
  8771/11000: episode: 310, duration: 0.027s, episode steps: 29, steps per second: 1082, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.083 [-1.192, 0.772], mean_best_reward: --
  8795/11000: episode: 311, duration: 0.023s, episode steps: 24, steps per second: 1049, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.741, 1.182], mean_best_reward: --
  8877/11000: episode: 312, duration: 0.075s, episode steps: 82, steps per second: 1093, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.039 [-0.795, 0.984], mean_best_reward: --
  8927/11000: episode: 313, duration: 0.057s, episode steps: 50, steps per second: 880, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.147 [-0.959, 0.528], mean_best_reward: --
  8983/11000: episode: 314, duration: 0.049s, episode steps: 56, steps per second: 1148, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.058 [-0.789, 1.254], mean_best_reward: --
  9006/11000: episode: 315, duration: 0.022s, episode steps: 23, steps per second: 1047, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.079 [-1.819, 0.994], mean_best_reward: --
  9066/11000: episode: 316, duration: 0.064s, episode steps: 60, steps per second: 939, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.154 [-1.580, 0.589], mean_best_reward: --
  9148/11000: episode: 317, duration: 0.120s, episode steps: 82, steps per second: 682, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.169 [-1.348, 1.729], mean_best_reward: --
  9163/11000: episode: 318, duration: 0.018s, episode steps: 15, steps per second: 833, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.092 [-1.054, 0.626], mean_best_reward: --
  9224/11000: episode: 319, duration: 0.065s, episode steps: 61, steps per second: 940, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.103 [-1.877, 0.790], mean_best_reward: --
  9238/11000: episode: 320, duration: 0.016s, episode steps: 14, steps per second: 852, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.750, 1.304], mean_best_reward: --
  9269/11000: episode: 321, duration: 0.042s, episode steps: 31, steps per second: 734, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.038 [-0.640, 1.333], mean_best_reward: --
  9333/11000: episode: 322, duration: 0.063s, episode steps: 64, steps per second: 1016, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.084 [-0.397, 0.812], mean_best_reward: --
  9463/11000: episode: 323, duration: 0.124s, episode steps: 130, steps per second: 1049, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.092 [-1.099, 1.103], mean_best_reward: --
  9552/11000: episode: 324, duration: 0.098s, episode steps: 89, steps per second: 904, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.158 [-1.280, 1.490], mean_best_reward: --
  9632/11000: episode: 325, duration: 0.081s, episode steps: 80, steps per second: 983, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.047 [-0.734, 1.215], mean_best_reward: --
  9657/11000: episode: 326, duration: 0.025s, episode steps: 25, steps per second: 997, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.087 [-2.033, 1.020], mean_best_reward: --
  9674/11000: episode: 327, duration: 0.019s, episode steps: 17, steps per second: 912, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.081 [-0.628, 1.354], mean_best_reward: --
  9739/11000: episode: 328, duration: 0.105s, episode steps: 65, steps per second: 622, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.041 [-1.441, 0.629], mean_best_reward: --
  9793/11000: episode: 329, duration: 0.141s, episode steps: 54, steps per second: 382, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.367, 1.001], mean_best_reward: --
  9825/11000: episode: 330, duration: 0.096s, episode steps: 32, steps per second: 333, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.068, 0.610], mean_best_reward: --
  9878/11000: episode: 331, duration: 0.085s, episode steps: 53, steps per second: 623, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.017 [-0.811, 1.309], mean_best_reward: --
  9933/11000: episode: 332, duration: 0.086s, episode steps: 55, steps per second: 641, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.010 [-1.304, 0.818], mean_best_reward: --
  9943/11000: episode: 333, duration: 0.017s, episode steps: 10, steps per second: 586, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.104 [-1.228, 2.051], mean_best_reward: --
  9967/11000: episode: 334, duration: 0.039s, episode steps: 24, steps per second: 620, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.063 [-1.213, 0.783], mean_best_reward: --
  9982/11000: episode: 335, duration: 0.025s, episode steps: 15, steps per second: 606, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.090 [-0.991, 1.531], mean_best_reward: --
 10020/11000: episode: 336, duration: 0.061s, episode steps: 38, steps per second: 623, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.923, 0.551], mean_best_reward: --
 10069/11000: episode: 337, duration: 0.068s, episode steps: 49, steps per second: 723, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.175 [-0.630, 1.114], mean_best_reward: --
 10129/11000: episode: 338, duration: 0.085s, episode steps: 60, steps per second: 706, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.115 [-1.163, 0.977], mean_best_reward: --
 10167/11000: episode: 339, duration: 0.052s, episode steps: 38, steps per second: 735, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.011 [-0.815, 1.202], mean_best_reward: --
 10223/11000: episode: 340, duration: 0.069s, episode steps: 56, steps per second: 814, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.085 [-0.798, 1.281], mean_best_reward: --
 10243/11000: episode: 341, duration: 0.020s, episode steps: 20, steps per second: 1007, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.126, 0.797], mean_best_reward: --
 10355/11000: episode: 342, duration: 0.106s, episode steps: 112, steps per second: 1055, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.174 [-1.537, 1.129], mean_best_reward: --
 10403/11000: episode: 343, duration: 0.048s, episode steps: 48, steps per second: 1010, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.139 [-0.549, 1.005], mean_best_reward: --
 10418/11000: episode: 344, duration: 0.020s, episode steps: 15, steps per second: 741, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.957, 1.627], mean_best_reward: --
 10462/11000: episode: 345, duration: 0.057s, episode steps: 44, steps per second: 772, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.106 [-0.610, 1.063], mean_best_reward: --
 10487/11000: episode: 346, duration: 0.031s, episode steps: 25, steps per second: 810, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.097 [-1.407, 1.008], mean_best_reward: --
 10519/11000: episode: 347, duration: 0.044s, episode steps: 32, steps per second: 720, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.041 [-0.769, 1.222], mean_best_reward: --
 10688/11000: episode: 348, duration: 0.205s, episode steps: 169, steps per second: 826, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.010 [-1.136, 1.580], mean_best_reward: --
 10764/11000: episode: 349, duration: 0.074s, episode steps: 76, steps per second: 1021, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.167 [-1.130, 0.748], mean_best_reward: --
 10789/11000: episode: 350, duration: 0.026s, episode steps: 25, steps per second: 977, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.998, 2.038], mean_best_reward: --
 10861/11000: episode: 351, duration: 0.072s, episode steps: 72, steps per second: 1006, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.084 [-0.837, 1.222], mean_best_reward: 133.000000
 10954/11000: episode: 352, duration: 0.090s, episode steps: 93, steps per second: 1029, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.043 [-0.766, 0.996], mean_best_reward: --
done, took 11.082 seconds
TEMPO:  1561719244.1017532
Training for 4500 steps ...
   18/4500: episode: 1, duration: 0.692s, episode steps: 18, steps per second: 26, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.076 [-1.212, 2.046], loss: 0.496989, mean_absolute_error: 0.542384, mean_q: 0.100202
   53/4500: episode: 2, duration: 0.077s, episode steps: 35, steps per second: 457, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.055 [-1.005, 1.858], loss: 0.506430, mean_absolute_error: 0.560013, mean_q: 0.158132
   75/4500: episode: 3, duration: 0.048s, episode steps: 22, steps per second: 455, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.073 [-0.964, 1.591], loss: 0.499735, mean_absolute_error: 0.641412, mean_q: 0.369078
   88/4500: episode: 4, duration: 0.028s, episode steps: 13, steps per second: 458, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.116 [-1.957, 1.173], loss: 0.476008, mean_absolute_error: 0.621915, mean_q: 0.348572
  118/4500: episode: 5, duration: 0.067s, episode steps: 30, steps per second: 448, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.038 [-2.182, 1.216], loss: 0.517172, mean_absolute_error: 0.740061, mean_q: 0.643857
  142/4500: episode: 6, duration: 0.053s, episode steps: 24, steps per second: 453, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.026 [-2.011, 1.228], loss: 0.616589, mean_absolute_error: 0.985241, mean_q: 1.257123
  152/4500: episode: 7, duration: 0.022s, episode steps: 10, steps per second: 457, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-3.084, 1.945], loss: 1.389943, mean_absolute_error: 1.419262, mean_q: 2.453297
  174/4500: episode: 8, duration: 0.047s, episode steps: 22, steps per second: 472, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.040 [-2.373, 1.532], loss: 1.084554, mean_absolute_error: 1.388869, mean_q: 2.209792
  220/4500: episode: 9, duration: 0.096s, episode steps: 46, steps per second: 478, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: 0.023 [-2.985, 2.270], loss: 1.345702, mean_absolute_error: 1.796791, mean_q: 3.052637
  230/4500: episode: 10, duration: 0.020s, episode steps: 10, steps per second: 488, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.058, 2.002], loss: 3.683193, mean_absolute_error: 2.729890, mean_q: 4.928355
  244/4500: episode: 11, duration: 0.030s, episode steps: 14, steps per second: 468, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.084 [-2.478, 1.556], loss: 2.138135, mean_absolute_error: 2.442288, mean_q: 4.310149
  253/4500: episode: 12, duration: 0.020s, episode steps: 9, steps per second: 457, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.841, 1.758], loss: 3.598316, mean_absolute_error: 2.814701, mean_q: 4.988115
  268/4500: episode: 13, duration: 0.033s, episode steps: 15, steps per second: 451, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.083 [-2.204, 1.375], loss: 1.897519, mean_absolute_error: 2.673293, mean_q: 4.758659
  286/4500: episode: 14, duration: 0.039s, episode steps: 18, steps per second: 465, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.052 [-1.888, 1.192], loss: 1.497356, mean_absolute_error: 2.809110, mean_q: 4.941686
  300/4500: episode: 15, duration: 0.032s, episode steps: 14, steps per second: 442, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.077 [-2.081, 1.232], loss: 1.944808, mean_absolute_error: 2.869693, mean_q: 4.886681
  314/4500: episode: 16, duration: 0.031s, episode steps: 14, steps per second: 448, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.109 [-1.988, 1.160], loss: 2.013618, mean_absolute_error: 3.273567, mean_q: 5.682393
  335/4500: episode: 17, duration: 0.045s, episode steps: 21, steps per second: 466, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.044 [-2.526, 1.704], loss: 2.877578, mean_absolute_error: 3.804940, mean_q: 6.828654
  346/4500: episode: 18, duration: 0.024s, episode steps: 11, steps per second: 460, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.747, 1.749], loss: 5.189485, mean_absolute_error: 4.119013, mean_q: 7.220039
  374/4500: episode: 19, duration: 0.064s, episode steps: 28, steps per second: 435, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.108 [-0.957, 0.639], loss: 1.150746, mean_absolute_error: 3.446218, mean_q: 6.106255
  384/4500: episode: 20, duration: 0.023s, episode steps: 10, steps per second: 439, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.126 [-2.983, 1.987], loss: 7.297979, mean_absolute_error: 5.345070, mean_q: 8.816777
  398/4500: episode: 21, duration: 0.030s, episode steps: 14, steps per second: 463, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.104 [-1.230, 2.146], loss: 5.892426, mean_absolute_error: 5.388244, mean_q: 9.049566
  411/4500: episode: 22, duration: 0.036s, episode steps: 13, steps per second: 361, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.128 [-1.328, 2.288], loss: 6.447906, mean_absolute_error: 5.489862, mean_q: 9.222829
  437/4500: episode: 23, duration: 0.055s, episode steps: 26, steps per second: 471, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.082 [-0.442, 0.927], loss: 1.872161, mean_absolute_error: 4.926858, mean_q: 8.748105
  459/4500: episode: 24, duration: 0.048s, episode steps: 22, steps per second: 458, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.082 [-0.634, 1.220], loss: 3.087102, mean_absolute_error: 5.380878, mean_q: 9.414657
  472/4500: episode: 25, duration: 0.028s, episode steps: 13, steps per second: 460, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.104 [-1.376, 2.234], loss: 8.529479, mean_absolute_error: 6.275652, mean_q: 10.716118
  485/4500: episode: 26, duration: 0.028s, episode steps: 13, steps per second: 473, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.117 [-1.386, 2.400], loss: 9.359895, mean_absolute_error: 6.414627, mean_q: 11.077554
  508/4500: episode: 27, duration: 0.049s, episode steps: 23, steps per second: 466, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.063 [-1.143, 1.767], loss: 4.312467, mean_absolute_error: 5.907690, mean_q: 10.520307
  538/4500: episode: 28, duration: 0.064s, episode steps: 30, steps per second: 467, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.036 [-2.294, 3.295], loss: 7.102851, mean_absolute_error: 6.215389, mean_q: 11.376109
  547/4500: episode: 29, duration: 0.020s, episode steps: 9, steps per second: 450, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.736, 2.746], loss: 17.121619, mean_absolute_error: 7.533269, mean_q: 12.965614
  559/4500: episode: 30, duration: 0.025s, episode steps: 12, steps per second: 475, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.094 [-1.988, 2.955], loss: 12.839380, mean_absolute_error: 7.022212, mean_q: 12.297821
  569/4500: episode: 31, duration: 0.022s, episode steps: 10, steps per second: 452, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.141 [-1.345, 2.201], loss: 10.965472, mean_absolute_error: 6.689246, mean_q: 11.479361
  602/4500: episode: 32, duration: 0.076s, episode steps: 33, steps per second: 436, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: 0.027 [-1.337, 2.086], loss: 3.398011, mean_absolute_error: 5.828892, mean_q: 10.493364
  631/4500: episode: 33, duration: 0.065s, episode steps: 29, steps per second: 446, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.008 [-2.362, 1.566], loss: 4.752796, mean_absolute_error: 6.443331, mean_q: 11.456094
  649/4500: episode: 34, duration: 0.039s, episode steps: 18, steps per second: 458, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.228, 2.086], loss: 5.200290, mean_absolute_error: 6.548139, mean_q: 11.788613
  666/4500: episode: 35, duration: 0.038s, episode steps: 17, steps per second: 448, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.081 [-1.337, 2.256], loss: 6.009101, mean_absolute_error: 6.616674, mean_q: 11.855539
  678/4500: episode: 36, duration: 0.028s, episode steps: 12, steps per second: 431, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.083 [-1.209, 1.889], loss: 8.454752, mean_absolute_error: 7.105453, mean_q: 12.387616
  689/4500: episode: 37, duration: 0.024s, episode steps: 11, steps per second: 450, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.109 [-1.389, 2.144], loss: 10.044380, mean_absolute_error: 7.468662, mean_q: 13.203283
  698/4500: episode: 38, duration: 0.019s, episode steps: 9, steps per second: 462, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.770, 2.837], loss: 13.912951, mean_absolute_error: 7.198035, mean_q: 12.735392
  714/4500: episode: 39, duration: 0.037s, episode steps: 16, steps per second: 431, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.546, 2.699], loss: 7.245972, mean_absolute_error: 6.661341, mean_q: 12.246053
  726/4500: episode: 40, duration: 0.027s, episode steps: 12, steps per second: 439, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.123 [-1.231, 2.142], loss: 7.080085, mean_absolute_error: 6.431800, mean_q: 11.730145
  736/4500: episode: 41, duration: 0.022s, episode steps: 10, steps per second: 461, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.944, 3.095], loss: 11.633967, mean_absolute_error: 6.955872, mean_q: 12.490114
  746/4500: episode: 42, duration: 0.022s, episode steps: 10, steps per second: 462, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.574, 2.472], loss: 8.828206, mean_absolute_error: 6.816157, mean_q: 12.341596
  763/4500: episode: 43, duration: 0.040s, episode steps: 17, steps per second: 424, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.041 [-1.806, 2.635], loss: 5.805598, mean_absolute_error: 6.285862, mean_q: 11.662672
  773/4500: episode: 44, duration: 0.023s, episode steps: 10, steps per second: 442, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.977, 3.060], loss: 10.262483, mean_absolute_error: 6.623899, mean_q: 12.142441
  784/4500: episode: 45, duration: 0.030s, episode steps: 11, steps per second: 368, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.101 [-1.605, 2.419], loss: 7.932295, mean_absolute_error: 6.321865, mean_q: 11.575947
  798/4500: episode: 46, duration: 0.032s, episode steps: 14, steps per second: 431, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.067 [-1.612, 2.501], loss: 5.717077, mean_absolute_error: 6.166669, mean_q: 11.313565
  819/4500: episode: 47, duration: 0.046s, episode steps: 21, steps per second: 452, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.077 [-0.804, 1.623], loss: 3.503315, mean_absolute_error: 6.178659, mean_q: 11.362547
  838/4500: episode: 48, duration: 0.040s, episode steps: 19, steps per second: 475, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.102 [-1.269, 0.595], loss: 5.818372, mean_absolute_error: 7.049063, mean_q: 12.445636
  851/4500: episode: 49, duration: 0.037s, episode steps: 13, steps per second: 356, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.107 [-2.352, 1.410], loss: 10.788481, mean_absolute_error: 7.746494, mean_q: 13.335390
  879/4500: episode: 50, duration: 0.060s, episode steps: 28, steps per second: 470, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.028 [-2.627, 1.788], loss: 6.693998, mean_absolute_error: 7.244850, mean_q: 13.164428
  893/4500: episode: 51, duration: 0.032s, episode steps: 14, steps per second: 436, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.113 [-2.230, 1.346], loss: 9.299281, mean_absolute_error: 7.689326, mean_q: 13.457086
  914/4500: episode: 52, duration: 0.045s, episode steps: 21, steps per second: 463, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.070 [-1.173, 0.816], loss: 3.319879, mean_absolute_error: 6.725287, mean_q: 12.208949
  938/4500: episode: 53, duration: 0.053s, episode steps: 24, steps per second: 450, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.040 [-1.358, 0.753], loss: 3.272095, mean_absolute_error: 6.973236, mean_q: 12.690785
 1001/4500: episode: 54, duration: 0.133s, episode steps: 63, steps per second: 473, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.155 [-1.777, 0.764], loss: 1.615879, mean_absolute_error: 6.659793, mean_q: 12.306591
 1013/4500: episode: 55, duration: 0.026s, episode steps: 12, steps per second: 455, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.558, 0.817], loss: 8.487407, mean_absolute_error: 8.167930, mean_q: 14.400851
 1043/4500: episode: 56, duration: 0.107s, episode steps: 30, steps per second: 281, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.037 [-1.644, 0.980], loss: 4.282334, mean_absolute_error: 7.840936, mean_q: 14.331751
 1071/4500: episode: 57, duration: 0.082s, episode steps: 28, steps per second: 342, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.958, 1.326], loss: 4.444690, mean_absolute_error: 7.956348, mean_q: 14.522564
 1087/4500: episode: 58, duration: 0.042s, episode steps: 16, steps per second: 383, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.059 [-1.222, 1.864], loss: 8.584383, mean_absolute_error: 8.368621, mean_q: 14.830271
 1112/4500: episode: 59, duration: 0.070s, episode steps: 25, steps per second: 355, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.061 [-0.647, 1.345], loss: 4.635028, mean_absolute_error: 7.922643, mean_q: 14.520833
 1136/4500: episode: 60, duration: 0.067s, episode steps: 24, steps per second: 357, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.072 [-1.661, 0.940], loss: 5.658150, mean_absolute_error: 8.394624, mean_q: 15.299324
 1156/4500: episode: 61, duration: 0.043s, episode steps: 20, steps per second: 468, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.093 [-1.756, 0.833], loss: 6.954414, mean_absolute_error: 8.271365, mean_q: 14.895187
 1166/4500: episode: 62, duration: 0.023s, episode steps: 10, steps per second: 432, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.130 [-1.608, 0.935], loss: 12.145659, mean_absolute_error: 8.830095, mean_q: 15.392825
 1180/4500: episode: 63, duration: 0.031s, episode steps: 14, steps per second: 454, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.106 [-2.047, 1.174], loss: 8.912445, mean_absolute_error: 8.281430, mean_q: 15.342105
 1190/4500: episode: 64, duration: 0.021s, episode steps: 10, steps per second: 472, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.132 [-1.589, 0.984], loss: 10.091775, mean_absolute_error: 8.375936, mean_q: 15.436637
 1201/4500: episode: 65, duration: 0.024s, episode steps: 11, steps per second: 463, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.121 [-2.298, 1.540], loss: 9.749507, mean_absolute_error: 8.280584, mean_q: 15.562730
 1214/4500: episode: 66, duration: 0.028s, episode steps: 13, steps per second: 457, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.115 [-2.784, 1.712], loss: 9.513006, mean_absolute_error: 7.769759, mean_q: 14.796842
 1225/4500: episode: 67, duration: 0.023s, episode steps: 11, steps per second: 476, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.301, 1.403], loss: 9.262531, mean_absolute_error: 7.629396, mean_q: 14.443970
 1239/4500: episode: 68, duration: 0.031s, episode steps: 14, steps per second: 458, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.124 [-2.175, 1.176], loss: 6.430138, mean_absolute_error: 7.310686, mean_q: 13.536830
 1251/4500: episode: 69, duration: 0.026s, episode steps: 12, steps per second: 460, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.132 [-2.583, 1.538], loss: 6.729999, mean_absolute_error: 7.166597, mean_q: 13.182349
 1265/4500: episode: 70, duration: 0.036s, episode steps: 14, steps per second: 394, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.101 [-1.545, 1.001], loss: 4.912683, mean_absolute_error: 7.032524, mean_q: 12.861273
 1277/4500: episode: 71, duration: 0.027s, episode steps: 12, steps per second: 440, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.122 [-1.713, 0.962], loss: 4.849189, mean_absolute_error: 6.643962, mean_q: 11.974008
 1309/4500: episode: 72, duration: 0.069s, episode steps: 32, steps per second: 462, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.104 [-0.547, 1.588], loss: 4.044686, mean_absolute_error: 7.748611, mean_q: 14.230001
 1341/4500: episode: 73, duration: 0.073s, episode steps: 32, steps per second: 440, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.128 [-0.733, 1.219], loss: 3.919216, mean_absolute_error: 8.046099, mean_q: 14.798558
 1361/4500: episode: 74, duration: 0.044s, episode steps: 20, steps per second: 453, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.201, 0.789], loss: 3.630699, mean_absolute_error: 6.999453, mean_q: 12.822720
 1377/4500: episode: 75, duration: 0.034s, episode steps: 16, steps per second: 474, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.112 [-1.798, 0.947], loss: 4.066587, mean_absolute_error: 7.194320, mean_q: 13.161022
 1397/4500: episode: 76, duration: 0.044s, episode steps: 20, steps per second: 452, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.116, 0.589], loss: 3.592603, mean_absolute_error: 7.048282, mean_q: 13.136683
 1420/4500: episode: 77, duration: 0.053s, episode steps: 23, steps per second: 434, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.068 [-0.960, 1.527], loss: 5.977634, mean_absolute_error: 8.537799, mean_q: 15.575895
 1470/4500: episode: 78, duration: 0.108s, episode steps: 50, steps per second: 464, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.146 [-0.908, 1.223], loss: 2.984260, mean_absolute_error: 8.161167, mean_q: 15.230623
 1481/4500: episode: 79, duration: 0.024s, episode steps: 11, steps per second: 460, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.105 [-1.025, 1.768], loss: 16.030558, mean_absolute_error: 10.268020, mean_q: 17.860410
 1543/4500: episode: 80, duration: 0.130s, episode steps: 62, steps per second: 476, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.007 [-0.922, 1.200], loss: 3.509073, mean_absolute_error: 8.759270, mean_q: 16.425825
 1557/4500: episode: 81, duration: 0.031s, episode steps: 14, steps per second: 458, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.089 [-1.554, 2.424], loss: 17.174425, mean_absolute_error: 10.479301, mean_q: 19.025944
 1573/4500: episode: 82, duration: 0.035s, episode steps: 16, steps per second: 460, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.064 [-1.214, 1.768], loss: 10.946555, mean_absolute_error: 9.882440, mean_q: 18.229048
 1590/4500: episode: 83, duration: 0.036s, episode steps: 17, steps per second: 466, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.077 [-1.168, 1.945], loss: 11.660083, mean_absolute_error: 9.525513, mean_q: 17.792369
 1602/4500: episode: 84, duration: 0.026s, episode steps: 12, steps per second: 463, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.094 [-1.602, 2.573], loss: 15.966139, mean_absolute_error: 9.832483, mean_q: 17.819268
 1613/4500: episode: 85, duration: 0.024s, episode steps: 11, steps per second: 466, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.168, 1.852], loss: 13.970509, mean_absolute_error: 9.471125, mean_q: 16.887026
 1656/4500: episode: 86, duration: 0.092s, episode steps: 43, steps per second: 466, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.110 [-1.649, 0.813], loss: 2.584534, mean_absolute_error: 8.172838, mean_q: 15.337393
 1681/4500: episode: 87, duration: 0.056s, episode steps: 25, steps per second: 449, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.062 [-0.774, 1.403], loss: 6.147360, mean_absolute_error: 9.162753, mean_q: 16.891859
 1712/4500: episode: 88, duration: 0.066s, episode steps: 31, steps per second: 467, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.058 [-0.755, 1.230], loss: 5.474909, mean_absolute_error: 9.357916, mean_q: 17.498180
 1737/4500: episode: 89, duration: 0.061s, episode steps: 25, steps per second: 413, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.046 [-1.252, 0.828], loss: 6.016526, mean_absolute_error: 9.322247, mean_q: 17.312690
 1748/4500: episode: 90, duration: 0.023s, episode steps: 11, steps per second: 471, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.127 [-1.835, 1.128], loss: 14.520050, mean_absolute_error: 9.980632, mean_q: 17.804679
 1758/4500: episode: 91, duration: 0.023s, episode steps: 10, steps per second: 428, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.563, 1.611], loss: 16.232091, mean_absolute_error: 9.758768, mean_q: 17.659299
 1772/4500: episode: 92, duration: 0.032s, episode steps: 14, steps per second: 444, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.095 [-1.576, 0.940], loss: 7.842701, mean_absolute_error: 8.713818, mean_q: 16.284349
 1787/4500: episode: 93, duration: 0.032s, episode steps: 15, steps per second: 463, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.085 [-1.861, 1.174], loss: 7.747160, mean_absolute_error: 8.570329, mean_q: 16.073307
 1797/4500: episode: 94, duration: 0.023s, episode steps: 10, steps per second: 431, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.114 [-2.194, 1.417], loss: 10.436900, mean_absolute_error: 8.471271, mean_q: 15.527750
 1820/4500: episode: 95, duration: 0.049s, episode steps: 23, steps per second: 473, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.063 [-0.755, 1.135], loss: 7.235944, mean_absolute_error: 9.158595, mean_q: 17.030281
 1843/4500: episode: 96, duration: 0.050s, episode steps: 23, steps per second: 464, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.061 [-1.025, 0.556], loss: 4.312774, mean_absolute_error: 7.863433, mean_q: 14.717226
 1899/4500: episode: 97, duration: 0.120s, episode steps: 56, steps per second: 466, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: 0.071 [-1.188, 2.030], loss: 3.268277, mean_absolute_error: 8.794617, mean_q: 16.547288
 1916/4500: episode: 98, duration: 0.037s, episode steps: 17, steps per second: 464, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.063 [-1.200, 1.951], loss: 10.777308, mean_absolute_error: 9.939690, mean_q: 17.929641
 1947/4500: episode: 99, duration: 0.067s, episode steps: 31, steps per second: 466, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.087 [-0.716, 1.225], loss: 5.398239, mean_absolute_error: 9.523578, mean_q: 17.852239
 1959/4500: episode: 100, duration: 0.026s, episode steps: 12, steps per second: 458, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.127 [-0.928, 1.511], loss: 15.108433, mean_absolute_error: 10.218013, mean_q: 18.238397
 1989/4500: episode: 101, duration: 0.066s, episode steps: 30, steps per second: 453, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.931, 0.439], loss: 4.158505, mean_absolute_error: 8.895358, mean_q: 16.734908
 2001/4500: episode: 102, duration: 0.027s, episode steps: 12, steps per second: 452, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.944, 1.482], loss: 13.236293, mean_absolute_error: 9.737478, mean_q: 17.331611
 2068/4500: episode: 103, duration: 0.149s, episode steps: 67, steps per second: 451, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.403 [0.000, 1.000], mean observation: -0.233 [-2.503, 2.309], loss: 4.129478, mean_absolute_error: 9.202036, mean_q: 17.407287
 2079/4500: episode: 104, duration: 0.024s, episode steps: 11, steps per second: 459, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.086 [-1.036, 1.736], loss: 16.696306, mean_absolute_error: 10.564916, mean_q: 18.660108
 2090/4500: episode: 105, duration: 0.025s, episode steps: 11, steps per second: 438, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.157, 1.778], loss: 15.066395, mean_absolute_error: 10.208545, mean_q: 18.182785
 2099/4500: episode: 106, duration: 0.020s, episode steps: 9, steps per second: 442, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.753, 2.862], loss: 16.941557, mean_absolute_error: 9.975437, mean_q: 17.582746
 2121/4500: episode: 107, duration: 0.050s, episode steps: 22, steps per second: 442, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.964, 1.407], loss: 7.154950, mean_absolute_error: 9.259836, mean_q: 17.041394
 2134/4500: episode: 108, duration: 0.031s, episode steps: 13, steps per second: 422, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.123 [-0.954, 1.793], loss: 10.493059, mean_absolute_error: 9.335294, mean_q: 16.895558
 2153/4500: episode: 109, duration: 0.043s, episode steps: 19, steps per second: 443, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.113 [-1.578, 0.755], loss: 6.955825, mean_absolute_error: 9.313245, mean_q: 17.199601
 2167/4500: episode: 110, duration: 0.038s, episode steps: 14, steps per second: 369, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.091 [-0.962, 1.628], loss: 8.728610, mean_absolute_error: 9.120361, mean_q: 16.503152
 2233/4500: episode: 111, duration: 0.148s, episode steps: 66, steps per second: 447, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.252 [-1.829, 1.251], loss: 2.723023, mean_absolute_error: 8.649642, mean_q: 16.424960
 2254/4500: episode: 112, duration: 0.048s, episode steps: 21, steps per second: 442, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.061 [-2.071, 1.212], loss: 7.328562, mean_absolute_error: 9.760858, mean_q: 18.118617
 2284/4500: episode: 113, duration: 0.071s, episode steps: 30, steps per second: 422, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.067 [-1.297, 0.604], loss: 4.679148, mean_absolute_error: 9.555789, mean_q: 18.013334
 2310/4500: episode: 114, duration: 0.058s, episode steps: 26, steps per second: 447, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.068 [-1.021, 1.880], loss: 6.211252, mean_absolute_error: 9.837305, mean_q: 18.292898
 2380/4500: episode: 115, duration: 0.159s, episode steps: 70, steps per second: 441, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.211 [-1.567, 1.012], loss: 2.885530, mean_absolute_error: 9.649569, mean_q: 18.417326
 2406/4500: episode: 116, duration: 0.085s, episode steps: 26, steps per second: 306, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.053 [-1.528, 2.525], loss: 7.604577, mean_absolute_error: 10.811147, mean_q: 20.293547
 2420/4500: episode: 117, duration: 0.040s, episode steps: 14, steps per second: 353, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.100 [-0.830, 1.619], loss: 14.395073, mean_absolute_error: 10.882581, mean_q: 20.002009
 2442/4500: episode: 118, duration: 0.059s, episode steps: 22, steps per second: 374, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.060 [-0.792, 1.341], loss: 7.742004, mean_absolute_error: 10.326357, mean_q: 19.261908
 2460/4500: episode: 119, duration: 0.041s, episode steps: 18, steps per second: 436, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.086 [-0.563, 1.260], loss: 9.819285, mean_absolute_error: 10.439747, mean_q: 19.270410
 2484/4500: episode: 120, duration: 0.056s, episode steps: 24, steps per second: 432, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.831, 1.360], loss: 8.169620, mean_absolute_error: 10.355494, mean_q: 19.379714
 2497/4500: episode: 121, duration: 0.029s, episode steps: 13, steps per second: 453, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.129 [-1.532, 0.751], loss: 13.147248, mean_absolute_error: 10.732165, mean_q: 19.539731
 2563/4500: episode: 122, duration: 0.147s, episode steps: 66, steps per second: 448, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.065 [-1.310, 1.013], loss: 2.924881, mean_absolute_error: 10.040758, mean_q: 19.251652
 2645/4500: episode: 123, duration: 0.183s, episode steps: 82, steps per second: 449, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.065 [-1.012, 0.860], loss: 3.275508, mean_absolute_error: 11.269580, mean_q: 21.653388
 2669/4500: episode: 124, duration: 0.055s, episode steps: 24, steps per second: 437, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.084 [-0.567, 1.349], loss: 9.983474, mean_absolute_error: 12.117061, mean_q: 22.659320
 2681/4500: episode: 125, duration: 0.027s, episode steps: 12, steps per second: 438, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.212, 1.945], loss: 15.510064, mean_absolute_error: 11.444493, mean_q: 20.946504
 2697/4500: episode: 126, duration: 0.037s, episode steps: 16, steps per second: 434, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.059 [-2.260, 1.566], loss: 21.297376, mean_absolute_error: 12.852411, mean_q: 23.569816
 2709/4500: episode: 127, duration: 0.026s, episode steps: 12, steps per second: 453, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.128 [-1.997, 1.147], loss: 19.361319, mean_absolute_error: 12.022468, mean_q: 22.642400
 2720/4500: episode: 128, duration: 0.024s, episode steps: 11, steps per second: 467, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.135 [-2.295, 1.375], loss: 19.948201, mean_absolute_error: 11.365299, mean_q: 21.569121
 2731/4500: episode: 129, duration: 0.025s, episode steps: 11, steps per second: 439, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.840, 1.781], loss: 15.140263, mean_absolute_error: 10.270193, mean_q: 19.866784
 2743/4500: episode: 130, duration: 0.028s, episode steps: 12, steps per second: 434, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.129 [-1.831, 1.125], loss: 11.136671, mean_absolute_error: 9.770186, mean_q: 18.519020
 2753/4500: episode: 131, duration: 0.022s, episode steps: 10, steps per second: 447, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.113 [-1.599, 1.014], loss: 11.442658, mean_absolute_error: 9.236242, mean_q: 17.311176
 2766/4500: episode: 132, duration: 0.029s, episode steps: 13, steps per second: 455, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.107 [-2.245, 1.359], loss: 7.827081, mean_absolute_error: 9.156473, mean_q: 17.472533
 2777/4500: episode: 133, duration: 0.024s, episode steps: 11, steps per second: 456, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.104 [-1.821, 1.192], loss: 8.006023, mean_absolute_error: 8.321128, mean_q: 15.799778
 2789/4500: episode: 134, duration: 0.026s, episode steps: 12, steps per second: 458, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.108 [-2.017, 1.164], loss: 7.951054, mean_absolute_error: 8.421696, mean_q: 15.732334
 2802/4500: episode: 135, duration: 0.031s, episode steps: 13, steps per second: 423, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.115 [-1.740, 0.937], loss: 6.827600, mean_absolute_error: 7.656020, mean_q: 14.221702
 2814/4500: episode: 136, duration: 0.026s, episode steps: 12, steps per second: 460, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-1.765, 1.212], loss: 6.197060, mean_absolute_error: 7.488855, mean_q: 13.973618
 2833/4500: episode: 137, duration: 0.042s, episode steps: 19, steps per second: 455, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.088 [-1.305, 0.736], loss: 5.724622, mean_absolute_error: 7.855413, mean_q: 14.206417
 2850/4500: episode: 138, duration: 0.038s, episode steps: 17, steps per second: 451, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.111 [-1.093, 0.622], loss: 5.405652, mean_absolute_error: 8.026464, mean_q: 14.645744
 2902/4500: episode: 139, duration: 0.114s, episode steps: 52, steps per second: 457, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.404 [0.000, 1.000], mean observation: -0.088 [-2.350, 2.629], loss: 4.400379, mean_absolute_error: 8.824588, mean_q: 16.569197
 2913/4500: episode: 140, duration: 0.025s, episode steps: 11, steps per second: 442, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.119 [-1.027, 1.835], loss: 15.376194, mean_absolute_error: 10.019986, mean_q: 18.126509
 2925/4500: episode: 141, duration: 0.028s, episode steps: 12, steps per second: 422, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-0.945, 1.641], loss: 11.848376, mean_absolute_error: 9.596111, mean_q: 17.672587
 2934/4500: episode: 142, duration: 0.020s, episode steps: 9, steps per second: 444, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.343, 2.228], loss: 15.675489, mean_absolute_error: 9.359217, mean_q: 17.007323
 2946/4500: episode: 143, duration: 0.027s, episode steps: 12, steps per second: 438, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-0.958, 1.630], loss: 10.761343, mean_absolute_error: 8.880340, mean_q: 16.410901
 2961/4500: episode: 144, duration: 0.038s, episode steps: 15, steps per second: 392, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.077 [-1.382, 2.164], loss: 7.476096, mean_absolute_error: 8.514849, mean_q: 15.697982
 2972/4500: episode: 145, duration: 0.025s, episode steps: 11, steps per second: 435, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.107 [-0.995, 1.576], loss: 10.274512, mean_absolute_error: 8.670615, mean_q: 15.514146
 3007/4500: episode: 146, duration: 0.073s, episode steps: 35, steps per second: 479, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.013 [-1.117, 0.792], loss: 3.015685, mean_absolute_error: 8.048840, mean_q: 15.336145
 3126/4500: episode: 147, duration: 0.261s, episode steps: 119, steps per second: 457, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.145 [-1.193, 1.187], loss: 2.485205, mean_absolute_error: 9.524336, mean_q: 18.258746
 3159/4500: episode: 148, duration: 0.075s, episode steps: 33, steps per second: 441, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.042 [-1.472, 0.794], loss: 4.257640, mean_absolute_error: 9.756105, mean_q: 18.585859
 3208/4500: episode: 149, duration: 0.112s, episode steps: 49, steps per second: 437, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.148 [-1.271, 0.560], loss: 3.448045, mean_absolute_error: 10.283895, mean_q: 19.696705
 3225/4500: episode: 150, duration: 0.036s, episode steps: 17, steps per second: 476, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.085 [-0.768, 1.378], loss: 11.817627, mean_absolute_error: 11.052143, mean_q: 20.412637
 3244/4500: episode: 151, duration: 0.038s, episode steps: 19, steps per second: 494, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.062 [-1.148, 1.878], loss: 9.277667, mean_absolute_error: 10.663276, mean_q: 20.157831
 3265/4500: episode: 152, duration: 0.047s, episode steps: 21, steps per second: 448, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.057 [-1.013, 1.779], loss: 8.377856, mean_absolute_error: 10.455446, mean_q: 19.506238
 3335/4500: episode: 153, duration: 0.149s, episode steps: 70, steps per second: 468, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.759, 1.157], loss: 3.374082, mean_absolute_error: 10.622109, mean_q: 20.445679
 3371/4500: episode: 154, duration: 0.076s, episode steps: 36, steps per second: 472, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.600, 1.089], loss: 7.107226, mean_absolute_error: 11.537909, mean_q: 21.884521
 3403/4500: episode: 155, duration: 0.070s, episode steps: 32, steps per second: 454, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.616, 0.944], loss: 7.895681, mean_absolute_error: 11.558057, mean_q: 21.775419
 3418/4500: episode: 156, duration: 0.032s, episode steps: 15, steps per second: 474, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.112 [-1.609, 0.778], loss: 11.347695, mean_absolute_error: 11.280853, mean_q: 21.052100
 3440/4500: episode: 157, duration: 0.047s, episode steps: 22, steps per second: 465, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.063 [-1.091, 0.809], loss: 9.402090, mean_absolute_error: 11.000894, mean_q: 20.958313
 3529/4500: episode: 158, duration: 0.198s, episode steps: 89, steps per second: 450, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.225 [-1.386, 1.069], loss: 3.599766, mean_absolute_error: 11.544882, mean_q: 22.381583
 3554/4500: episode: 159, duration: 0.053s, episode steps: 25, steps per second: 473, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.033 [-0.798, 1.082], loss: 12.058673, mean_absolute_error: 12.530716, mean_q: 23.633941
 3620/4500: episode: 160, duration: 0.143s, episode steps: 66, steps per second: 462, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.009 [-1.381, 1.064], loss: 4.553050, mean_absolute_error: 12.082282, mean_q: 23.490666
 3644/4500: episode: 161, duration: 0.055s, episode steps: 24, steps per second: 438, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.073 [-1.373, 0.630], loss: 9.142419, mean_absolute_error: 12.652848, mean_q: 24.085907
 3700/4500: episode: 162, duration: 0.119s, episode steps: 56, steps per second: 469, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.069 [-1.364, 1.656], loss: 5.817986, mean_absolute_error: 12.801607, mean_q: 24.769850
 3736/4500: episode: 163, duration: 0.104s, episode steps: 36, steps per second: 347, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-0.929, 0.870], loss: 8.155808, mean_absolute_error: 13.054461, mean_q: 25.207977
 3757/4500: episode: 164, duration: 0.047s, episode steps: 21, steps per second: 443, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.090 [-1.136, 0.566], loss: 12.981168, mean_absolute_error: 12.979129, mean_q: 24.431971
 3774/4500: episode: 165, duration: 0.034s, episode steps: 17, steps per second: 497, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.091 [-1.395, 0.788], loss: 13.542870, mean_absolute_error: 12.543337, mean_q: 23.328819
 3786/4500: episode: 166, duration: 0.027s, episode steps: 12, steps per second: 444, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.088 [-1.621, 1.012], loss: 16.812678, mean_absolute_error: 11.961144, mean_q: 22.043572
 3805/4500: episode: 167, duration: 0.042s, episode steps: 19, steps per second: 456, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.085 [-1.417, 0.943], loss: 11.421329, mean_absolute_error: 11.490669, mean_q: 21.313756
 3825/4500: episode: 168, duration: 0.041s, episode steps: 20, steps per second: 482, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.956, 0.617], loss: 10.781009, mean_absolute_error: 11.552968, mean_q: 21.074289
 3870/4500: episode: 169, duration: 0.094s, episode steps: 45, steps per second: 477, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.060 [-1.194, 1.257], loss: 6.839391, mean_absolute_error: 11.718465, mean_q: 22.305293
 3883/4500: episode: 170, duration: 0.028s, episode steps: 13, steps per second: 469, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.098 [-1.029, 1.885], loss: 19.150452, mean_absolute_error: 12.657493, mean_q: 23.164558
 3902/4500: episode: 171, duration: 0.042s, episode steps: 19, steps per second: 457, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.110 [-0.573, 1.377], loss: 12.239343, mean_absolute_error: 12.103262, mean_q: 22.709348
 3927/4500: episode: 172, duration: 0.059s, episode steps: 25, steps per second: 424, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.082 [-0.609, 1.445], loss: 9.453576, mean_absolute_error: 11.532567, mean_q: 21.741967
 3940/4500: episode: 173, duration: 0.029s, episode steps: 13, steps per second: 454, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.097 [-0.620, 1.173], loss: 16.135217, mean_absolute_error: 11.458468, mean_q: 21.076118
 3957/4500: episode: 174, duration: 0.036s, episode steps: 17, steps per second: 473, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.053 [-1.210, 1.821], loss: 10.295470, mean_absolute_error: 10.891006, mean_q: 20.286718
 3973/4500: episode: 175, duration: 0.035s, episode steps: 16, steps per second: 462, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.079 [-0.978, 1.497], loss: 11.872955, mean_absolute_error: 10.730103, mean_q: 19.689646
 3993/4500: episode: 176, duration: 0.043s, episode steps: 20, steps per second: 470, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.108 [-0.358, 0.876], loss: 11.402632, mean_absolute_error: 11.032073, mean_q: 20.526623
 4056/4500: episode: 177, duration: 0.131s, episode steps: 63, steps per second: 483, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.039 [-0.989, 0.728], loss: 3.307325, mean_absolute_error: 10.510007, mean_q: 20.307334
 4121/4500: episode: 178, duration: 0.141s, episode steps: 65, steps per second: 460, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.038 [-1.169, 1.600], loss: 4.631641, mean_absolute_error: 11.436076, mean_q: 22.068459
 4152/4500: episode: 179, duration: 0.068s, episode steps: 31, steps per second: 456, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.076 [-1.306, 0.621], loss: 7.471124, mean_absolute_error: 11.781640, mean_q: 22.536061
 4187/4500: episode: 180, duration: 0.076s, episode steps: 35, steps per second: 461, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.066 [-0.861, 0.603], loss: 6.814880, mean_absolute_error: 11.650681, mean_q: 22.221969
 4209/4500: episode: 181, duration: 0.047s, episode steps: 22, steps per second: 468, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.035 [-1.505, 0.994], loss: 8.433857, mean_absolute_error: 11.671824, mean_q: 22.310574
 4232/4500: episode: 182, duration: 0.049s, episode steps: 23, steps per second: 470, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.099 [-1.266, 0.426], loss: 7.367616, mean_absolute_error: 11.505088, mean_q: 21.787169
 4264/4500: episode: 183, duration: 0.068s, episode steps: 32, steps per second: 467, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.590, 0.894], loss: 8.261961, mean_absolute_error: 11.743840, mean_q: 22.327182
 4341/4500: episode: 184, duration: 0.160s, episode steps: 77, steps per second: 480, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.202 [-0.788, 1.317], loss: 3.243023, mean_absolute_error: 11.695993, mean_q: 22.859449
 4368/4500: episode: 185, duration: 0.059s, episode steps: 27, steps per second: 461, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.085 [-1.521, 0.609], loss: 8.368020, mean_absolute_error: 12.349427, mean_q: 23.500367
 4421/4500: episode: 186, duration: 0.126s, episode steps: 53, steps per second: 422, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.143 [-1.113, 0.462], loss: 5.261669, mean_absolute_error: 12.238670, mean_q: 23.765472
 4442/4500: episode: 187, duration: 0.045s, episode steps: 21, steps per second: 471, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.073 [-0.954, 1.425], loss: 15.218028, mean_absolute_error: 13.038658, mean_q: 24.241968
 4454/4500: episode: 188, duration: 0.028s, episode steps: 12, steps per second: 422, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-0.977, 1.630], loss: 19.047502, mean_absolute_error: 12.620401, mean_q: 23.065391
done, took 10.817 seconds
TEMPO:  1561719254.9192123


DQN


Testing for 5 episodes ...
Episode 1: reward: 151.000, steps: 151
Episode 2: reward: 164.000, steps: 164
Episode 3: reward: 159.000, steps: 159
Episode 4: reward: 165.000, steps: 165
Episode 5: reward: 177.000, steps: 177

\CEM


Testing for 5 episodes ...
Episode 1: reward: 178.000, steps: 178
Episode 2: reward: 147.000, steps: 147
Episode 3: reward: 183.000, steps: 183
Episode 4: reward: 187.000, steps: 187
Episode 5: reward: 114.000, steps: 114

\SARSA


Testing for 5 episodes ...
Episode 1: reward: 61.000, steps: 61
Episode 2: reward: 50.000, steps: 50
Episode 3: reward: 66.000, steps: 66
Episode 4: reward: 77.000, steps: 77
Episode 5: reward: 164.000, steps: 164

\DUEL


Testing for 5 episodes ...
Episode 1: reward: 158.000, steps: 158
Episode 2: reward: 200.000, steps: 200
Episode 3: reward: 170.000, steps: 170
Episode 4: reward: 182.000, steps: 182
Episode 5: reward: 200.000, steps: 200
