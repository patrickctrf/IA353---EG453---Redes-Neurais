{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 5\n",
    "\n",
    "Para a arealização da questão 5, o primeiro passo foi verificar qual seria o número de steps exigidos por cada método para resultar um um tempo aproximado de impressão. Para isto, foi utilizado o script abaixo, no qual se ajustou manualmente o número de iterações para o treinamento por cada técnica reproduzindo o script diversas vezes. Ao final, regulamos o tempo de treinamento de todas em cerca de 10,5 segundos, pois este era o tempo em que as melhores técnicas começavam a saturar seus resultados em 100% de recompensa nos testes (200/200 em 5 vezes). Foi utilizado um treinamento de 3200 steps.\n",
    "\n",
    "Ainda utilizando o script abaixo, verificamos o desempenho médio de cada técnica no teste. Sendo 5 recompensas de até 200 pontos para cada uma, a técnica de melhor desempenho foi a Deep Q Learning (DQN), com 196,8 steps em média antes de deixar o pêndulo cair (em 4 tentativas o pêndulo sequer caiu, com a contagem saturando em 200). \n",
    "\n",
    "A segunda melhor técnica foi a Cross-Entropy Method (CEM), que obteve 192,4 steps em média, tendo treinado com 11000 (o maior número de steps do grupo). O tempo de execução de seus step é rápido, mas ela exige muitos steps para melhorar seu desempenho.\n",
    "\n",
    "A terceira melhor técnica foi a Duel DQN, que atingiu 165,6 de desempenho após um treinamento de apenas 2900 steps (o menor do grupo). Esta técnica não necessita de tantos steps para ser treinada, mas estes são de lenta execução.\n",
    "\n",
    "O pior resultado fica com a State-ActionReward-State-Action, que utilizou treinamentos de 4500 steps e obteve desempenho médio de 24,2 antes de falhar nos testes, deixando o pêndulo cair do carrinho muito rapidmente em relação à outra.\n",
    "\n",
    "Vale ressaltar que, quando aumentado o número de steps dos treinamentos, todas as técnicas atingiram desempenho médio igual ou próximo a 100%, indicando que elas podem cumprir seu objetivo, demandando apenas diferentes tempos de treino para tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0628 07:54:50.469259 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0628 07:54:50.484366 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0628 07:54:50.497800 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0628 07:54:50.672117 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0628 07:54:50.672897 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 07:54:50.804264 140545833047872 deprecation_wrapper.py:119] From /home/patrick/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "TEMPO:  1561719291.546456\n",
      "Training for 2900 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/patrick/anaconda3/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39/2900: episode: 1, duration: 1.292s, episode steps: 39, steps per second: 30, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.153 [-0.990, 1.781], loss: 0.410016, mean_absolute_error: 0.561866, mean_q: 0.256616\n",
      "   60/2900: episode: 2, duration: 0.068s, episode steps: 21, steps per second: 307, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.009 [-1.990, 1.412], loss: 0.186312, mean_absolute_error: 0.576259, mean_q: 0.599455\n",
      "   97/2900: episode: 3, duration: 0.124s, episode steps: 37, steps per second: 299, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: 0.008 [-2.402, 1.795], loss: 0.041041, mean_absolute_error: 0.681984, mean_q: 1.158313\n",
      "  122/2900: episode: 4, duration: 0.099s, episode steps: 25, steps per second: 253, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-1.022, 2.040], loss: 0.017976, mean_absolute_error: 0.773460, mean_q: 1.449667\n",
      "  132/2900: episode: 5, duration: 0.033s, episode steps: 10, steps per second: 304, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.108 [-2.030, 1.217], loss: 0.018090, mean_absolute_error: 0.822996, mean_q: 1.587707\n",
      "  145/2900: episode: 6, duration: 0.044s, episode steps: 13, steps per second: 299, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.123 [-0.785, 1.465], loss: 0.026360, mean_absolute_error: 0.879854, mean_q: 1.679214\n",
      "  183/2900: episode: 7, duration: 0.127s, episode steps: 38, steps per second: 298, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.107 [-0.787, 1.176], loss: 0.026769, mean_absolute_error: 0.976113, mean_q: 1.888601\n",
      "  195/2900: episode: 8, duration: 0.046s, episode steps: 12, steps per second: 260, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.375, 2.175], loss: 0.027476, mean_absolute_error: 1.066162, mean_q: 2.069053\n",
      "  209/2900: episode: 9, duration: 0.045s, episode steps: 14, steps per second: 309, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.090 [-2.050, 1.228], loss: 0.049152, mean_absolute_error: 1.138972, mean_q: 2.209898\n",
      "  224/2900: episode: 10, duration: 0.050s, episode steps: 15, steps per second: 300, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.694, 0.970], loss: 0.039039, mean_absolute_error: 1.185468, mean_q: 2.303268\n",
      "  238/2900: episode: 11, duration: 0.049s, episode steps: 14, steps per second: 288, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.096 [-1.208, 2.009], loss: 0.053671, mean_absolute_error: 1.256770, mean_q: 2.425184\n",
      "  249/2900: episode: 12, duration: 0.039s, episode steps: 11, steps per second: 282, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.103 [-2.142, 1.410], loss: 0.057431, mean_absolute_error: 1.307612, mean_q: 2.538501\n",
      "  262/2900: episode: 13, duration: 0.045s, episode steps: 13, steps per second: 290, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.091 [-2.353, 1.417], loss: 0.048603, mean_absolute_error: 1.341644, mean_q: 2.589199\n",
      "  319/2900: episode: 14, duration: 0.207s, episode steps: 57, steps per second: 276, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.152 [-2.071, 1.937], loss: 0.062953, mean_absolute_error: 1.478511, mean_q: 2.847005\n",
      "  346/2900: episode: 15, duration: 0.097s, episode steps: 27, steps per second: 279, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.052 [-0.805, 1.600], loss: 0.062965, mean_absolute_error: 1.652239, mean_q: 3.205262\n",
      "  361/2900: episode: 16, duration: 0.051s, episode steps: 15, steps per second: 293, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.084 [-2.393, 1.571], loss: 0.089545, mean_absolute_error: 1.735902, mean_q: 3.294625\n",
      "  390/2900: episode: 17, duration: 0.109s, episode steps: 29, steps per second: 267, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.042 [-0.782, 1.582], loss: 0.117013, mean_absolute_error: 1.868935, mean_q: 3.578652\n",
      "  418/2900: episode: 18, duration: 0.095s, episode steps: 28, steps per second: 293, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.035 [-1.659, 1.156], loss: 0.097038, mean_absolute_error: 1.972619, mean_q: 3.788746\n",
      "  430/2900: episode: 19, duration: 0.040s, episode steps: 12, steps per second: 301, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.125 [-1.542, 2.501], loss: 0.113488, mean_absolute_error: 2.048425, mean_q: 3.915236\n",
      "  457/2900: episode: 20, duration: 0.097s, episode steps: 27, steps per second: 278, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.054 [-1.754, 2.782], loss: 0.106106, mean_absolute_error: 2.145951, mean_q: 4.115187\n",
      "  468/2900: episode: 21, duration: 0.041s, episode steps: 11, steps per second: 267, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.146 [-1.159, 2.026], loss: 0.119666, mean_absolute_error: 2.249658, mean_q: 4.323699\n",
      "  479/2900: episode: 22, duration: 0.037s, episode steps: 11, steps per second: 299, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.121 [-1.386, 2.293], loss: 0.133461, mean_absolute_error: 2.266572, mean_q: 4.309992\n",
      "  491/2900: episode: 23, duration: 0.040s, episode steps: 12, steps per second: 300, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.126 [-1.146, 2.076], loss: 0.167983, mean_absolute_error: 2.363292, mean_q: 4.476419\n",
      "  506/2900: episode: 24, duration: 0.051s, episode steps: 15, steps per second: 292, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.108 [-0.780, 1.490], loss: 0.098532, mean_absolute_error: 2.371911, mean_q: 4.534491\n",
      "  522/2900: episode: 25, duration: 0.059s, episode steps: 16, steps per second: 270, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.051 [-1.380, 2.021], loss: 0.143169, mean_absolute_error: 2.450586, mean_q: 4.655739\n",
      "  542/2900: episode: 26, duration: 0.066s, episode steps: 20, steps per second: 303, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.100 [-0.790, 1.785], loss: 0.149175, mean_absolute_error: 2.497596, mean_q: 4.752035\n",
      "  553/2900: episode: 27, duration: 0.036s, episode steps: 11, steps per second: 302, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.138 [-1.698, 0.973], loss: 0.164006, mean_absolute_error: 2.551883, mean_q: 4.850903\n",
      "  587/2900: episode: 28, duration: 0.118s, episode steps: 34, steps per second: 289, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.050 [-1.237, 2.264], loss: 0.144714, mean_absolute_error: 2.661916, mean_q: 5.093681\n",
      "  608/2900: episode: 29, duration: 0.068s, episode steps: 21, steps per second: 310, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.056 [-1.264, 0.648], loss: 0.165351, mean_absolute_error: 2.765212, mean_q: 5.271184\n",
      "  628/2900: episode: 30, duration: 0.083s, episode steps: 20, steps per second: 242, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.032 [-1.992, 1.407], loss: 0.210773, mean_absolute_error: 2.853961, mean_q: 5.463167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  664/2900: episode: 31, duration: 0.132s, episode steps: 36, steps per second: 273, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.361 [0.000, 1.000], mean observation: -0.022 [-1.972, 2.740], loss: 0.181011, mean_absolute_error: 2.958665, mean_q: 5.685248\n",
      "  692/2900: episode: 32, duration: 0.091s, episode steps: 28, steps per second: 307, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.042 [-1.423, 0.930], loss: 0.140912, mean_absolute_error: 3.107859, mean_q: 6.020526\n",
      "  719/2900: episode: 33, duration: 0.088s, episode steps: 27, steps per second: 308, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.026 [-1.669, 1.189], loss: 0.191176, mean_absolute_error: 3.231830, mean_q: 6.267021\n",
      "  734/2900: episode: 34, duration: 0.055s, episode steps: 15, steps per second: 272, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.129, 2.000], loss: 0.230597, mean_absolute_error: 3.309911, mean_q: 6.383002\n",
      "  760/2900: episode: 35, duration: 0.085s, episode steps: 26, steps per second: 306, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.703, 1.147], loss: 0.215402, mean_absolute_error: 3.381451, mean_q: 6.550233\n",
      "  789/2900: episode: 36, duration: 0.093s, episode steps: 29, steps per second: 312, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.068 [-0.608, 0.924], loss: 0.202511, mean_absolute_error: 3.504919, mean_q: 6.805653\n",
      "  819/2900: episode: 37, duration: 0.100s, episode steps: 30, steps per second: 301, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.077 [-1.789, 0.822], loss: 0.196389, mean_absolute_error: 3.639936, mean_q: 7.078656\n",
      "  837/2900: episode: 38, duration: 0.059s, episode steps: 18, steps per second: 304, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.068 [-1.679, 1.188], loss: 0.202414, mean_absolute_error: 3.757139, mean_q: 7.309000\n",
      "  859/2900: episode: 39, duration: 0.081s, episode steps: 22, steps per second: 273, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.077 [-1.313, 0.783], loss: 0.378568, mean_absolute_error: 3.805889, mean_q: 7.368187\n",
      "  876/2900: episode: 40, duration: 0.058s, episode steps: 17, steps per second: 294, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.079 [-1.873, 1.162], loss: 0.286450, mean_absolute_error: 3.898169, mean_q: 7.624951\n",
      "  926/2900: episode: 41, duration: 0.164s, episode steps: 50, steps per second: 305, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.012 [-2.104, 1.189], loss: 0.265567, mean_absolute_error: 4.056952, mean_q: 7.961437\n",
      "  965/2900: episode: 42, duration: 0.122s, episode steps: 39, steps per second: 319, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.061 [-0.586, 0.945], loss: 0.377866, mean_absolute_error: 4.232874, mean_q: 8.242741\n",
      "  982/2900: episode: 43, duration: 0.060s, episode steps: 17, steps per second: 284, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.118 [-0.559, 1.206], loss: 0.299227, mean_absolute_error: 4.369838, mean_q: 8.517796\n",
      "  996/2900: episode: 44, duration: 0.052s, episode steps: 14, steps per second: 267, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.084 [-1.433, 0.831], loss: 0.392094, mean_absolute_error: 4.438239, mean_q: 8.634451\n",
      " 1026/2900: episode: 45, duration: 0.102s, episode steps: 30, steps per second: 295, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.428, 0.955], loss: 0.271708, mean_absolute_error: 4.575877, mean_q: 8.964684\n",
      " 1053/2900: episode: 46, duration: 0.085s, episode steps: 27, steps per second: 316, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.011 [-1.425, 1.011], loss: 0.458589, mean_absolute_error: 4.672956, mean_q: 9.119616\n",
      " 1097/2900: episode: 47, duration: 0.143s, episode steps: 44, steps per second: 307, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.388, 0.875], loss: 0.400657, mean_absolute_error: 4.808175, mean_q: 9.392777\n",
      " 1119/2900: episode: 48, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.079 [-1.690, 0.952], loss: 0.379845, mean_absolute_error: 4.967041, mean_q: 9.708843\n",
      " 1178/2900: episode: 49, duration: 0.303s, episode steps: 59, steps per second: 194, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.107 [-0.779, 0.943], loss: 0.447626, mean_absolute_error: 5.099752, mean_q: 9.978365\n",
      " 1200/2900: episode: 50, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.142, 0.575], loss: 0.418421, mean_absolute_error: 5.317847, mean_q: 10.398204\n",
      " 1212/2900: episode: 51, duration: 0.048s, episode steps: 12, steps per second: 249, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.492, 1.011], loss: 0.529815, mean_absolute_error: 5.258430, mean_q: 10.221739\n",
      " 1245/2900: episode: 52, duration: 0.115s, episode steps: 33, steps per second: 288, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: -0.054 [-1.383, 1.695], loss: 0.372687, mean_absolute_error: 5.484053, mean_q: 10.738366\n",
      " 1262/2900: episode: 53, duration: 0.055s, episode steps: 17, steps per second: 312, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.094 [-1.544, 0.755], loss: 0.448563, mean_absolute_error: 5.531707, mean_q: 10.871658\n",
      " 1308/2900: episode: 54, duration: 0.153s, episode steps: 46, steps per second: 301, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.023 [-1.080, 0.953], loss: 0.479424, mean_absolute_error: 5.721945, mean_q: 11.191198\n",
      " 1416/2900: episode: 55, duration: 0.408s, episode steps: 108, steps per second: 265, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.443 [-0.698, 2.227], loss: 0.411677, mean_absolute_error: 5.983414, mean_q: 11.788319\n",
      " 1438/2900: episode: 56, duration: 0.098s, episode steps: 22, steps per second: 224, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.061 [-0.810, 1.312], loss: 0.565134, mean_absolute_error: 6.133311, mean_q: 12.078339\n",
      " 1477/2900: episode: 57, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.048 [-1.273, 0.594], loss: 0.419468, mean_absolute_error: 6.321717, mean_q: 12.522439\n",
      " 1556/2900: episode: 58, duration: 0.436s, episode steps: 79, steps per second: 181, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.111 [-1.045, 0.526], loss: 0.503512, mean_absolute_error: 6.519304, mean_q: 12.930772\n",
      " 1648/2900: episode: 59, duration: 0.493s, episode steps: 92, steps per second: 187, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.444 [-0.820, 1.806], loss: 0.567929, mean_absolute_error: 6.824312, mean_q: 13.594226\n",
      " 1667/2900: episode: 60, duration: 0.064s, episode steps: 19, steps per second: 299, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.084 [-0.822, 1.173], loss: 0.569702, mean_absolute_error: 7.094046, mean_q: 14.139682\n",
      " 1684/2900: episode: 61, duration: 0.060s, episode steps: 17, steps per second: 283, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.075 [-0.605, 1.027], loss: 0.653586, mean_absolute_error: 7.155879, mean_q: 14.196536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1797/2900: episode: 62, duration: 0.444s, episode steps: 113, steps per second: 254, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.288 [-0.958, 1.494], loss: 0.644449, mean_absolute_error: 7.446884, mean_q: 14.854182\n",
      " 1943/2900: episode: 63, duration: 0.906s, episode steps: 146, steps per second: 161, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.154 [-0.918, 1.163], loss: 0.834452, mean_absolute_error: 7.994983, mean_q: 15.991569\n",
      " 2072/2900: episode: 64, duration: 0.705s, episode steps: 129, steps per second: 183, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.062 [-1.445, 1.198], loss: 0.867386, mean_absolute_error: 8.624294, mean_q: 17.293127\n",
      " 2154/2900: episode: 65, duration: 0.455s, episode steps: 82, steps per second: 180, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.163 [-1.153, 1.043], loss: 1.126946, mean_absolute_error: 9.188817, mean_q: 18.398054\n",
      " 2354/2900: episode: 66, duration: 0.998s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.107 [-1.397, 1.676], loss: 1.120701, mean_absolute_error: 9.893047, mean_q: 19.894209\n",
      " 2482/2900: episode: 67, duration: 0.478s, episode steps: 128, steps per second: 268, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.199 [-1.035, 1.278], loss: 1.212474, mean_absolute_error: 10.653069, mean_q: 21.537664\n",
      " 2574/2900: episode: 68, duration: 0.487s, episode steps: 92, steps per second: 189, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.157 [-0.967, 1.117], loss: 1.074126, mean_absolute_error: 11.213053, mean_q: 22.675928\n",
      " 2729/2900: episode: 69, duration: 0.779s, episode steps: 155, steps per second: 199, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.220 [-1.152, 1.657], loss: 1.467797, mean_absolute_error: 11.797890, mean_q: 23.757227\n",
      " 2834/2900: episode: 70, duration: 0.471s, episode steps: 105, steps per second: 223, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.204 [-1.378, 0.934], loss: 1.378125, mean_absolute_error: 12.477792, mean_q: 25.210587\n",
      "done, took 13.620 seconds\n",
      "TEMPO:  1561719305.1664028\n",
      "Training for 3200 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29/3200: episode: 1, duration: 0.845s, episode steps: 29, steps per second: 34, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.041 [-1.064, 0.626], loss: 0.455805, mean_absolute_error: 0.499867, mean_q: 0.049333\n",
      "   61/3200: episode: 2, duration: 0.127s, episode steps: 32, steps per second: 251, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.048 [-2.420, 1.518], loss: 0.307264, mean_absolute_error: 0.515373, mean_q: 0.270659\n",
      "   79/3200: episode: 3, duration: 0.072s, episode steps: 18, steps per second: 250, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.107 [-0.763, 1.610], loss: 0.131428, mean_absolute_error: 0.558231, mean_q: 0.664464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  108/3200: episode: 4, duration: 0.111s, episode steps: 29, steps per second: 260, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.055 [-1.424, 0.995], loss: 0.042255, mean_absolute_error: 0.671613, mean_q: 1.167719\n",
      "  124/3200: episode: 5, duration: 0.057s, episode steps: 16, steps per second: 281, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.066 [-1.749, 1.031], loss: 0.025494, mean_absolute_error: 0.720847, mean_q: 1.304302\n",
      "  168/3200: episode: 6, duration: 0.165s, episode steps: 44, steps per second: 267, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.040 [-1.329, 1.731], loss: 0.021315, mean_absolute_error: 0.805734, mean_q: 1.496822\n",
      "  191/3200: episode: 7, duration: 0.081s, episode steps: 23, steps per second: 283, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.105 [-0.551, 1.054], loss: 0.012945, mean_absolute_error: 0.923046, mean_q: 1.789144\n",
      "  204/3200: episode: 8, duration: 0.051s, episode steps: 13, steps per second: 255, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.842, 1.798], loss: 0.017199, mean_absolute_error: 0.987263, mean_q: 1.941510\n",
      "  259/3200: episode: 9, duration: 0.192s, episode steps: 55, steps per second: 286, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.077 [-1.306, 1.419], loss: 0.036595, mean_absolute_error: 1.136119, mean_q: 2.204782\n",
      "  273/3200: episode: 10, duration: 0.049s, episode steps: 14, steps per second: 287, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.113 [-1.169, 2.068], loss: 0.034613, mean_absolute_error: 1.246275, mean_q: 2.408919\n",
      "  304/3200: episode: 11, duration: 0.104s, episode steps: 31, steps per second: 299, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.016 [-1.623, 1.206], loss: 0.062291, mean_absolute_error: 1.361492, mean_q: 2.628145\n",
      "  327/3200: episode: 12, duration: 0.079s, episode steps: 23, steps per second: 290, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.099 [-0.804, 1.184], loss: 0.042869, mean_absolute_error: 1.462292, mean_q: 2.859310\n",
      "  344/3200: episode: 13, duration: 0.060s, episode steps: 17, steps per second: 285, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.094 [-0.947, 1.828], loss: 0.067712, mean_absolute_error: 1.549509, mean_q: 2.992504\n",
      "  375/3200: episode: 14, duration: 0.101s, episode steps: 31, steps per second: 307, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.067 [-1.993, 0.988], loss: 0.101100, mean_absolute_error: 1.662176, mean_q: 3.177122\n",
      "  397/3200: episode: 15, duration: 0.078s, episode steps: 22, steps per second: 283, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.068 [-0.786, 1.537], loss: 0.067519, mean_absolute_error: 1.743702, mean_q: 3.385732\n",
      "  444/3200: episode: 16, duration: 0.156s, episode steps: 47, steps per second: 301, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.065 [-1.476, 1.497], loss: 0.079898, mean_absolute_error: 1.901564, mean_q: 3.711152\n",
      "  475/3200: episode: 17, duration: 0.101s, episode steps: 31, steps per second: 306, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.029 [-0.826, 1.426], loss: 0.112804, mean_absolute_error: 2.057702, mean_q: 4.001348\n",
      "  494/3200: episode: 18, duration: 0.069s, episode steps: 19, steps per second: 276, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.101 [-0.808, 1.236], loss: 0.123861, mean_absolute_error: 2.166257, mean_q: 4.211483\n",
      "  512/3200: episode: 19, duration: 0.055s, episode steps: 18, steps per second: 329, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.074 [-0.942, 1.587], loss: 0.127872, mean_absolute_error: 2.267416, mean_q: 4.437156\n",
      "  529/3200: episode: 20, duration: 0.056s, episode steps: 17, steps per second: 305, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.060 [-1.507, 2.196], loss: 0.191642, mean_absolute_error: 2.333669, mean_q: 4.490963\n",
      "  543/3200: episode: 21, duration: 0.047s, episode steps: 14, steps per second: 300, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.125, 0.573], loss: 0.215088, mean_absolute_error: 2.418023, mean_q: 4.671952\n",
      "  565/3200: episode: 22, duration: 0.070s, episode steps: 22, steps per second: 316, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.064 [-1.188, 2.115], loss: 0.194787, mean_absolute_error: 2.520581, mean_q: 4.886597\n",
      "  600/3200: episode: 23, duration: 0.153s, episode steps: 35, steps per second: 229, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.061 [-1.100, 0.896], loss: 0.215037, mean_absolute_error: 2.616286, mean_q: 5.019549\n",
      "  645/3200: episode: 24, duration: 0.146s, episode steps: 45, steps per second: 309, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.072 [-1.186, 1.666], loss: 0.212981, mean_absolute_error: 2.769162, mean_q: 5.377914\n",
      "  663/3200: episode: 25, duration: 0.063s, episode steps: 18, steps per second: 287, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.084 [-1.325, 2.188], loss: 0.227207, mean_absolute_error: 2.890542, mean_q: 5.632577\n",
      "  681/3200: episode: 26, duration: 0.061s, episode steps: 18, steps per second: 296, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.076 [-0.989, 1.551], loss: 0.212964, mean_absolute_error: 2.988226, mean_q: 5.795792\n",
      "  711/3200: episode: 27, duration: 0.095s, episode steps: 30, steps per second: 315, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.061 [-1.452, 0.808], loss: 0.336357, mean_absolute_error: 3.108849, mean_q: 5.983800\n",
      "  733/3200: episode: 28, duration: 0.073s, episode steps: 22, steps per second: 303, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.083 [-1.359, 0.771], loss: 0.236535, mean_absolute_error: 3.202934, mean_q: 6.218567\n",
      "  751/3200: episode: 29, duration: 0.058s, episode steps: 18, steps per second: 308, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.107 [-1.259, 0.741], loss: 0.362505, mean_absolute_error: 3.246634, mean_q: 6.286040\n",
      "  811/3200: episode: 30, duration: 0.190s, episode steps: 60, steps per second: 316, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.106 [-0.577, 1.089], loss: 0.298299, mean_absolute_error: 3.441173, mean_q: 6.685385\n",
      "  846/3200: episode: 31, duration: 0.113s, episode steps: 35, steps per second: 309, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.019 [-0.603, 1.014], loss: 0.302136, mean_absolute_error: 3.622185, mean_q: 7.045794\n",
      "  886/3200: episode: 32, duration: 0.126s, episode steps: 40, steps per second: 317, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.081 [-0.812, 2.083], loss: 0.386359, mean_absolute_error: 3.768752, mean_q: 7.314650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  962/3200: episode: 33, duration: 0.239s, episode steps: 76, steps per second: 318, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.051 [-1.441, 1.340], loss: 0.302551, mean_absolute_error: 4.002175, mean_q: 7.863830\n",
      " 1053/3200: episode: 34, duration: 0.291s, episode steps: 91, steps per second: 313, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.226 [-1.720, 1.695], loss: 0.426884, mean_absolute_error: 4.335735, mean_q: 8.489305\n",
      " 1129/3200: episode: 35, duration: 0.244s, episode steps: 76, steps per second: 312, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.115 [-1.720, 1.674], loss: 0.349334, mean_absolute_error: 4.584423, mean_q: 9.086555\n",
      " 1186/3200: episode: 36, duration: 0.179s, episode steps: 57, steps per second: 318, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.039 [-1.704, 1.166], loss: 0.291945, mean_absolute_error: 4.891210, mean_q: 9.746830\n",
      " 1277/3200: episode: 37, duration: 0.284s, episode steps: 91, steps per second: 320, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.073 [-1.278, 1.301], loss: 0.419813, mean_absolute_error: 5.180714, mean_q: 10.331476\n",
      " 1319/3200: episode: 38, duration: 0.133s, episode steps: 42, steps per second: 315, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.791, 1.250], loss: 0.465255, mean_absolute_error: 5.458228, mean_q: 10.882476\n",
      " 1461/3200: episode: 39, duration: 0.453s, episode steps: 142, steps per second: 313, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.058 [-1.528, 1.647], loss: 0.474612, mean_absolute_error: 5.826397, mean_q: 11.647607\n",
      " 1606/3200: episode: 40, duration: 0.449s, episode steps: 145, steps per second: 323, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.041 [-1.344, 1.336], loss: 0.495734, mean_absolute_error: 6.408646, mean_q: 12.894839\n",
      " 1806/3200: episode: 41, duration: 0.619s, episode steps: 200, steps per second: 323, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.173 [-1.091, 1.684], loss: 0.462492, mean_absolute_error: 7.120741, mean_q: 14.403490\n",
      " 2006/3200: episode: 42, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.258 [-1.227, 2.253], loss: 0.515597, mean_absolute_error: 7.936899, mean_q: 16.133495\n",
      " 2206/3200: episode: 43, duration: 0.653s, episode steps: 200, steps per second: 306, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.094 [-1.507, 1.536], loss: 0.466819, mean_absolute_error: 8.800211, mean_q: 17.971035\n",
      " 2385/3200: episode: 44, duration: 0.575s, episode steps: 179, steps per second: 311, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.288 [-2.047, 1.175], loss: 0.665352, mean_absolute_error: 9.703229, mean_q: 19.796179\n",
      " 2585/3200: episode: 45, duration: 0.742s, episode steps: 200, steps per second: 270, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.255 [-1.477, 1.831], loss: 0.721128, mean_absolute_error: 10.593252, mean_q: 21.652561\n",
      " 2785/3200: episode: 46, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.051 [-1.290, 1.081], loss: 0.784800, mean_absolute_error: 11.519992, mean_q: 23.522373\n",
      " 2982/3200: episode: 47, duration: 1.096s, episode steps: 197, steps per second: 180, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.214 [-1.496, 1.136], loss: 1.100897, mean_absolute_error: 12.509510, mean_q: 25.544302\n",
      " 3149/3200: episode: 48, duration: 1.032s, episode steps: 167, steps per second: 162, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.277 [-1.823, 0.863], loss: 1.049378, mean_absolute_error: 13.425343, mean_q: 27.423056\n",
      "done, took 12.478 seconds\n",
      "TEMPO:  1561719317.6446831\n",
      "Training for 11000 steps ...\n",
      "    11/11000: episode: 1, duration: 0.172s, episode steps: 11, steps per second: 64, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.162 [-2.902, 1.728], mean_best_reward: --\n",
      "    23/11000: episode: 2, duration: 0.021s, episode steps: 12, steps per second: 585, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.194, 2.042], mean_best_reward: --\n",
      "    36/11000: episode: 3, duration: 0.025s, episode steps: 13, steps per second: 518, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.080 [-1.798, 2.755], mean_best_reward: --\n",
      "   127/11000: episode: 4, duration: 0.144s, episode steps: 91, steps per second: 633, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.050 [-0.908, 1.119], mean_best_reward: --\n",
      "   138/11000: episode: 5, duration: 0.016s, episode steps: 11, steps per second: 694, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.274, 1.369], mean_best_reward: --\n",
      "   151/11000: episode: 6, duration: 0.017s, episode steps: 13, steps per second: 755, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.120 [-1.556, 0.823], mean_best_reward: --\n",
      "   175/11000: episode: 7, duration: 0.032s, episode steps: 24, steps per second: 742, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.057 [-0.938, 1.371], mean_best_reward: --\n",
      "   211/11000: episode: 8, duration: 0.048s, episode steps: 36, steps per second: 751, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.043 [-2.264, 1.239], mean_best_reward: --\n",
      "   229/11000: episode: 9, duration: 0.018s, episode steps: 18, steps per second: 987, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.609, 1.009], mean_best_reward: --\n",
      "   244/11000: episode: 10, duration: 0.016s, episode steps: 15, steps per second: 947, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.099 [-0.809, 1.340], mean_best_reward: --\n",
      "   258/11000: episode: 11, duration: 0.015s, episode steps: 14, steps per second: 923, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.112 [-2.001, 1.161], mean_best_reward: --\n",
      "   283/11000: episode: 12, duration: 0.028s, episode steps: 25, steps per second: 883, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.094 [-0.585, 1.209], mean_best_reward: --\n",
      "   298/11000: episode: 13, duration: 0.017s, episode steps: 15, steps per second: 888, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.109 [-0.751, 1.230], mean_best_reward: --\n",
      "   318/11000: episode: 14, duration: 0.021s, episode steps: 20, steps per second: 960, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.071 [-0.956, 1.535], mean_best_reward: --\n",
      "   335/11000: episode: 15, duration: 0.019s, episode steps: 17, steps per second: 893, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.090 [-2.879, 1.747], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   353/11000: episode: 16, duration: 0.021s, episode steps: 18, steps per second: 870, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.096 [-1.790, 0.948], mean_best_reward: --\n",
      "   369/11000: episode: 17, duration: 0.017s, episode steps: 16, steps per second: 956, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.080 [-1.177, 1.992], mean_best_reward: --\n",
      "   380/11000: episode: 18, duration: 0.012s, episode steps: 11, steps per second: 927, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.738, 2.794], mean_best_reward: --\n",
      "   392/11000: episode: 19, duration: 0.012s, episode steps: 12, steps per second: 979, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.115 [-1.227, 2.081], mean_best_reward: --\n",
      "   412/11000: episode: 20, duration: 0.019s, episode steps: 20, steps per second: 1079, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.049 [-1.365, 2.135], mean_best_reward: --\n",
      "   422/11000: episode: 21, duration: 0.011s, episode steps: 10, steps per second: 924, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.176, 1.350], mean_best_reward: --\n",
      "   441/11000: episode: 22, duration: 0.019s, episode steps: 19, steps per second: 1003, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.091 [-2.400, 1.366], mean_best_reward: --\n",
      "   491/11000: episode: 23, duration: 0.048s, episode steps: 50, steps per second: 1052, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.008 [-1.102, 1.270], mean_best_reward: --\n",
      "   506/11000: episode: 24, duration: 0.015s, episode steps: 15, steps per second: 997, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.812, 1.396], mean_best_reward: --\n",
      "   519/11000: episode: 25, duration: 0.012s, episode steps: 13, steps per second: 1097, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.961, 1.626], mean_best_reward: --\n",
      "   528/11000: episode: 26, duration: 0.012s, episode steps: 9, steps per second: 734, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.168 [-2.348, 1.355], mean_best_reward: --\n",
      "   543/11000: episode: 27, duration: 0.016s, episode steps: 15, steps per second: 965, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.111 [-1.816, 0.955], mean_best_reward: --\n",
      "   558/11000: episode: 28, duration: 0.018s, episode steps: 15, steps per second: 820, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-0.791, 1.501], mean_best_reward: --\n",
      "   569/11000: episode: 29, duration: 0.012s, episode steps: 11, steps per second: 940, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.107 [-1.366, 0.792], mean_best_reward: --\n",
      "   579/11000: episode: 30, duration: 0.012s, episode steps: 10, steps per second: 824, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.001, 1.169], mean_best_reward: --\n",
      "   592/11000: episode: 31, duration: 0.015s, episode steps: 13, steps per second: 895, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-0.987, 1.772], mean_best_reward: --\n",
      "   614/11000: episode: 32, duration: 0.019s, episode steps: 22, steps per second: 1129, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.094 [-2.641, 1.526], mean_best_reward: --\n",
      "   629/11000: episode: 33, duration: 0.014s, episode steps: 15, steps per second: 1043, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.083 [-1.553, 0.987], mean_best_reward: --\n",
      "   641/11000: episode: 34, duration: 0.012s, episode steps: 12, steps per second: 991, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.188, 1.346], mean_best_reward: --\n",
      "   668/11000: episode: 35, duration: 0.025s, episode steps: 27, steps per second: 1064, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.103 [-0.605, 1.595], mean_best_reward: --\n",
      "   697/11000: episode: 36, duration: 0.032s, episode steps: 29, steps per second: 900, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.102 [-1.088, 0.709], mean_best_reward: --\n",
      "   727/11000: episode: 37, duration: 0.070s, episode steps: 30, steps per second: 427, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.037 [-1.068, 1.327], mean_best_reward: --\n",
      "   738/11000: episode: 38, duration: 0.018s, episode steps: 11, steps per second: 626, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.497, 1.574], mean_best_reward: --\n",
      "   746/11000: episode: 39, duration: 0.011s, episode steps: 8, steps per second: 751, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.538, 2.552], mean_best_reward: --\n",
      "   778/11000: episode: 40, duration: 0.041s, episode steps: 32, steps per second: 788, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.073 [-0.556, 1.375], mean_best_reward: --\n",
      "   793/11000: episode: 41, duration: 0.016s, episode steps: 15, steps per second: 909, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.998, 1.829], mean_best_reward: --\n",
      "   801/11000: episode: 42, duration: 0.009s, episode steps: 8, steps per second: 870, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.547, 2.570], mean_best_reward: --\n",
      "   817/11000: episode: 43, duration: 0.022s, episode steps: 16, steps per second: 739, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.097 [-1.164, 0.741], mean_best_reward: --\n",
      "   860/11000: episode: 44, duration: 0.050s, episode steps: 43, steps per second: 863, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.122 [-0.792, 1.975], mean_best_reward: --\n",
      "   872/11000: episode: 45, duration: 0.017s, episode steps: 12, steps per second: 694, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-1.815, 1.148], mean_best_reward: --\n",
      "   884/11000: episode: 46, duration: 0.020s, episode steps: 12, steps per second: 603, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.422, 2.249], mean_best_reward: --\n",
      "   895/11000: episode: 47, duration: 0.012s, episode steps: 11, steps per second: 924, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.117 [-1.749, 2.806], mean_best_reward: --\n",
      "   909/11000: episode: 48, duration: 0.015s, episode steps: 14, steps per second: 934, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.083 [-1.552, 1.021], mean_best_reward: --\n",
      "   921/11000: episode: 49, duration: 0.014s, episode steps: 12, steps per second: 850, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.123 [-1.445, 0.784], mean_best_reward: --\n",
      "   932/11000: episode: 50, duration: 0.011s, episode steps: 11, steps per second: 980, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.138 [-1.353, 2.277], mean_best_reward: --\n",
      "   951/11000: episode: 51, duration: 0.020s, episode steps: 19, steps per second: 971, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.065 [-0.829, 1.444], mean_best_reward: --\n",
      "   972/11000: episode: 52, duration: 0.026s, episode steps: 21, steps per second: 817, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.012 [-2.024, 1.401], mean_best_reward: --\n",
      "   985/11000: episode: 53, duration: 0.013s, episode steps: 13, steps per second: 1000, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.092 [-1.318, 1.945], mean_best_reward: --\n",
      "  1000/11000: episode: 54, duration: 0.018s, episode steps: 15, steps per second: 853, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-2.760, 1.709], mean_best_reward: --\n",
      "  1008/11000: episode: 55, duration: 0.011s, episode steps: 8, steps per second: 707, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.543, 2.560], mean_best_reward: --\n",
      "  1029/11000: episode: 56, duration: 0.024s, episode steps: 21, steps per second: 891, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.113 [-1.031, 0.569], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1066/11000: episode: 57, duration: 0.038s, episode steps: 37, steps per second: 982, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.595 [0.000, 1.000], mean observation: -0.004 [-2.254, 1.573], mean_best_reward: --\n",
      "  1078/11000: episode: 58, duration: 0.015s, episode steps: 12, steps per second: 825, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.799, 1.285], mean_best_reward: --\n",
      "  1091/11000: episode: 59, duration: 0.015s, episode steps: 13, steps per second: 886, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.110 [-1.380, 2.333], mean_best_reward: --\n",
      "  1111/11000: episode: 60, duration: 0.020s, episode steps: 20, steps per second: 1003, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-1.196, 0.430], mean_best_reward: --\n",
      "  1123/11000: episode: 61, duration: 0.013s, episode steps: 12, steps per second: 906, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.110 [-2.568, 1.595], mean_best_reward: --\n",
      "  1138/11000: episode: 62, duration: 0.016s, episode steps: 15, steps per second: 924, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.097 [-2.237, 1.337], mean_best_reward: --\n",
      "  1146/11000: episode: 63, duration: 0.010s, episode steps: 8, steps per second: 786, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.533, 1.550], mean_best_reward: --\n",
      "  1195/11000: episode: 64, duration: 0.047s, episode steps: 49, steps per second: 1053, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.036 [-0.827, 1.082], mean_best_reward: --\n",
      "  1210/11000: episode: 65, duration: 0.016s, episode steps: 15, steps per second: 928, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.076 [-2.460, 1.582], mean_best_reward: --\n",
      "  1226/11000: episode: 66, duration: 0.017s, episode steps: 16, steps per second: 948, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.073 [-0.984, 1.615], mean_best_reward: --\n",
      "  1243/11000: episode: 67, duration: 0.017s, episode steps: 17, steps per second: 996, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.091 [-2.021, 1.188], mean_best_reward: --\n",
      "  1262/11000: episode: 68, duration: 0.023s, episode steps: 19, steps per second: 829, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.086 [-1.034, 1.916], mean_best_reward: --\n",
      "  1292/11000: episode: 69, duration: 0.028s, episode steps: 30, steps per second: 1079, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.015 [-1.153, 0.762], mean_best_reward: --\n",
      "  1303/11000: episode: 70, duration: 0.013s, episode steps: 11, steps per second: 874, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-0.958, 1.589], mean_best_reward: --\n",
      "  1314/11000: episode: 71, duration: 0.011s, episode steps: 11, steps per second: 985, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.374, 2.312], mean_best_reward: --\n",
      "  1331/11000: episode: 72, duration: 0.017s, episode steps: 17, steps per second: 974, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.074 [-2.276, 1.384], mean_best_reward: --\n",
      "  1350/11000: episode: 73, duration: 0.018s, episode steps: 19, steps per second: 1075, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.063 [-1.389, 2.215], mean_best_reward: --\n",
      "  1377/11000: episode: 74, duration: 0.025s, episode steps: 27, steps per second: 1064, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.082 [-1.583, 0.774], mean_best_reward: --\n",
      "  1410/11000: episode: 75, duration: 0.032s, episode steps: 33, steps per second: 1018, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.032 [-0.924, 1.357], mean_best_reward: --\n",
      "  1418/11000: episode: 76, duration: 0.009s, episode steps: 8, steps per second: 864, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.151 [-1.325, 2.208], mean_best_reward: --\n",
      "  1438/11000: episode: 77, duration: 0.020s, episode steps: 20, steps per second: 996, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.056 [-2.991, 1.928], mean_best_reward: --\n",
      "  1458/11000: episode: 78, duration: 0.020s, episode steps: 20, steps per second: 978, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.077 [-0.813, 1.290], mean_best_reward: --\n",
      "  1469/11000: episode: 79, duration: 0.016s, episode steps: 11, steps per second: 686, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.111 [-2.409, 1.589], mean_best_reward: --\n",
      "  1489/11000: episode: 80, duration: 0.022s, episode steps: 20, steps per second: 900, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.073 [-1.600, 2.584], mean_best_reward: --\n",
      "  1497/11000: episode: 81, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.537, 2.505], mean_best_reward: --\n",
      "  1515/11000: episode: 82, duration: 0.019s, episode steps: 18, steps per second: 958, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.068 [-2.002, 3.044], mean_best_reward: --\n",
      "  1536/11000: episode: 83, duration: 0.023s, episode steps: 21, steps per second: 896, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.066 [-1.379, 2.275], mean_best_reward: --\n",
      "  1576/11000: episode: 84, duration: 0.056s, episode steps: 40, steps per second: 709, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.024 [-1.678, 1.961], mean_best_reward: --\n",
      "  1597/11000: episode: 85, duration: 0.024s, episode steps: 21, steps per second: 860, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.047 [-1.772, 1.149], mean_best_reward: --\n",
      "  1606/11000: episode: 86, duration: 0.010s, episode steps: 9, steps per second: 913, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.157 [-1.325, 2.241], mean_best_reward: --\n",
      "  1629/11000: episode: 87, duration: 0.035s, episode steps: 23, steps per second: 663, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.052 [-1.428, 0.770], mean_best_reward: --\n",
      "  1648/11000: episode: 88, duration: 0.029s, episode steps: 19, steps per second: 665, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.087 [-1.106, 0.580], mean_best_reward: --\n",
      "  1665/11000: episode: 89, duration: 0.024s, episode steps: 17, steps per second: 718, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.105 [-0.750, 1.470], mean_best_reward: --\n",
      "  1684/11000: episode: 90, duration: 0.029s, episode steps: 19, steps per second: 649, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.069 [-2.025, 1.317], mean_best_reward: --\n",
      "  1705/11000: episode: 91, duration: 0.043s, episode steps: 21, steps per second: 488, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.076 [-1.504, 0.758], mean_best_reward: --\n",
      "  1714/11000: episode: 92, duration: 0.021s, episode steps: 9, steps per second: 421, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-1.216, 1.871], mean_best_reward: --\n",
      "  1730/11000: episode: 93, duration: 0.030s, episode steps: 16, steps per second: 539, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.079 [-1.388, 0.823], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1750/11000: episode: 94, duration: 0.033s, episode steps: 20, steps per second: 612, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.086 [-0.967, 1.590], mean_best_reward: --\n",
      "  1767/11000: episode: 95, duration: 0.030s, episode steps: 17, steps per second: 575, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.080 [-1.227, 0.813], mean_best_reward: --\n",
      "  1779/11000: episode: 96, duration: 0.020s, episode steps: 12, steps per second: 606, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.131 [-1.571, 2.636], mean_best_reward: --\n",
      "  1791/11000: episode: 97, duration: 0.026s, episode steps: 12, steps per second: 462, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.098 [-1.000, 1.553], mean_best_reward: --\n",
      "  1817/11000: episode: 98, duration: 0.045s, episode steps: 26, steps per second: 580, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.002 [-1.600, 2.236], mean_best_reward: --\n",
      "  1831/11000: episode: 99, duration: 0.031s, episode steps: 14, steps per second: 453, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.098 [-1.164, 1.971], mean_best_reward: --\n",
      "  1851/11000: episode: 100, duration: 0.031s, episode steps: 20, steps per second: 655, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.053 [-1.944, 2.948], mean_best_reward: --\n",
      "  1871/11000: episode: 101, duration: 0.036s, episode steps: 20, steps per second: 558, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.075 [-1.904, 1.138], mean_best_reward: --\n",
      "  1882/11000: episode: 102, duration: 0.019s, episode steps: 11, steps per second: 591, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.116 [-1.775, 2.820], mean_best_reward: --\n",
      "  1894/11000: episode: 103, duration: 0.021s, episode steps: 12, steps per second: 570, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.133 [-1.526, 2.553], mean_best_reward: --\n",
      "  1924/11000: episode: 104, duration: 0.055s, episode steps: 30, steps per second: 550, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.091 [-0.840, 2.048], mean_best_reward: --\n",
      "  1940/11000: episode: 105, duration: 0.029s, episode steps: 16, steps per second: 555, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.078 [-1.512, 2.342], mean_best_reward: --\n",
      "  1953/11000: episode: 106, duration: 0.024s, episode steps: 13, steps per second: 549, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.119 [-0.785, 1.286], mean_best_reward: --\n",
      "  1966/11000: episode: 107, duration: 0.025s, episode steps: 13, steps per second: 517, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.085 [-1.727, 2.634], mean_best_reward: --\n",
      "  1996/11000: episode: 108, duration: 0.048s, episode steps: 30, steps per second: 631, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.041 [-1.479, 0.779], mean_best_reward: --\n",
      "  2014/11000: episode: 109, duration: 0.025s, episode steps: 18, steps per second: 733, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.355, 2.303], mean_best_reward: --\n",
      "  2024/11000: episode: 110, duration: 0.023s, episode steps: 10, steps per second: 443, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.157 [-2.614, 1.531], mean_best_reward: --\n",
      "  2035/11000: episode: 111, duration: 0.023s, episode steps: 11, steps per second: 477, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.133 [-1.850, 0.940], mean_best_reward: --\n",
      "  2044/11000: episode: 112, duration: 0.022s, episode steps: 9, steps per second: 412, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.165 [-1.148, 1.974], mean_best_reward: --\n",
      "  2055/11000: episode: 113, duration: 0.022s, episode steps: 11, steps per second: 507, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.143 [-1.858, 0.949], mean_best_reward: --\n",
      "  2066/11000: episode: 114, duration: 0.024s, episode steps: 11, steps per second: 451, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.135 [-1.383, 2.348], mean_best_reward: --\n",
      "  2078/11000: episode: 115, duration: 0.028s, episode steps: 12, steps per second: 431, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.064, 1.200], mean_best_reward: --\n",
      "  2180/11000: episode: 116, duration: 0.204s, episode steps: 102, steps per second: 500, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.250 [-1.329, 1.830], mean_best_reward: --\n",
      "  2203/11000: episode: 117, duration: 0.032s, episode steps: 23, steps per second: 719, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.028 [-2.024, 1.391], mean_best_reward: --\n",
      "  2221/11000: episode: 118, duration: 0.021s, episode steps: 18, steps per second: 878, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.065 [-2.252, 1.513], mean_best_reward: --\n",
      "  2285/11000: episode: 119, duration: 0.080s, episode steps: 64, steps per second: 804, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: 0.182 [-2.238, 2.432], mean_best_reward: --\n",
      "  2300/11000: episode: 120, duration: 0.031s, episode steps: 15, steps per second: 483, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.083 [-1.555, 2.269], mean_best_reward: --\n",
      "  2321/11000: episode: 121, duration: 0.042s, episode steps: 21, steps per second: 501, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.087 [-1.501, 0.606], mean_best_reward: --\n",
      "  2336/11000: episode: 122, duration: 0.032s, episode steps: 15, steps per second: 472, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.081 [-1.810, 2.836], mean_best_reward: --\n",
      "  2348/11000: episode: 123, duration: 0.021s, episode steps: 12, steps per second: 574, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.087 [-2.027, 1.222], mean_best_reward: --\n",
      "  2357/11000: episode: 124, duration: 0.018s, episode steps: 9, steps per second: 514, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.799, 2.794], mean_best_reward: --\n",
      "  2369/11000: episode: 125, duration: 0.029s, episode steps: 12, steps per second: 408, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.109 [-1.952, 1.127], mean_best_reward: --\n",
      "  2381/11000: episode: 126, duration: 0.023s, episode steps: 12, steps per second: 523, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.106 [-0.824, 1.264], mean_best_reward: --\n",
      "  2412/11000: episode: 127, duration: 0.044s, episode steps: 31, steps per second: 711, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.060 [-1.532, 0.818], mean_best_reward: --\n",
      "  2426/11000: episode: 128, duration: 0.025s, episode steps: 14, steps per second: 571, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.116 [-3.081, 1.913], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2452/11000: episode: 129, duration: 0.051s, episode steps: 26, steps per second: 513, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.008 [-1.602, 2.428], mean_best_reward: --\n",
      "  2471/11000: episode: 130, duration: 0.039s, episode steps: 19, steps per second: 487, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.063 [-1.148, 1.726], mean_best_reward: --\n",
      "  2489/11000: episode: 131, duration: 0.028s, episode steps: 18, steps per second: 633, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.020 [-3.277, 2.340], mean_best_reward: --\n",
      "  2518/11000: episode: 132, duration: 0.043s, episode steps: 29, steps per second: 669, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.052 [-1.190, 2.080], mean_best_reward: --\n",
      "  2532/11000: episode: 133, duration: 0.025s, episode steps: 14, steps per second: 569, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.123 [-0.747, 1.523], mean_best_reward: --\n",
      "  2560/11000: episode: 134, duration: 0.035s, episode steps: 28, steps per second: 795, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.138 [-0.404, 0.774], mean_best_reward: --\n",
      "  2600/11000: episode: 135, duration: 0.060s, episode steps: 40, steps per second: 666, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.482, 1.326], mean_best_reward: --\n",
      "  2611/11000: episode: 136, duration: 0.020s, episode steps: 11, steps per second: 558, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-1.142, 1.828], mean_best_reward: --\n",
      "  2625/11000: episode: 137, duration: 0.021s, episode steps: 14, steps per second: 683, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.077 [-1.586, 1.018], mean_best_reward: --\n",
      "  2635/11000: episode: 138, duration: 0.015s, episode steps: 10, steps per second: 650, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.550, 2.590], mean_best_reward: --\n",
      "  2653/11000: episode: 139, duration: 0.024s, episode steps: 18, steps per second: 754, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.089 [-1.526, 0.787], mean_best_reward: --\n",
      "  2665/11000: episode: 140, duration: 0.016s, episode steps: 12, steps per second: 743, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.110 [-1.389, 2.213], mean_best_reward: --\n",
      "  2679/11000: episode: 141, duration: 0.017s, episode steps: 14, steps per second: 807, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.071 [-2.368, 1.546], mean_best_reward: --\n",
      "  2692/11000: episode: 142, duration: 0.014s, episode steps: 13, steps per second: 903, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.094 [-1.208, 2.013], mean_best_reward: --\n",
      "  2703/11000: episode: 143, duration: 0.011s, episode steps: 11, steps per second: 994, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.104 [-1.181, 1.904], mean_best_reward: --\n",
      "  2726/11000: episode: 144, duration: 0.027s, episode steps: 23, steps per second: 854, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.058 [-0.619, 1.172], mean_best_reward: --\n",
      "  2747/11000: episode: 145, duration: 0.031s, episode steps: 21, steps per second: 685, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.093 [-0.661, 1.487], mean_best_reward: --\n",
      "  2767/11000: episode: 146, duration: 0.028s, episode steps: 20, steps per second: 717, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.072 [-1.234, 0.604], mean_best_reward: --\n",
      "  2777/11000: episode: 147, duration: 0.013s, episode steps: 10, steps per second: 794, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.100 [-1.579, 2.411], mean_best_reward: --\n",
      "  2798/11000: episode: 148, duration: 0.031s, episode steps: 21, steps per second: 686, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.086 [-1.526, 0.788], mean_best_reward: --\n",
      "  2840/11000: episode: 149, duration: 0.039s, episode steps: 42, steps per second: 1073, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.028 [-0.603, 1.220], mean_best_reward: --\n",
      "  2851/11000: episode: 150, duration: 0.013s, episode steps: 11, steps per second: 838, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.121 [-1.376, 2.329], mean_best_reward: --\n",
      "  2867/11000: episode: 151, duration: 0.016s, episode steps: 16, steps per second: 976, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.080 [-1.958, 3.023], mean_best_reward: 41.500000\n",
      "  2889/11000: episode: 152, duration: 0.021s, episode steps: 22, steps per second: 1041, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.088 [-0.973, 1.926], mean_best_reward: --\n",
      "  2906/11000: episode: 153, duration: 0.017s, episode steps: 17, steps per second: 1011, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.084 [-1.031, 1.915], mean_best_reward: --\n",
      "  2920/11000: episode: 154, duration: 0.016s, episode steps: 14, steps per second: 855, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.080 [-0.820, 1.436], mean_best_reward: --\n",
      "  2945/11000: episode: 155, duration: 0.029s, episode steps: 25, steps per second: 859, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.104 [-0.796, 1.520], mean_best_reward: --\n",
      "  2958/11000: episode: 156, duration: 0.018s, episode steps: 13, steps per second: 736, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.089 [-0.823, 1.347], mean_best_reward: --\n",
      "  2977/11000: episode: 157, duration: 0.022s, episode steps: 19, steps per second: 879, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.119 [-0.654, 1.590], mean_best_reward: --\n",
      "  3011/11000: episode: 158, duration: 0.056s, episode steps: 34, steps per second: 602, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.272, 0.822], mean_best_reward: --\n",
      "  3026/11000: episode: 159, duration: 0.030s, episode steps: 15, steps per second: 500, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-1.223, 2.058], mean_best_reward: --\n",
      "  3037/11000: episode: 160, duration: 0.015s, episode steps: 11, steps per second: 732, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-0.971, 1.738], mean_best_reward: --\n",
      "  3052/11000: episode: 161, duration: 0.021s, episode steps: 15, steps per second: 705, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.102 [-0.902, 0.632], mean_best_reward: --\n",
      "  3075/11000: episode: 162, duration: 0.029s, episode steps: 23, steps per second: 792, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.049 [-0.994, 1.474], mean_best_reward: --\n",
      "  3089/11000: episode: 163, duration: 0.022s, episode steps: 14, steps per second: 650, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-1.034, 1.722], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3108/11000: episode: 164, duration: 0.021s, episode steps: 19, steps per second: 908, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.080 [-0.798, 1.238], mean_best_reward: --\n",
      "  3133/11000: episode: 165, duration: 0.028s, episode steps: 25, steps per second: 881, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.050 [-1.354, 2.187], mean_best_reward: --\n",
      "  3159/11000: episode: 166, duration: 0.027s, episode steps: 26, steps per second: 955, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.079 [-0.840, 1.712], mean_best_reward: --\n",
      "  3180/11000: episode: 167, duration: 0.030s, episode steps: 21, steps per second: 699, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.077 [-0.644, 1.379], mean_best_reward: --\n",
      "  3204/11000: episode: 168, duration: 0.026s, episode steps: 24, steps per second: 933, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.070 [-0.809, 1.730], mean_best_reward: --\n",
      "  3228/11000: episode: 169, duration: 0.022s, episode steps: 24, steps per second: 1085, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.056 [-1.338, 2.077], mean_best_reward: --\n",
      "  3252/11000: episode: 170, duration: 0.026s, episode steps: 24, steps per second: 938, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.076 [-1.004, 1.841], mean_best_reward: --\n",
      "  3272/11000: episode: 171, duration: 0.025s, episode steps: 20, steps per second: 804, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.755, 1.575], mean_best_reward: --\n",
      "  3316/11000: episode: 172, duration: 0.050s, episode steps: 44, steps per second: 872, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.126 [-1.511, 0.686], mean_best_reward: --\n",
      "  3338/11000: episode: 173, duration: 0.026s, episode steps: 22, steps per second: 845, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.070 [-0.946, 1.521], mean_best_reward: --\n",
      "  3370/11000: episode: 174, duration: 0.039s, episode steps: 32, steps per second: 811, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: 0.012 [-1.570, 2.154], mean_best_reward: --\n",
      "  3404/11000: episode: 175, duration: 0.041s, episode steps: 34, steps per second: 823, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.001 [-0.822, 1.150], mean_best_reward: --\n",
      "  3434/11000: episode: 176, duration: 0.035s, episode steps: 30, steps per second: 862, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.020 [-0.982, 1.338], mean_best_reward: --\n",
      "  3455/11000: episode: 177, duration: 0.025s, episode steps: 21, steps per second: 857, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.392, 2.549], mean_best_reward: --\n",
      "  3478/11000: episode: 178, duration: 0.030s, episode steps: 23, steps per second: 756, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.093 [-0.753, 1.517], mean_best_reward: --\n",
      "  3501/11000: episode: 179, duration: 0.039s, episode steps: 23, steps per second: 594, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.108 [-0.769, 1.738], mean_best_reward: --\n",
      "  3515/11000: episode: 180, duration: 0.016s, episode steps: 14, steps per second: 857, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.099 [-0.561, 1.213], mean_best_reward: --\n",
      "  3533/11000: episode: 181, duration: 0.019s, episode steps: 18, steps per second: 932, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.085 [-0.938, 1.709], mean_best_reward: --\n",
      "  3565/11000: episode: 182, duration: 0.044s, episode steps: 32, steps per second: 725, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.062 [-0.619, 1.341], mean_best_reward: --\n",
      "  3601/11000: episode: 183, duration: 0.037s, episode steps: 36, steps per second: 965, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.031 [-1.202, 1.410], mean_best_reward: --\n",
      "  3627/11000: episode: 184, duration: 0.024s, episode steps: 26, steps per second: 1068, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.077 [-0.738, 1.136], mean_best_reward: --\n",
      "  3644/11000: episode: 185, duration: 0.017s, episode steps: 17, steps per second: 1005, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.110 [-0.997, 1.522], mean_best_reward: --\n",
      "  3668/11000: episode: 186, duration: 0.022s, episode steps: 24, steps per second: 1090, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.085 [-0.942, 1.526], mean_best_reward: --\n",
      "  3691/11000: episode: 187, duration: 0.025s, episode steps: 23, steps per second: 931, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.112 [-0.563, 1.481], mean_best_reward: --\n",
      "  3726/11000: episode: 188, duration: 0.035s, episode steps: 35, steps per second: 1009, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.063 [-1.442, 0.774], mean_best_reward: --\n",
      "  3744/11000: episode: 189, duration: 0.018s, episode steps: 18, steps per second: 1003, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.118 [-0.763, 1.475], mean_best_reward: --\n",
      "  3760/11000: episode: 190, duration: 0.018s, episode steps: 16, steps per second: 912, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.158, 0.587], mean_best_reward: --\n",
      "  3770/11000: episode: 191, duration: 0.011s, episode steps: 10, steps per second: 892, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-0.989, 1.694], mean_best_reward: --\n",
      "  3786/11000: episode: 192, duration: 0.017s, episode steps: 16, steps per second: 922, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.076 [-1.124, 1.697], mean_best_reward: --\n",
      "  3801/11000: episode: 193, duration: 0.014s, episode steps: 15, steps per second: 1070, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.979, 1.482], mean_best_reward: --\n",
      "  3815/11000: episode: 194, duration: 0.014s, episode steps: 14, steps per second: 988, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.099 [-1.147, 2.042], mean_best_reward: --\n",
      "  3841/11000: episode: 195, duration: 0.023s, episode steps: 26, steps per second: 1108, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.136 [-0.557, 1.016], mean_best_reward: --\n",
      "  3870/11000: episode: 196, duration: 0.028s, episode steps: 29, steps per second: 1048, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.078 [-0.950, 1.879], mean_best_reward: --\n",
      "  3887/11000: episode: 197, duration: 0.020s, episode steps: 17, steps per second: 860, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.125 [-0.811, 0.365], mean_best_reward: --\n",
      "  3896/11000: episode: 198, duration: 0.010s, episode steps: 9, steps per second: 930, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.369, 2.252], mean_best_reward: --\n",
      "  3905/11000: episode: 199, duration: 0.009s, episode steps: 9, steps per second: 1048, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.328, 2.128], mean_best_reward: --\n",
      "  3924/11000: episode: 200, duration: 0.018s, episode steps: 19, steps per second: 1044, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.116 [-0.921, 0.593], mean_best_reward: --\n",
      "  3956/11000: episode: 201, duration: 0.030s, episode steps: 32, steps per second: 1072, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.059 [-0.949, 1.701], mean_best_reward: 75.500000\n",
      "  3980/11000: episode: 202, duration: 0.026s, episode steps: 24, steps per second: 923, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.160, 0.819], mean_best_reward: --\n",
      "  4001/11000: episode: 203, duration: 0.020s, episode steps: 21, steps per second: 1032, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.060 [-1.721, 0.991], mean_best_reward: --\n",
      "  4023/11000: episode: 204, duration: 0.021s, episode steps: 22, steps per second: 1057, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.090 [-0.624, 1.408], mean_best_reward: --\n",
      "  4063/11000: episode: 205, duration: 0.041s, episode steps: 40, steps per second: 983, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.075 [-1.088, 0.744], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4089/11000: episode: 206, duration: 0.032s, episode steps: 26, steps per second: 801, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.009 [-1.496, 1.979], mean_best_reward: --\n",
      "  4098/11000: episode: 207, duration: 0.010s, episode steps: 9, steps per second: 929, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.162 [-1.153, 1.965], mean_best_reward: --\n",
      "  4116/11000: episode: 208, duration: 0.021s, episode steps: 18, steps per second: 840, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.057 [-1.174, 1.750], mean_best_reward: --\n",
      "  4139/11000: episode: 209, duration: 0.023s, episode steps: 23, steps per second: 1008, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.074 [-1.372, 0.794], mean_best_reward: --\n",
      "  4184/11000: episode: 210, duration: 0.046s, episode steps: 45, steps per second: 988, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.106 [-0.863, 1.784], mean_best_reward: --\n",
      "  4205/11000: episode: 211, duration: 0.019s, episode steps: 21, steps per second: 1087, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.072 [-0.631, 1.005], mean_best_reward: --\n",
      "  4223/11000: episode: 212, duration: 0.019s, episode steps: 18, steps per second: 925, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.082 [-1.595, 2.573], mean_best_reward: --\n",
      "  4272/11000: episode: 213, duration: 0.049s, episode steps: 49, steps per second: 990, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.026 [-1.504, 1.199], mean_best_reward: --\n",
      "  4306/11000: episode: 214, duration: 0.035s, episode steps: 34, steps per second: 958, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.012 [-1.583, 2.110], mean_best_reward: --\n",
      "  4336/11000: episode: 215, duration: 0.030s, episode steps: 30, steps per second: 1012, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.103 [-0.715, 0.404], mean_best_reward: --\n",
      "  4368/11000: episode: 216, duration: 0.032s, episode steps: 32, steps per second: 1012, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.656 [0.000, 1.000], mean observation: 0.023 [-2.574, 2.067], mean_best_reward: --\n",
      "  4385/11000: episode: 217, duration: 0.017s, episode steps: 17, steps per second: 998, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.105 [-0.569, 1.111], mean_best_reward: --\n",
      "  4455/11000: episode: 218, duration: 0.063s, episode steps: 70, steps per second: 1109, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: 0.281 [-1.866, 2.264], mean_best_reward: --\n",
      "  4479/11000: episode: 219, duration: 0.022s, episode steps: 24, steps per second: 1068, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.063 [-0.983, 1.683], mean_best_reward: --\n",
      "  4489/11000: episode: 220, duration: 0.010s, episode steps: 10, steps per second: 980, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.162 [-1.143, 2.073], mean_best_reward: --\n",
      "  4522/11000: episode: 221, duration: 0.037s, episode steps: 33, steps per second: 896, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.011 [-2.056, 1.557], mean_best_reward: --\n",
      "  4554/11000: episode: 222, duration: 0.031s, episode steps: 32, steps per second: 1038, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.996, 1.614], mean_best_reward: --\n",
      "  4566/11000: episode: 223, duration: 0.013s, episode steps: 12, steps per second: 928, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-1.394, 2.076], mean_best_reward: --\n",
      "  4579/11000: episode: 224, duration: 0.013s, episode steps: 13, steps per second: 986, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.099 [-1.018, 1.579], mean_best_reward: --\n",
      "  4610/11000: episode: 225, duration: 0.034s, episode steps: 31, steps per second: 911, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.079 [-0.602, 1.547], mean_best_reward: --\n",
      "  4633/11000: episode: 226, duration: 0.024s, episode steps: 23, steps per second: 976, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.069 [-1.137, 1.926], mean_best_reward: --\n",
      "  4661/11000: episode: 227, duration: 0.031s, episode steps: 28, steps per second: 913, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.034 [-1.097, 0.795], mean_best_reward: --\n",
      "  4674/11000: episode: 228, duration: 0.013s, episode steps: 13, steps per second: 1015, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.108 [-0.932, 1.573], mean_best_reward: --\n",
      "  4725/11000: episode: 229, duration: 0.054s, episode steps: 51, steps per second: 947, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.107 [-0.376, 0.994], mean_best_reward: --\n",
      "  4736/11000: episode: 230, duration: 0.011s, episode steps: 11, steps per second: 980, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.105 [-1.798, 2.773], mean_best_reward: --\n",
      "  4786/11000: episode: 231, duration: 0.051s, episode steps: 50, steps per second: 979, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.151 [-0.598, 0.974], mean_best_reward: --\n",
      "  4806/11000: episode: 232, duration: 0.019s, episode steps: 20, steps per second: 1042, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.397, 0.788], mean_best_reward: --\n",
      "  4839/11000: episode: 233, duration: 0.036s, episode steps: 33, steps per second: 907, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.062 [-1.860, 1.697], mean_best_reward: --\n",
      "  4863/11000: episode: 234, duration: 0.025s, episode steps: 24, steps per second: 969, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.556, 1.232], mean_best_reward: --\n",
      "  4907/11000: episode: 235, duration: 0.047s, episode steps: 44, steps per second: 941, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: -0.078 [-2.322, 2.617], mean_best_reward: --\n",
      "  4920/11000: episode: 236, duration: 0.015s, episode steps: 13, steps per second: 845, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.083 [-1.353, 2.171], mean_best_reward: --\n",
      "  4951/11000: episode: 237, duration: 0.035s, episode steps: 31, steps per second: 896, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.108 [-0.545, 0.930], mean_best_reward: --\n",
      "  4962/11000: episode: 238, duration: 0.013s, episode steps: 11, steps per second: 860, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.091 [-1.213, 1.825], mean_best_reward: --\n",
      "  4978/11000: episode: 239, duration: 0.017s, episode steps: 16, steps per second: 924, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.763, 1.387], mean_best_reward: --\n",
      "  5006/11000: episode: 240, duration: 0.032s, episode steps: 28, steps per second: 881, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.030 [-1.124, 1.709], mean_best_reward: --\n",
      "  5026/11000: episode: 241, duration: 0.021s, episode steps: 20, steps per second: 956, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.062 [-1.196, 1.711], mean_best_reward: --\n",
      "  5041/11000: episode: 242, duration: 0.016s, episode steps: 15, steps per second: 923, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.111 [-1.219, 0.745], mean_best_reward: --\n",
      "  5061/11000: episode: 243, duration: 0.022s, episode steps: 20, steps per second: 915, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.010 [-3.215, 2.302], mean_best_reward: --\n",
      "  5098/11000: episode: 244, duration: 0.037s, episode steps: 37, steps per second: 1004, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.124 [-0.732, 1.569], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5141/11000: episode: 245, duration: 0.048s, episode steps: 43, steps per second: 888, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.003 [-0.926, 1.369], mean_best_reward: --\n",
      "  5165/11000: episode: 246, duration: 0.024s, episode steps: 24, steps per second: 1005, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.091 [-1.282, 0.795], mean_best_reward: --\n",
      "  5181/11000: episode: 247, duration: 0.015s, episode steps: 16, steps per second: 1045, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.077 [-2.174, 1.415], mean_best_reward: --\n",
      "  5195/11000: episode: 248, duration: 0.015s, episode steps: 14, steps per second: 906, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.100 [-2.021, 1.180], mean_best_reward: --\n",
      "  5245/11000: episode: 249, duration: 0.059s, episode steps: 50, steps per second: 849, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.057 [-0.804, 1.137], mean_best_reward: --\n",
      "  5255/11000: episode: 250, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.117 [-1.406, 2.213], mean_best_reward: --\n",
      "  5351/11000: episode: 251, duration: 0.100s, episode steps: 96, steps per second: 962, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.021 [-0.753, 1.074], mean_best_reward: 86.000000\n",
      "  5379/11000: episode: 252, duration: 0.027s, episode steps: 28, steps per second: 1042, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.095 [-1.326, 0.410], mean_best_reward: --\n",
      "  5410/11000: episode: 253, duration: 0.032s, episode steps: 31, steps per second: 958, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.031 [-1.969, 1.383], mean_best_reward: --\n",
      "  5426/11000: episode: 254, duration: 0.021s, episode steps: 16, steps per second: 762, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.072 [-0.770, 1.264], mean_best_reward: --\n",
      "  5446/11000: episode: 255, duration: 0.028s, episode steps: 20, steps per second: 709, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.063 [-1.522, 1.010], mean_best_reward: --\n",
      "  5481/11000: episode: 256, duration: 0.036s, episode steps: 35, steps per second: 974, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.015 [-1.132, 0.733], mean_best_reward: --\n",
      "  5511/11000: episode: 257, duration: 0.029s, episode steps: 30, steps per second: 1026, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.032 [-1.140, 0.732], mean_best_reward: --\n",
      "  5549/11000: episode: 258, duration: 0.041s, episode steps: 38, steps per second: 921, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.053 [-0.983, 1.439], mean_best_reward: --\n",
      "  5580/11000: episode: 259, duration: 0.029s, episode steps: 31, steps per second: 1054, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.098 [-1.132, 0.396], mean_best_reward: --\n",
      "  5600/11000: episode: 260, duration: 0.022s, episode steps: 20, steps per second: 927, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.059 [-1.504, 0.789], mean_best_reward: --\n",
      "  5638/11000: episode: 261, duration: 0.039s, episode steps: 38, steps per second: 972, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.085 [-0.530, 0.999], mean_best_reward: --\n",
      "  5672/11000: episode: 262, duration: 0.033s, episode steps: 34, steps per second: 1038, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.287, 0.574], mean_best_reward: --\n",
      "  5686/11000: episode: 263, duration: 0.017s, episode steps: 14, steps per second: 839, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.117 [-1.485, 0.784], mean_best_reward: --\n",
      "  5704/11000: episode: 264, duration: 0.019s, episode steps: 18, steps per second: 932, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.641, 1.033], mean_best_reward: --\n",
      "  5743/11000: episode: 265, duration: 0.046s, episode steps: 39, steps per second: 841, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.080 [-0.587, 0.973], mean_best_reward: --\n",
      "  5761/11000: episode: 266, duration: 0.022s, episode steps: 18, steps per second: 835, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.087 [-1.180, 0.821], mean_best_reward: --\n",
      "  5861/11000: episode: 267, duration: 0.097s, episode steps: 100, steps per second: 1029, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.410 [-1.518, 2.793], mean_best_reward: --\n",
      "  5876/11000: episode: 268, duration: 0.015s, episode steps: 15, steps per second: 1012, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.101 [-0.637, 0.957], mean_best_reward: --\n",
      "  5911/11000: episode: 269, duration: 0.034s, episode steps: 35, steps per second: 1016, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.104 [-1.331, 0.571], mean_best_reward: --\n",
      "  5929/11000: episode: 270, duration: 0.017s, episode steps: 18, steps per second: 1057, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.020 [-1.393, 1.832], mean_best_reward: --\n",
      "  5958/11000: episode: 271, duration: 0.028s, episode steps: 29, steps per second: 1047, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.007 [-1.511, 1.186], mean_best_reward: --\n",
      "  6001/11000: episode: 272, duration: 0.042s, episode steps: 43, steps per second: 1025, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.085 [-1.937, 0.811], mean_best_reward: --\n",
      "  6043/11000: episode: 273, duration: 0.039s, episode steps: 42, steps per second: 1086, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.159 [-0.757, 1.174], mean_best_reward: --\n",
      "  6083/11000: episode: 274, duration: 0.041s, episode steps: 40, steps per second: 968, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.263, 0.558], mean_best_reward: --\n",
      "  6095/11000: episode: 275, duration: 0.013s, episode steps: 12, steps per second: 891, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.120 [-2.144, 1.172], mean_best_reward: --\n",
      "  6163/11000: episode: 276, duration: 0.070s, episode steps: 68, steps per second: 967, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.104 [-0.655, 1.740], mean_best_reward: --\n",
      "  6177/11000: episode: 277, duration: 0.015s, episode steps: 14, steps per second: 915, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.085 [-1.575, 0.834], mean_best_reward: --\n",
      "  6192/11000: episode: 278, duration: 0.016s, episode steps: 15, steps per second: 965, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.087 [-1.197, 0.613], mean_best_reward: --\n",
      "  6225/11000: episode: 279, duration: 0.034s, episode steps: 33, steps per second: 977, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.038 [-1.777, 1.317], mean_best_reward: --\n",
      "  6252/11000: episode: 280, duration: 0.027s, episode steps: 27, steps per second: 987, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.067 [-1.089, 0.602], mean_best_reward: --\n",
      "  6293/11000: episode: 281, duration: 0.038s, episode steps: 41, steps per second: 1065, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.066 [-0.902, 0.537], mean_best_reward: --\n",
      "  6311/11000: episode: 282, duration: 0.017s, episode steps: 18, steps per second: 1036, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.086 [-1.451, 0.759], mean_best_reward: --\n",
      "  6330/11000: episode: 283, duration: 0.022s, episode steps: 19, steps per second: 858, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.084 [-1.226, 0.795], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6353/11000: episode: 284, duration: 0.048s, episode steps: 23, steps per second: 476, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.081 [-1.537, 0.771], mean_best_reward: --\n",
      "  6372/11000: episode: 285, duration: 0.044s, episode steps: 19, steps per second: 435, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.091 [-0.976, 1.793], mean_best_reward: --\n",
      "  6438/11000: episode: 286, duration: 0.093s, episode steps: 66, steps per second: 709, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.124 [-1.305, 0.786], mean_best_reward: --\n",
      "  6454/11000: episode: 287, duration: 0.022s, episode steps: 16, steps per second: 737, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.052 [-1.200, 1.650], mean_best_reward: --\n",
      "  6521/11000: episode: 288, duration: 0.073s, episode steps: 67, steps per second: 912, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.074 [-0.668, 1.341], mean_best_reward: --\n",
      "  6590/11000: episode: 289, duration: 0.067s, episode steps: 69, steps per second: 1023, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.179 [-0.840, 1.467], mean_best_reward: --\n",
      "  6603/11000: episode: 290, duration: 0.012s, episode steps: 13, steps per second: 1055, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.126 [-1.330, 0.759], mean_best_reward: --\n",
      "  6676/11000: episode: 291, duration: 0.071s, episode steps: 73, steps per second: 1029, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.055 [-1.736, 0.825], mean_best_reward: --\n",
      "  6694/11000: episode: 292, duration: 0.019s, episode steps: 18, steps per second: 969, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.106 [-0.770, 1.548], mean_best_reward: --\n",
      "  6711/11000: episode: 293, duration: 0.018s, episode steps: 17, steps per second: 957, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.112 [-0.573, 1.180], mean_best_reward: --\n",
      "  6728/11000: episode: 294, duration: 0.023s, episode steps: 17, steps per second: 750, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.112 [-0.559, 1.298], mean_best_reward: --\n",
      "  6764/11000: episode: 295, duration: 0.033s, episode steps: 36, steps per second: 1075, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.034 [-0.794, 1.152], mean_best_reward: --\n",
      "  6784/11000: episode: 296, duration: 0.018s, episode steps: 20, steps per second: 1091, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.078 [-0.761, 1.235], mean_best_reward: --\n",
      "  6802/11000: episode: 297, duration: 0.020s, episode steps: 18, steps per second: 918, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.579, 1.184], mean_best_reward: --\n",
      "  6904/11000: episode: 298, duration: 0.098s, episode steps: 102, steps per second: 1045, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.260 [-0.985, 1.561], mean_best_reward: --\n",
      "  6930/11000: episode: 299, duration: 0.024s, episode steps: 26, steps per second: 1094, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.348, 0.995], mean_best_reward: --\n",
      "  6948/11000: episode: 300, duration: 0.023s, episode steps: 18, steps per second: 767, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.186, 0.749], mean_best_reward: --\n",
      "  6974/11000: episode: 301, duration: 0.025s, episode steps: 26, steps per second: 1037, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.129 [-0.556, 0.944], mean_best_reward: 67.500000\n",
      "  6985/11000: episode: 302, duration: 0.011s, episode steps: 11, steps per second: 1011, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.139 [-0.748, 1.437], mean_best_reward: --\n",
      "  7005/11000: episode: 303, duration: 0.019s, episode steps: 20, steps per second: 1042, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.118 [-0.963, 0.584], mean_best_reward: --\n",
      "  7046/11000: episode: 304, duration: 0.040s, episode steps: 41, steps per second: 1032, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.025 [-1.651, 1.343], mean_best_reward: --\n",
      "  7086/11000: episode: 305, duration: 0.044s, episode steps: 40, steps per second: 906, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.049 [-0.968, 1.261], mean_best_reward: --\n",
      "  7114/11000: episode: 306, duration: 0.032s, episode steps: 28, steps per second: 882, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.039 [-0.813, 1.555], mean_best_reward: --\n",
      "  7170/11000: episode: 307, duration: 0.067s, episode steps: 56, steps per second: 835, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.026 [-0.929, 1.163], mean_best_reward: --\n",
      "  7187/11000: episode: 308, duration: 0.022s, episode steps: 17, steps per second: 760, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.108 [-0.985, 1.814], mean_best_reward: --\n",
      "  7207/11000: episode: 309, duration: 0.025s, episode steps: 20, steps per second: 807, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.116 [-0.786, 1.726], mean_best_reward: --\n",
      "  7258/11000: episode: 310, duration: 0.052s, episode steps: 51, steps per second: 975, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.173 [-0.978, 1.406], mean_best_reward: --\n",
      "  7287/11000: episode: 311, duration: 0.039s, episode steps: 29, steps per second: 742, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.062 [-0.620, 1.481], mean_best_reward: --\n",
      "  7321/11000: episode: 312, duration: 0.037s, episode steps: 34, steps per second: 931, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.129 [-0.864, 1.263], mean_best_reward: --\n",
      "  7336/11000: episode: 313, duration: 0.015s, episode steps: 15, steps per second: 999, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.100 [-0.828, 1.523], mean_best_reward: --\n",
      "  7427/11000: episode: 314, duration: 0.111s, episode steps: 91, steps per second: 818, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.133 [-1.037, 1.611], mean_best_reward: --\n",
      "  7485/11000: episode: 315, duration: 0.057s, episode steps: 58, steps per second: 1014, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.076 [-1.225, 0.994], mean_best_reward: --\n",
      "  7497/11000: episode: 316, duration: 0.014s, episode steps: 12, steps per second: 857, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.609, 0.986], mean_best_reward: --\n",
      "  7523/11000: episode: 317, duration: 0.026s, episode steps: 26, steps per second: 993, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.047 [-1.228, 0.868], mean_best_reward: --\n",
      "  7546/11000: episode: 318, duration: 0.021s, episode steps: 23, steps per second: 1114, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.093 [-0.364, 0.783], mean_best_reward: --\n",
      "  7562/11000: episode: 319, duration: 0.015s, episode steps: 16, steps per second: 1051, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.076 [-1.335, 2.119], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7659/11000: episode: 320, duration: 0.101s, episode steps: 97, steps per second: 957, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.330 [-1.177, 2.024], mean_best_reward: --\n",
      "  7674/11000: episode: 321, duration: 0.016s, episode steps: 15, steps per second: 917, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.106 [-0.748, 1.261], mean_best_reward: --\n",
      "  7755/11000: episode: 322, duration: 0.087s, episode steps: 81, steps per second: 928, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.008 [-1.333, 1.201], mean_best_reward: --\n",
      "  7783/11000: episode: 323, duration: 0.028s, episode steps: 28, steps per second: 998, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.421, 0.604], mean_best_reward: --\n",
      "  7828/11000: episode: 324, duration: 0.046s, episode steps: 45, steps per second: 974, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.039 [-1.614, 1.377], mean_best_reward: --\n",
      "  7880/11000: episode: 325, duration: 0.057s, episode steps: 52, steps per second: 916, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: 0.069 [-1.205, 2.053], mean_best_reward: --\n",
      "  7900/11000: episode: 326, duration: 0.021s, episode steps: 20, steps per second: 956, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.085 [-1.622, 0.833], mean_best_reward: --\n",
      "  7920/11000: episode: 327, duration: 0.022s, episode steps: 20, steps per second: 892, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.092 [-1.324, 0.625], mean_best_reward: --\n",
      "  7959/11000: episode: 328, duration: 0.039s, episode steps: 39, steps per second: 1001, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.590 [0.000, 1.000], mean observation: -0.016 [-2.220, 1.349], mean_best_reward: --\n",
      "  7980/11000: episode: 329, duration: 0.022s, episode steps: 21, steps per second: 946, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.736, 1.182], mean_best_reward: --\n",
      "  7991/11000: episode: 330, duration: 0.013s, episode steps: 11, steps per second: 850, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.116 [-1.144, 1.751], mean_best_reward: --\n",
      "  8019/11000: episode: 331, duration: 0.026s, episode steps: 28, steps per second: 1090, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.050 [-1.601, 0.999], mean_best_reward: --\n",
      "  8056/11000: episode: 332, duration: 0.038s, episode steps: 37, steps per second: 973, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.040 [-0.813, 1.189], mean_best_reward: --\n",
      "  8135/11000: episode: 333, duration: 0.081s, episode steps: 79, steps per second: 980, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.010 [-1.483, 1.337], mean_best_reward: --\n",
      "  8156/11000: episode: 334, duration: 0.021s, episode steps: 21, steps per second: 1005, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.083 [-0.638, 1.221], mean_best_reward: --\n",
      "  8235/11000: episode: 335, duration: 0.078s, episode steps: 79, steps per second: 1015, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.096 [-0.984, 1.285], mean_best_reward: --\n",
      "  8257/11000: episode: 336, duration: 0.021s, episode steps: 22, steps per second: 1068, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.045 [-0.988, 1.461], mean_best_reward: --\n",
      "  8287/11000: episode: 337, duration: 0.029s, episode steps: 30, steps per second: 1032, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-1.102, 0.746], mean_best_reward: --\n",
      "  8353/11000: episode: 338, duration: 0.069s, episode steps: 66, steps per second: 961, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.071 [-1.449, 1.302], mean_best_reward: --\n",
      "  8445/11000: episode: 339, duration: 0.089s, episode steps: 92, steps per second: 1038, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.306 [-1.354, 1.192], mean_best_reward: --\n",
      "  8465/11000: episode: 340, duration: 0.020s, episode steps: 20, steps per second: 994, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.060 [-1.217, 2.082], mean_best_reward: --\n",
      "  8488/11000: episode: 341, duration: 0.022s, episode steps: 23, steps per second: 1044, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.009 [-0.995, 1.378], mean_best_reward: --\n",
      "  8565/11000: episode: 342, duration: 0.075s, episode steps: 77, steps per second: 1033, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.008 [-1.162, 0.644], mean_best_reward: --\n",
      "  8639/11000: episode: 343, duration: 0.077s, episode steps: 74, steps per second: 962, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.199 [-1.362, 1.303], mean_best_reward: --\n",
      "  8658/11000: episode: 344, duration: 0.018s, episode steps: 19, steps per second: 1031, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.095 [-0.752, 1.582], mean_best_reward: --\n",
      "  8684/11000: episode: 345, duration: 0.030s, episode steps: 26, steps per second: 880, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.185, 0.441], mean_best_reward: --\n",
      "  8705/11000: episode: 346, duration: 0.022s, episode steps: 21, steps per second: 966, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.080 [-0.756, 1.253], mean_best_reward: --\n",
      "  8717/11000: episode: 347, duration: 0.015s, episode steps: 12, steps per second: 797, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.944, 1.554], mean_best_reward: --\n",
      "  8772/11000: episode: 348, duration: 0.060s, episode steps: 55, steps per second: 916, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.109 [-1.516, 1.675], mean_best_reward: --\n",
      "  8805/11000: episode: 349, duration: 0.033s, episode steps: 33, steps per second: 1000, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.103 [-0.596, 1.643], mean_best_reward: --\n",
      "  8822/11000: episode: 350, duration: 0.019s, episode steps: 17, steps per second: 902, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.066 [-0.830, 1.484], mean_best_reward: --\n",
      "  8848/11000: episode: 351, duration: 0.028s, episode steps: 26, steps per second: 944, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.020 [-2.108, 1.368], mean_best_reward: 93.500000\n",
      "  8864/11000: episode: 352, duration: 0.016s, episode steps: 16, steps per second: 1026, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.101 [-0.931, 1.419], mean_best_reward: --\n",
      "  8914/11000: episode: 353, duration: 0.047s, episode steps: 50, steps per second: 1055, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.019 [-1.608, 1.163], mean_best_reward: --\n",
      "  8942/11000: episode: 354, duration: 0.028s, episode steps: 28, steps per second: 1016, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.053 [-1.328, 0.803], mean_best_reward: --\n",
      "  8962/11000: episode: 355, duration: 0.022s, episode steps: 20, steps per second: 930, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.077 [-1.740, 0.957], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8974/11000: episode: 356, duration: 0.016s, episode steps: 12, steps per second: 762, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.105 [-1.187, 1.830], mean_best_reward: --\n",
      "  8996/11000: episode: 357, duration: 0.026s, episode steps: 22, steps per second: 851, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.074 [-1.178, 0.798], mean_best_reward: --\n",
      "  9013/11000: episode: 358, duration: 0.020s, episode steps: 17, steps per second: 833, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.103 [-2.356, 1.372], mean_best_reward: --\n",
      "  9031/11000: episode: 359, duration: 0.020s, episode steps: 18, steps per second: 903, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.111 [-0.756, 1.273], mean_best_reward: --\n",
      "  9114/11000: episode: 360, duration: 0.079s, episode steps: 83, steps per second: 1051, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.197 [-0.759, 1.152], mean_best_reward: --\n",
      "  9137/11000: episode: 361, duration: 0.021s, episode steps: 23, steps per second: 1099, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.051 [-2.142, 1.357], mean_best_reward: --\n",
      "  9153/11000: episode: 362, duration: 0.015s, episode steps: 16, steps per second: 1053, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.078 [-1.013, 1.681], mean_best_reward: --\n",
      "  9170/11000: episode: 363, duration: 0.017s, episode steps: 17, steps per second: 1020, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.108 [-2.459, 1.338], mean_best_reward: --\n",
      "  9189/11000: episode: 364, duration: 0.027s, episode steps: 19, steps per second: 710, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.101 [-0.749, 1.557], mean_best_reward: --\n",
      "  9220/11000: episode: 365, duration: 0.029s, episode steps: 31, steps per second: 1087, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.034 [-0.984, 1.473], mean_best_reward: --\n",
      "  9233/11000: episode: 366, duration: 0.014s, episode steps: 13, steps per second: 962, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.119 [-2.202, 1.367], mean_best_reward: --\n",
      "  9261/11000: episode: 367, duration: 0.027s, episode steps: 28, steps per second: 1028, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.044 [-1.309, 0.973], mean_best_reward: --\n",
      "  9306/11000: episode: 368, duration: 0.043s, episode steps: 45, steps per second: 1037, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.172 [-0.452, 1.364], mean_best_reward: --\n",
      "  9333/11000: episode: 369, duration: 0.026s, episode steps: 27, steps per second: 1046, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.024 [-1.308, 0.834], mean_best_reward: --\n",
      "  9395/11000: episode: 370, duration: 0.059s, episode steps: 62, steps per second: 1057, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.652, 1.347], mean_best_reward: --\n",
      "  9412/11000: episode: 371, duration: 0.022s, episode steps: 17, steps per second: 785, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.065 [-0.837, 1.455], mean_best_reward: --\n",
      "  9436/11000: episode: 372, duration: 0.024s, episode steps: 24, steps per second: 1007, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.073 [-1.797, 0.979], mean_best_reward: --\n",
      "  9456/11000: episode: 373, duration: 0.020s, episode steps: 20, steps per second: 989, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.067 [-1.314, 0.837], mean_best_reward: --\n",
      "  9509/11000: episode: 374, duration: 0.062s, episode steps: 53, steps per second: 852, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.026 [-1.322, 0.846], mean_best_reward: --\n",
      "  9525/11000: episode: 375, duration: 0.016s, episode steps: 16, steps per second: 996, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.418, 0.929], mean_best_reward: --\n",
      "  9544/11000: episode: 376, duration: 0.019s, episode steps: 19, steps per second: 988, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.038 [-1.719, 1.032], mean_best_reward: --\n",
      "  9664/11000: episode: 377, duration: 0.124s, episode steps: 120, steps per second: 970, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.037 [-1.014, 1.008], mean_best_reward: --\n",
      "  9677/11000: episode: 378, duration: 0.012s, episode steps: 13, steps per second: 1070, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.098 [-1.932, 1.143], mean_best_reward: --\n",
      "  9706/11000: episode: 379, duration: 0.034s, episode steps: 29, steps per second: 861, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.062 [-0.784, 1.308], mean_best_reward: --\n",
      "  9721/11000: episode: 380, duration: 0.022s, episode steps: 15, steps per second: 692, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.093 [-0.794, 1.338], mean_best_reward: --\n",
      "  9733/11000: episode: 381, duration: 0.014s, episode steps: 12, steps per second: 865, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.153, 2.030], mean_best_reward: --\n",
      "  9756/11000: episode: 382, duration: 0.030s, episode steps: 23, steps per second: 775, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.696 [0.000, 1.000], mean observation: -0.020 [-2.580, 1.785], mean_best_reward: --\n",
      "  9844/11000: episode: 383, duration: 0.114s, episode steps: 88, steps per second: 773, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.786, 1.544], mean_best_reward: --\n",
      "  9902/11000: episode: 384, duration: 0.059s, episode steps: 58, steps per second: 991, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.006 [-1.204, 0.923], mean_best_reward: --\n",
      "  9923/11000: episode: 385, duration: 0.022s, episode steps: 21, steps per second: 954, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.042 [-1.289, 0.812], mean_best_reward: --\n",
      "  9957/11000: episode: 386, duration: 0.030s, episode steps: 34, steps per second: 1130, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.072 [-0.563, 1.024], mean_best_reward: --\n",
      "  9977/11000: episode: 387, duration: 0.028s, episode steps: 20, steps per second: 723, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.585, 0.986], mean_best_reward: --\n",
      "  9994/11000: episode: 388, duration: 0.020s, episode steps: 17, steps per second: 831, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.086 [-0.838, 1.494], mean_best_reward: --\n",
      " 10040/11000: episode: 389, duration: 0.047s, episode steps: 46, steps per second: 982, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.055 [-1.226, 0.712], mean_best_reward: --\n",
      " 10052/11000: episode: 390, duration: 0.016s, episode steps: 12, steps per second: 771, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.127 [-2.616, 1.537], mean_best_reward: --\n",
      " 10112/11000: episode: 391, duration: 0.064s, episode steps: 60, steps per second: 933, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.088 [-0.603, 0.936], mean_best_reward: --\n",
      " 10152/11000: episode: 392, duration: 0.039s, episode steps: 40, steps per second: 1016, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.161, 0.577], mean_best_reward: --\n",
      " 10176/11000: episode: 393, duration: 0.024s, episode steps: 24, steps per second: 1005, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.575, 1.279], mean_best_reward: --\n",
      " 10214/11000: episode: 394, duration: 0.039s, episode steps: 38, steps per second: 985, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.052 [-1.266, 0.558], mean_best_reward: --\n",
      " 10228/11000: episode: 395, duration: 0.015s, episode steps: 14, steps per second: 935, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.102 [-1.360, 2.204], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10257/11000: episode: 396, duration: 0.036s, episode steps: 29, steps per second: 804, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.113 [-1.117, 0.587], mean_best_reward: --\n",
      " 10281/11000: episode: 397, duration: 0.025s, episode steps: 24, steps per second: 953, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.827, 1.046], mean_best_reward: --\n",
      " 10293/11000: episode: 398, duration: 0.013s, episode steps: 12, steps per second: 920, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.126 [-2.053, 1.159], mean_best_reward: --\n",
      " 10304/11000: episode: 399, duration: 0.011s, episode steps: 11, steps per second: 977, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.138 [-1.675, 0.942], mean_best_reward: --\n",
      " 10358/11000: episode: 400, duration: 0.081s, episode steps: 54, steps per second: 663, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.108 [-0.824, 1.111], mean_best_reward: --\n",
      " 10370/11000: episode: 401, duration: 0.015s, episode steps: 12, steps per second: 814, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.175, 1.754], mean_best_reward: 85.000000\n",
      " 10390/11000: episode: 402, duration: 0.021s, episode steps: 20, steps per second: 945, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-0.980, 0.401], mean_best_reward: --\n",
      " 10409/11000: episode: 403, duration: 0.021s, episode steps: 19, steps per second: 904, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.074 [-0.757, 1.374], mean_best_reward: --\n",
      " 10452/11000: episode: 404, duration: 0.046s, episode steps: 43, steps per second: 933, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.042 [-0.946, 1.556], mean_best_reward: --\n",
      " 10481/11000: episode: 405, duration: 0.028s, episode steps: 29, steps per second: 1042, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.075 [-1.285, 0.759], mean_best_reward: --\n",
      " 10536/11000: episode: 406, duration: 0.052s, episode steps: 55, steps per second: 1052, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.141 [-1.252, 0.620], mean_best_reward: --\n",
      " 10616/11000: episode: 407, duration: 0.075s, episode steps: 80, steps per second: 1064, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.185, 0.731], mean_best_reward: --\n",
      " 10638/11000: episode: 408, duration: 0.021s, episode steps: 22, steps per second: 1057, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.454, 1.097], mean_best_reward: --\n",
      " 10650/11000: episode: 409, duration: 0.011s, episode steps: 12, steps per second: 1062, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.121 [-1.565, 0.817], mean_best_reward: --\n",
      " 10730/11000: episode: 410, duration: 0.081s, episode steps: 80, steps per second: 983, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.090 [-1.231, 0.672], mean_best_reward: --\n",
      " 10757/11000: episode: 411, duration: 0.026s, episode steps: 27, steps per second: 1024, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.045 [-0.982, 1.908], mean_best_reward: --\n",
      " 10787/11000: episode: 412, duration: 0.028s, episode steps: 30, steps per second: 1091, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.113, 0.538], mean_best_reward: --\n",
      " 10798/11000: episode: 413, duration: 0.010s, episode steps: 11, steps per second: 1052, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.119 [-1.787, 1.130], mean_best_reward: --\n",
      " 10811/11000: episode: 414, duration: 0.014s, episode steps: 13, steps per second: 937, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.078 [-1.216, 1.816], mean_best_reward: --\n",
      " 10876/11000: episode: 415, duration: 0.062s, episode steps: 65, steps per second: 1053, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.170 [-1.275, 0.552], mean_best_reward: --\n",
      " 10961/11000: episode: 416, duration: 0.085s, episode steps: 85, steps per second: 1002, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.364 [-2.475, 1.424], mean_best_reward: --\n",
      " 10976/11000: episode: 417, duration: 0.019s, episode steps: 15, steps per second: 778, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.783, 0.980], mean_best_reward: --\n",
      " 10992/11000: episode: 418, duration: 0.028s, episode steps: 16, steps per second: 563, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.112 [-1.328, 0.608], mean_best_reward: --\n",
      "done, took 12.989 seconds\n",
      "TEMPO:  1561719330.6351814\n",
      "Training for 4500 steps ...\n",
      "   36/4500: episode: 1, duration: 0.804s, episode steps: 36, steps per second: 45, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.011 [-1.564, 1.295], loss: 0.490722, mean_absolute_error: 0.515100, mean_q: 0.027512\n",
      "   45/4500: episode: 2, duration: 0.019s, episode steps: 9, steps per second: 466, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.781, 1.789], loss: 0.478482, mean_absolute_error: 0.530541, mean_q: 0.208199\n",
      "   60/4500: episode: 3, duration: 0.032s, episode steps: 15, steps per second: 476, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.108 [-1.156, 0.586], loss: 0.493597, mean_absolute_error: 0.582523, mean_q: 0.271250\n",
      "  132/4500: episode: 4, duration: 0.154s, episode steps: 72, steps per second: 469, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.168 [-1.800, 0.582], loss: 0.548472, mean_absolute_error: 0.777434, mean_q: 0.742037\n",
      "  163/4500: episode: 5, duration: 0.075s, episode steps: 31, steps per second: 411, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.677 [0.000, 1.000], mean observation: -0.006 [-2.996, 2.155], loss: 0.711550, mean_absolute_error: 1.134773, mean_q: 1.648233\n",
      "  193/4500: episode: 6, duration: 0.068s, episode steps: 30, steps per second: 439, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: -0.033 [-2.319, 1.534], loss: 0.779894, mean_absolute_error: 1.421076, mean_q: 2.349198\n",
      "  203/4500: episode: 7, duration: 0.021s, episode steps: 10, steps per second: 473, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.009, 1.946], loss: 1.535691, mean_absolute_error: 1.741558, mean_q: 3.212418\n",
      "  212/4500: episode: 8, duration: 0.020s, episode steps: 9, steps per second: 444, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-2.791, 1.810], loss: 1.800271, mean_absolute_error: 1.837065, mean_q: 3.480015\n",
      "  238/4500: episode: 9, duration: 0.064s, episode steps: 26, steps per second: 406, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.654 [0.000, 1.000], mean observation: -0.008 [-2.278, 1.563], loss: 1.479240, mean_absolute_error: 2.116010, mean_q: 3.699569\n",
      "  251/4500: episode: 10, duration: 0.029s, episode steps: 13, steps per second: 455, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.114 [-2.299, 1.333], loss: 1.646986, mean_absolute_error: 2.238743, mean_q: 3.975870\n",
      "  263/4500: episode: 11, duration: 0.025s, episode steps: 12, steps per second: 474, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.121 [-3.097, 1.948], loss: 2.866282, mean_absolute_error: 2.751692, mean_q: 5.048938\n",
      "  274/4500: episode: 12, duration: 0.025s, episode steps: 11, steps per second: 447, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.112 [-2.344, 1.395], loss: 2.134904, mean_absolute_error: 2.685822, mean_q: 4.839999\n",
      "  286/4500: episode: 13, duration: 0.026s, episode steps: 12, steps per second: 467, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.110 [-2.487, 1.533], loss: 2.542954, mean_absolute_error: 3.083742, mean_q: 5.658081\n",
      "  303/4500: episode: 14, duration: 0.037s, episode steps: 17, steps per second: 455, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.097 [-1.727, 0.949], loss: 1.389127, mean_absolute_error: 3.080021, mean_q: 5.621323\n",
      "  315/4500: episode: 15, duration: 0.027s, episode steps: 12, steps per second: 451, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.100 [-2.485, 1.606], loss: 3.234669, mean_absolute_error: 3.660203, mean_q: 6.593032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  331/4500: episode: 16, duration: 0.036s, episode steps: 16, steps per second: 444, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.102 [-2.374, 1.387], loss: 2.911917, mean_absolute_error: 3.691833, mean_q: 6.473026\n",
      "  346/4500: episode: 17, duration: 0.037s, episode steps: 15, steps per second: 404, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.107 [-1.330, 0.562], loss: 1.382095, mean_absolute_error: 3.569553, mean_q: 6.345711\n",
      "  364/4500: episode: 18, duration: 0.040s, episode steps: 18, steps per second: 454, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.066 [-1.560, 0.990], loss: 1.714757, mean_absolute_error: 4.101874, mean_q: 7.170851\n",
      "  374/4500: episode: 19, duration: 0.023s, episode steps: 10, steps per second: 434, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.150 [-2.098, 1.335], loss: 3.701931, mean_absolute_error: 4.884411, mean_q: 8.514851\n",
      "  386/4500: episode: 20, duration: 0.027s, episode steps: 12, steps per second: 446, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.571, 0.987], loss: 2.615790, mean_absolute_error: 4.392668, mean_q: 7.986222\n",
      "  399/4500: episode: 21, duration: 0.028s, episode steps: 13, steps per second: 463, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.094 [-2.831, 1.795], loss: 4.629346, mean_absolute_error: 4.627447, mean_q: 8.597983\n",
      "  411/4500: episode: 22, duration: 0.026s, episode steps: 12, steps per second: 461, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.102 [-1.978, 1.196], loss: 3.788179, mean_absolute_error: 4.835700, mean_q: 8.939443\n",
      "  420/4500: episode: 23, duration: 0.020s, episode steps: 9, steps per second: 454, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.258, 1.395], loss: 5.520600, mean_absolute_error: 5.174304, mean_q: 9.393312\n",
      "  434/4500: episode: 24, duration: 0.034s, episode steps: 14, steps per second: 416, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.086 [-2.038, 1.228], loss: 2.718496, mean_absolute_error: 4.698905, mean_q: 8.611090\n",
      "  443/4500: episode: 25, duration: 0.020s, episode steps: 9, steps per second: 458, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.156 [-2.231, 1.326], loss: 5.092709, mean_absolute_error: 5.147369, mean_q: 9.289830\n",
      "  453/4500: episode: 26, duration: 0.022s, episode steps: 10, steps per second: 451, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.606, 1.612], loss: 4.529722, mean_absolute_error: 4.908507, mean_q: 8.976341\n",
      "  464/4500: episode: 27, duration: 0.024s, episode steps: 11, steps per second: 457, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.131 [-2.805, 1.721], loss: 3.830138, mean_absolute_error: 4.707319, mean_q: 8.639991\n",
      "  477/4500: episode: 28, duration: 0.030s, episode steps: 13, steps per second: 429, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.102 [-2.191, 1.370], loss: 2.801228, mean_absolute_error: 4.775528, mean_q: 8.849424\n",
      "  498/4500: episode: 29, duration: 0.044s, episode steps: 21, steps per second: 479, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.085 [-1.134, 1.986], loss: 7.650111, mean_absolute_error: 6.545586, mean_q: 11.176612\n",
      "  511/4500: episode: 30, duration: 0.029s, episode steps: 13, steps per second: 451, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.093 [-1.344, 2.090], loss: 14.411348, mean_absolute_error: 8.000557, mean_q: 13.097095\n",
      "  522/4500: episode: 31, duration: 0.028s, episode steps: 11, steps per second: 388, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.131 [-0.829, 1.485], loss: 9.435044, mean_absolute_error: 7.385971, mean_q: 12.839304\n",
      "  535/4500: episode: 32, duration: 0.027s, episode steps: 13, steps per second: 480, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-1.181, 1.842], loss: 8.493617, mean_absolute_error: 7.246339, mean_q: 12.981225\n",
      "  546/4500: episode: 33, duration: 0.024s, episode steps: 11, steps per second: 465, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.588, 2.504], loss: 18.075878, mean_absolute_error: 7.834061, mean_q: 13.897960\n",
      "  557/4500: episode: 34, duration: 0.023s, episode steps: 11, steps per second: 472, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.360, 2.275], loss: 11.999730, mean_absolute_error: 7.266638, mean_q: 13.058699\n",
      "  567/4500: episode: 35, duration: 0.021s, episode steps: 10, steps per second: 479, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.158 [-1.519, 2.580], loss: 15.027025, mean_absolute_error: 7.475413, mean_q: 13.488646\n",
      "  577/4500: episode: 36, duration: 0.027s, episode steps: 10, steps per second: 376, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.529, 2.507], loss: 12.960178, mean_absolute_error: 7.572202, mean_q: 14.050190\n",
      "  586/4500: episode: 37, duration: 0.018s, episode steps: 9, steps per second: 491, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.726, 2.733], loss: 16.450041, mean_absolute_error: 6.870238, mean_q: 12.765747\n",
      "  596/4500: episode: 38, duration: 0.022s, episode steps: 10, steps per second: 450, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.106 [-1.988, 2.990], loss: 17.650387, mean_absolute_error: 7.169023, mean_q: 13.891170\n",
      "  605/4500: episode: 39, duration: 0.028s, episode steps: 9, steps per second: 319, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.812, 2.835], loss: 16.478014, mean_absolute_error: 6.895677, mean_q: 13.229878\n",
      "  615/4500: episode: 40, duration: 0.025s, episode steps: 10, steps per second: 403, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.941, 3.097], loss: 15.021256, mean_absolute_error: 6.791006, mean_q: 13.200082\n",
      "  625/4500: episode: 41, duration: 0.023s, episode steps: 10, steps per second: 441, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.559, 2.571], loss: 11.019214, mean_absolute_error: 6.263082, mean_q: 11.804372\n",
      "  637/4500: episode: 42, duration: 0.028s, episode steps: 12, steps per second: 435, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.124 [-1.943, 3.015], loss: 10.131876, mean_absolute_error: 6.183176, mean_q: 11.764690\n",
      "  646/4500: episode: 43, duration: 0.020s, episode steps: 9, steps per second: 451, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.152 [-1.338, 2.251], loss: 7.604594, mean_absolute_error: 6.115957, mean_q: 11.631268\n",
      "  662/4500: episode: 44, duration: 0.034s, episode steps: 16, steps per second: 467, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.062 [-1.995, 3.019], loss: 8.056321, mean_absolute_error: 6.095828, mean_q: 11.618102\n",
      "  671/4500: episode: 45, duration: 0.021s, episode steps: 9, steps per second: 428, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.545, 2.477], loss: 11.040137, mean_absolute_error: 6.089637, mean_q: 11.660844\n",
      "  684/4500: episode: 46, duration: 0.027s, episode steps: 13, steps per second: 479, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.118 [-1.710, 2.697], loss: 6.041809, mean_absolute_error: 5.970118, mean_q: 11.507247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  695/4500: episode: 47, duration: 0.026s, episode steps: 11, steps per second: 429, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.146 [-1.731, 2.783], loss: 8.395165, mean_absolute_error: 5.950137, mean_q: 11.384029\n",
      "  706/4500: episode: 48, duration: 0.027s, episode steps: 11, steps per second: 402, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.789, 2.829], loss: 8.435202, mean_absolute_error: 5.932163, mean_q: 11.424528\n",
      "  715/4500: episode: 49, duration: 0.020s, episode steps: 9, steps per second: 444, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.789, 2.804], loss: 9.081702, mean_absolute_error: 5.743052, mean_q: 11.090290\n",
      "  727/4500: episode: 50, duration: 0.026s, episode steps: 12, steps per second: 467, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.230, 2.027], loss: 5.824203, mean_absolute_error: 5.656024, mean_q: 10.502335\n",
      "  736/4500: episode: 51, duration: 0.020s, episode steps: 9, steps per second: 449, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.722, 2.781], loss: 8.551478, mean_absolute_error: 5.776015, mean_q: 11.061068\n",
      "  745/4500: episode: 52, duration: 0.019s, episode steps: 9, steps per second: 467, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.794, 2.854], loss: 8.198180, mean_absolute_error: 5.719431, mean_q: 10.835550\n",
      "  757/4500: episode: 53, duration: 0.026s, episode steps: 12, steps per second: 468, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.112 [-1.956, 2.985], loss: 6.004531, mean_absolute_error: 5.486088, mean_q: 10.304113\n",
      "  769/4500: episode: 54, duration: 0.026s, episode steps: 12, steps per second: 462, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.110 [-1.528, 2.535], loss: 5.479340, mean_absolute_error: 5.560278, mean_q: 10.350703\n",
      "  777/4500: episode: 55, duration: 0.019s, episode steps: 8, steps per second: 431, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.609, 2.537], loss: 6.806487, mean_absolute_error: 5.393380, mean_q: 9.911068\n",
      "  788/4500: episode: 56, duration: 0.028s, episode steps: 11, steps per second: 396, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.108 [-1.802, 2.727], loss: 5.430426, mean_absolute_error: 5.464138, mean_q: 10.213179\n",
      "  799/4500: episode: 57, duration: 0.025s, episode steps: 11, steps per second: 446, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.153 [-1.513, 2.418], loss: 4.793187, mean_absolute_error: 5.296728, mean_q: 9.876515\n",
      "  818/4500: episode: 58, duration: 0.041s, episode steps: 19, steps per second: 463, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.047 [-1.415, 2.173], loss: 3.107916, mean_absolute_error: 5.378008, mean_q: 9.977767\n",
      "  833/4500: episode: 59, duration: 0.031s, episode steps: 15, steps per second: 479, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.084 [-1.720, 2.724], loss: 3.597568, mean_absolute_error: 5.428562, mean_q: 9.983612\n",
      "  843/4500: episode: 60, duration: 0.022s, episode steps: 10, steps per second: 445, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.598, 2.448], loss: 4.664810, mean_absolute_error: 5.366761, mean_q: 9.606499\n",
      "  854/4500: episode: 61, duration: 0.025s, episode steps: 11, steps per second: 447, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-0.982, 1.858], loss: 4.034064, mean_absolute_error: 5.363822, mean_q: 9.653393\n",
      "  867/4500: episode: 62, duration: 0.029s, episode steps: 13, steps per second: 455, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.082 [-1.405, 2.253], loss: 3.712303, mean_absolute_error: 5.484030, mean_q: 9.926788\n",
      "  884/4500: episode: 63, duration: 0.046s, episode steps: 17, steps per second: 370, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.065 [-1.371, 2.224], loss: 2.989577, mean_absolute_error: 5.460825, mean_q: 10.012445\n",
      "  898/4500: episode: 64, duration: 0.031s, episode steps: 14, steps per second: 451, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.111 [-1.163, 2.045], loss: 3.215655, mean_absolute_error: 5.345442, mean_q: 9.659213\n",
      "  911/4500: episode: 65, duration: 0.029s, episode steps: 13, steps per second: 452, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.124 [-1.439, 0.743], loss: 5.089076, mean_absolute_error: 6.530284, mean_q: 11.426782\n",
      "  925/4500: episode: 66, duration: 0.031s, episode steps: 14, steps per second: 446, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.101 [-0.950, 1.638], loss: 3.290047, mean_absolute_error: 5.475394, mean_q: 9.818013\n",
      "  958/4500: episode: 67, duration: 0.072s, episode steps: 33, steps per second: 458, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.104 [-1.013, 0.486], loss: 2.413528, mean_absolute_error: 5.916546, mean_q: 10.810432\n",
      "  970/4500: episode: 68, duration: 0.027s, episode steps: 12, steps per second: 439, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-0.967, 1.720], loss: 3.806739, mean_absolute_error: 5.774825, mean_q: 10.135559\n",
      "  992/4500: episode: 69, duration: 0.056s, episode steps: 22, steps per second: 396, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.052 [-1.193, 1.924], loss: 2.284918, mean_absolute_error: 5.782904, mean_q: 10.463170\n",
      " 1005/4500: episode: 70, duration: 0.034s, episode steps: 13, steps per second: 386, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.101 [-0.843, 1.479], loss: 3.565963, mean_absolute_error: 5.822021, mean_q: 10.330784\n",
      " 1020/4500: episode: 71, duration: 0.032s, episode steps: 15, steps per second: 473, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.067 [-2.195, 1.412], loss: 6.903027, mean_absolute_error: 7.074753, mean_q: 12.461860\n",
      " 1039/4500: episode: 72, duration: 0.042s, episode steps: 19, steps per second: 450, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.083 [-1.145, 0.737], loss: 3.574381, mean_absolute_error: 6.582815, mean_q: 11.973390\n",
      " 1051/4500: episode: 73, duration: 0.026s, episode steps: 12, steps per second: 457, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.123 [-2.114, 1.214], loss: 7.901315, mean_absolute_error: 7.206188, mean_q: 12.504253\n",
      " 1072/4500: episode: 74, duration: 0.046s, episode steps: 21, steps per second: 455, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.067 [-0.832, 1.428], loss: 2.906212, mean_absolute_error: 6.186142, mean_q: 11.184425\n",
      " 1091/4500: episode: 75, duration: 0.063s, episode steps: 19, steps per second: 300, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.066 [-0.830, 1.492], loss: 2.915183, mean_absolute_error: 6.230530, mean_q: 11.202553\n",
      " 1113/4500: episode: 76, duration: 0.048s, episode steps: 22, steps per second: 461, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.086 [-0.760, 1.593], loss: 2.654579, mean_absolute_error: 6.268543, mean_q: 11.378051\n",
      " 1148/4500: episode: 77, duration: 0.077s, episode steps: 35, steps per second: 454, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.027 [-2.226, 1.766], loss: 4.758221, mean_absolute_error: 7.222843, mean_q: 13.171796\n",
      " 1160/4500: episode: 78, duration: 0.027s, episode steps: 12, steps per second: 442, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.100 [-2.561, 1.593], loss: 11.627728, mean_absolute_error: 7.922985, mean_q: 14.199822\n",
      " 1172/4500: episode: 79, duration: 0.025s, episode steps: 12, steps per second: 478, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.128 [-2.317, 1.363], loss: 10.034831, mean_absolute_error: 7.567968, mean_q: 13.780678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1191/4500: episode: 80, duration: 0.046s, episode steps: 19, steps per second: 412, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.065 [-1.478, 0.961], loss: 3.900349, mean_absolute_error: 7.030389, mean_q: 13.109353\n",
      " 1202/4500: episode: 81, duration: 0.024s, episode steps: 11, steps per second: 462, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-1.868, 1.189], loss: 8.600678, mean_absolute_error: 7.406077, mean_q: 13.398121\n",
      " 1213/4500: episode: 82, duration: 0.024s, episode steps: 11, steps per second: 451, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.159 [-2.301, 1.327], loss: 7.953047, mean_absolute_error: 7.302922, mean_q: 13.248173\n",
      " 1231/4500: episode: 83, duration: 0.039s, episode steps: 18, steps per second: 463, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.054 [-1.490, 0.996], loss: 3.823276, mean_absolute_error: 6.754769, mean_q: 12.471630\n",
      " 1241/4500: episode: 84, duration: 0.022s, episode steps: 10, steps per second: 450, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.270, 1.396], loss: 8.672136, mean_absolute_error: 7.130952, mean_q: 12.659856\n",
      " 1252/4500: episode: 85, duration: 0.025s, episode steps: 11, steps per second: 436, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-1.871, 1.196], loss: 6.648108, mean_absolute_error: 6.916925, mean_q: 12.467525\n",
      " 1263/4500: episode: 86, duration: 0.023s, episode steps: 11, steps per second: 475, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.123 [-2.826, 1.790], loss: 6.570252, mean_absolute_error: 6.825800, mean_q: 12.115648\n",
      " 1279/4500: episode: 87, duration: 0.036s, episode steps: 16, steps per second: 445, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.101 [-1.297, 0.761], loss: 3.514994, mean_absolute_error: 6.318461, mean_q: 11.396760\n",
      " 1295/4500: episode: 88, duration: 0.040s, episode steps: 16, steps per second: 399, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.061 [-1.531, 0.989], loss: 3.505812, mean_absolute_error: 6.212466, mean_q: 11.215708\n",
      " 1309/4500: episode: 89, duration: 0.031s, episode steps: 14, steps per second: 449, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.114 [-2.081, 1.167], loss: 3.702683, mean_absolute_error: 6.318048, mean_q: 11.317356\n",
      " 1338/4500: episode: 90, duration: 0.060s, episode steps: 29, steps per second: 480, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.046 [-1.900, 1.145], loss: 2.063506, mean_absolute_error: 6.413591, mean_q: 11.852542\n",
      " 1364/4500: episode: 91, duration: 0.056s, episode steps: 26, steps per second: 466, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.023 [-1.192, 1.796], loss: 3.644400, mean_absolute_error: 6.927386, mean_q: 12.586784\n",
      " 1382/4500: episode: 92, duration: 0.041s, episode steps: 18, steps per second: 439, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.101 [-0.961, 1.879], loss: 5.123554, mean_absolute_error: 7.372687, mean_q: 13.219018\n",
      " 1414/4500: episode: 93, duration: 0.076s, episode steps: 32, steps per second: 420, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.055 [-1.266, 0.780], loss: 2.384272, mean_absolute_error: 6.823898, mean_q: 12.613395\n",
      " 1427/4500: episode: 94, duration: 0.028s, episode steps: 13, steps per second: 471, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.128 [-1.809, 0.967], loss: 5.382546, mean_absolute_error: 7.126993, mean_q: 12.819104\n",
      " 1451/4500: episode: 95, duration: 0.057s, episode steps: 24, steps per second: 422, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.246, 0.469], loss: 3.119120, mean_absolute_error: 6.957403, mean_q: 12.764860\n",
      " 1470/4500: episode: 96, duration: 0.040s, episode steps: 19, steps per second: 476, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.085 [-0.966, 1.798], loss: 5.207918, mean_absolute_error: 7.648626, mean_q: 13.793920\n",
      " 1513/4500: episode: 97, duration: 0.096s, episode steps: 43, steps per second: 449, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: 0.017 [-1.006, 1.816], loss: 2.853792, mean_absolute_error: 7.712220, mean_q: 14.330667\n",
      " 1554/4500: episode: 98, duration: 0.090s, episode steps: 41, steps per second: 454, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.110 [-1.298, 0.891], loss: 2.892199, mean_absolute_error: 8.013499, mean_q: 14.996767\n",
      " 1564/4500: episode: 99, duration: 0.022s, episode steps: 10, steps per second: 451, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.125 [-1.837, 1.143], loss: 9.797050, mean_absolute_error: 8.305746, mean_q: 14.799205\n",
      " 1584/4500: episode: 100, duration: 0.045s, episode steps: 20, steps per second: 444, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.086 [-1.190, 0.597], loss: 5.248838, mean_absolute_error: 7.891792, mean_q: 14.585494\n",
      " 1621/4500: episode: 101, duration: 0.088s, episode steps: 37, steps per second: 419, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.039 [-1.099, 0.824], loss: 3.105547, mean_absolute_error: 8.017793, mean_q: 15.049749\n",
      " 1636/4500: episode: 102, duration: 0.033s, episode steps: 15, steps per second: 457, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.021, 1.823], loss: 10.077815, mean_absolute_error: 9.206159, mean_q: 16.477377\n",
      " 1656/4500: episode: 103, duration: 0.044s, episode steps: 20, steps per second: 457, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.064 [-0.953, 1.633], loss: 7.106629, mean_absolute_error: 8.815841, mean_q: 16.407849\n",
      " 1670/4500: episode: 104, duration: 0.031s, episode steps: 14, steps per second: 445, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.091 [-2.000, 3.053], loss: 9.576906, mean_absolute_error: 8.909003, mean_q: 16.134445\n",
      " 1680/4500: episode: 105, duration: 0.022s, episode steps: 10, steps per second: 453, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.105 [-1.214, 1.931], loss: 12.272947, mean_absolute_error: 8.938629, mean_q: 15.765620\n",
      " 1693/4500: episode: 106, duration: 0.028s, episode steps: 13, steps per second: 464, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.123 [-1.150, 2.087], loss: 7.587387, mean_absolute_error: 8.434070, mean_q: 15.314447\n",
      " 1756/4500: episode: 107, duration: 0.142s, episode steps: 63, steps per second: 444, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.041 [-1.548, 0.615], loss: 2.448020, mean_absolute_error: 8.475469, mean_q: 16.006317\n",
      " 1818/4500: episode: 108, duration: 0.131s, episode steps: 62, steps per second: 472, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.134 [-1.555, 0.605], loss: 3.394893, mean_absolute_error: 9.419640, mean_q: 17.820782\n",
      " 1832/4500: episode: 109, duration: 0.030s, episode steps: 14, steps per second: 462, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.074 [-1.013, 1.481], loss: 11.835574, mean_absolute_error: 9.921992, mean_q: 18.046291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1863/4500: episode: 110, duration: 0.072s, episode steps: 31, steps per second: 432, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.099 [-0.800, 0.398], loss: 5.528775, mean_absolute_error: 9.950999, mean_q: 18.657060\n",
      " 1877/4500: episode: 111, duration: 0.029s, episode steps: 14, steps per second: 481, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.741, 1.298], loss: 11.911693, mean_absolute_error: 9.966255, mean_q: 18.435518\n",
      " 1903/4500: episode: 112, duration: 0.066s, episode steps: 26, steps per second: 392, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.207, 0.738], loss: 6.307186, mean_absolute_error: 9.793403, mean_q: 18.342842\n",
      " 1944/4500: episode: 113, duration: 0.088s, episode steps: 41, steps per second: 466, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.050 [-1.554, 0.814], loss: 4.639231, mean_absolute_error: 10.064297, mean_q: 19.032783\n",
      " 1965/4500: episode: 114, duration: 0.052s, episode steps: 21, steps per second: 405, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.067 [-1.164, 2.006], loss: 6.994250, mean_absolute_error: 10.115456, mean_q: 18.846509\n",
      " 2009/4500: episode: 115, duration: 0.097s, episode steps: 44, steps per second: 455, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.050 [-1.246, 1.166], loss: 4.023833, mean_absolute_error: 10.157141, mean_q: 19.356987\n",
      " 2025/4500: episode: 116, duration: 0.034s, episode steps: 16, steps per second: 467, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.078 [-2.145, 1.386], loss: 14.205845, mean_absolute_error: 11.472809, mean_q: 21.024708\n",
      " 2040/4500: episode: 117, duration: 0.034s, episode steps: 15, steps per second: 440, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.101 [-1.088, 0.600], loss: 11.672957, mean_absolute_error: 10.570355, mean_q: 19.527692\n",
      " 2052/4500: episode: 118, duration: 0.027s, episode steps: 12, steps per second: 444, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.804, 1.246], loss: 13.880027, mean_absolute_error: 9.958013, mean_q: 18.102415\n",
      " 2068/4500: episode: 119, duration: 0.041s, episode steps: 16, steps per second: 394, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.117 [-0.769, 1.633], loss: 8.680518, mean_absolute_error: 9.693906, mean_q: 17.902221\n",
      " 2091/4500: episode: 120, duration: 0.051s, episode steps: 23, steps per second: 451, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.075 [-0.593, 1.116], loss: 6.620112, mean_absolute_error: 9.624362, mean_q: 17.875524\n",
      " 2131/4500: episode: 121, duration: 0.085s, episode steps: 40, steps per second: 473, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.032 [-1.368, 0.955], loss: 4.234032, mean_absolute_error: 9.681035, mean_q: 18.371637\n",
      " 2155/4500: episode: 122, duration: 0.052s, episode steps: 24, steps per second: 464, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.051, 0.613], loss: 6.495537, mean_absolute_error: 9.798203, mean_q: 18.330377\n",
      " 2188/4500: episode: 123, duration: 0.077s, episode steps: 33, steps per second: 431, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.022 [-0.965, 1.341], loss: 4.762924, mean_absolute_error: 9.894718, mean_q: 18.728521\n",
      " 2241/4500: episode: 124, duration: 0.116s, episode steps: 53, steps per second: 458, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.042 [-1.544, 0.827], loss: 4.097403, mean_absolute_error: 10.384744, mean_q: 19.884953\n",
      " 2254/4500: episode: 125, duration: 0.030s, episode steps: 13, steps per second: 436, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.104 [-1.722, 1.158], loss: 14.169250, mean_absolute_error: 11.331069, mean_q: 20.890908\n",
      " 2287/4500: episode: 126, duration: 0.076s, episode steps: 33, steps per second: 437, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.073 [-0.457, 1.389], loss: 5.530955, mean_absolute_error: 10.740425, mean_q: 20.318217\n",
      " 2311/4500: episode: 127, duration: 0.052s, episode steps: 24, steps per second: 461, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.445, 1.005], loss: 7.493846, mean_absolute_error: 10.427142, mean_q: 19.664743\n",
      " 2323/4500: episode: 128, duration: 0.027s, episode steps: 12, steps per second: 449, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.108 [-1.371, 0.796], loss: 15.358443, mean_absolute_error: 10.796662, mean_q: 19.806523\n",
      " 2347/4500: episode: 129, duration: 0.057s, episode steps: 24, steps per second: 423, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.085 [-1.526, 0.760], loss: 7.388398, mean_absolute_error: 10.424609, mean_q: 19.720146\n",
      " 2374/4500: episode: 130, duration: 0.060s, episode steps: 27, steps per second: 448, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.086 [-1.125, 0.599], loss: 6.638213, mean_absolute_error: 10.116640, mean_q: 19.145671\n",
      " 2400/4500: episode: 131, duration: 0.062s, episode steps: 26, steps per second: 422, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.162, 0.628], loss: 6.026855, mean_absolute_error: 9.989644, mean_q: 18.890978\n",
      " 2414/4500: episode: 132, duration: 0.030s, episode steps: 14, steps per second: 465, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.099 [-0.943, 1.480], loss: 12.483195, mean_absolute_error: 10.406986, mean_q: 19.089997\n",
      " 2460/4500: episode: 133, duration: 0.099s, episode steps: 46, steps per second: 465, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.012 [-0.938, 1.588], loss: 3.953473, mean_absolute_error: 10.252022, mean_q: 19.670129\n",
      " 2480/4500: episode: 134, duration: 0.044s, episode steps: 20, steps per second: 450, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.059 [-2.382, 1.541], loss: 8.893805, mean_absolute_error: 10.894709, mean_q: 20.313138\n",
      " 2514/4500: episode: 135, duration: 0.083s, episode steps: 34, steps per second: 410, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.052 [-1.477, 0.805], loss: 4.519633, mean_absolute_error: 10.320192, mean_q: 19.680762\n",
      " 2536/4500: episode: 136, duration: 0.049s, episode steps: 22, steps per second: 446, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.027 [-1.361, 1.999], loss: 7.075740, mean_absolute_error: 10.378506, mean_q: 19.639577\n",
      " 2647/4500: episode: 137, duration: 0.247s, episode steps: 111, steps per second: 450, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.185 [-1.026, 1.629], loss: 3.244462, mean_absolute_error: 11.247866, mean_q: 21.768851\n",
      " 2664/4500: episode: 138, duration: 0.037s, episode steps: 17, steps per second: 457, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.072 [-1.420, 0.939], loss: 14.507118, mean_absolute_error: 12.476698, mean_q: 23.476091\n",
      " 2676/4500: episode: 139, duration: 0.026s, episode steps: 12, steps per second: 459, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.101 [-1.965, 1.335], loss: 18.889827, mean_absolute_error: 12.384124, mean_q: 22.941789\n",
      " 2692/4500: episode: 140, duration: 0.056s, episode steps: 16, steps per second: 284, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.062 [-1.947, 1.206], loss: 11.382273, mean_absolute_error: 11.284675, mean_q: 21.465310\n",
      " 2706/4500: episode: 141, duration: 0.046s, episode steps: 14, steps per second: 306, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.093 [-1.466, 0.808], loss: 11.452714, mean_absolute_error: 10.936301, mean_q: 20.414922\n",
      " 2719/4500: episode: 142, duration: 0.028s, episode steps: 13, steps per second: 471, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.077 [-1.543, 1.024], loss: 10.222486, mean_absolute_error: 9.989335, mean_q: 18.742321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2733/4500: episode: 143, duration: 0.036s, episode steps: 14, steps per second: 390, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.083 [-1.706, 1.201], loss: 8.123520, mean_absolute_error: 9.427062, mean_q: 17.745267\n",
      " 2742/4500: episode: 144, duration: 0.023s, episode steps: 9, steps per second: 384, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.128 [-1.970, 1.212], loss: 10.684997, mean_absolute_error: 9.329206, mean_q: 17.194337\n",
      " 2755/4500: episode: 145, duration: 0.042s, episode steps: 13, steps per second: 310, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.077 [-2.129, 1.397], loss: 6.249128, mean_absolute_error: 8.701442, mean_q: 16.126359\n",
      " 2787/4500: episode: 146, duration: 0.068s, episode steps: 32, steps per second: 472, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.159, 0.394], loss: 4.343933, mean_absolute_error: 9.586228, mean_q: 17.660289\n",
      " 2797/4500: episode: 147, duration: 0.022s, episode steps: 10, steps per second: 446, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.121 [-1.586, 0.938], loss: 7.610015, mean_absolute_error: 8.117843, mean_q: 14.775570\n",
      " 2849/4500: episode: 148, duration: 0.119s, episode steps: 52, steps per second: 438, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.195 [-0.678, 1.007], loss: 3.799604, mean_absolute_error: 9.706082, mean_q: 18.505938\n",
      " 2877/4500: episode: 149, duration: 0.062s, episode steps: 28, steps per second: 451, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.044 [-1.320, 2.049], loss: 8.125736, mean_absolute_error: 10.671284, mean_q: 20.120294\n",
      " 2925/4500: episode: 150, duration: 0.104s, episode steps: 48, steps per second: 461, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.009 [-0.763, 1.348], loss: 5.351521, mean_absolute_error: 11.227370, mean_q: 21.516576\n",
      " 2944/4500: episode: 151, duration: 0.046s, episode steps: 19, steps per second: 412, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.092 [-0.557, 1.095], loss: 12.822019, mean_absolute_error: 11.582405, mean_q: 21.586656\n",
      " 2965/4500: episode: 152, duration: 0.048s, episode steps: 21, steps per second: 438, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.087 [-0.594, 1.133], loss: 10.202627, mean_absolute_error: 11.318686, mean_q: 21.101347\n",
      " 3032/4500: episode: 153, duration: 0.144s, episode steps: 67, steps per second: 466, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.026 [-0.962, 1.512], loss: 4.172978, mean_absolute_error: 11.123030, mean_q: 21.358163\n",
      " 3048/4500: episode: 154, duration: 0.039s, episode steps: 16, steps per second: 413, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.091 [-1.092, 0.590], loss: 11.329271, mean_absolute_error: 10.441753, mean_q: 19.401054\n",
      " 3089/4500: episode: 155, duration: 0.090s, episode steps: 41, steps per second: 456, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.063 [-0.610, 0.957], loss: 6.524728, mean_absolute_error: 11.653535, mean_q: 22.212632\n",
      " 3191/4500: episode: 156, duration: 0.241s, episode steps: 102, steps per second: 423, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.227 [-0.774, 1.310], loss: 2.411849, mean_absolute_error: 10.867510, mean_q: 21.108854\n",
      " 3208/4500: episode: 157, duration: 0.044s, episode steps: 17, steps per second: 387, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.110 [-1.462, 0.758], loss: 13.722590, mean_absolute_error: 12.867589, mean_q: 24.076828\n",
      " 3227/4500: episode: 158, duration: 0.043s, episode steps: 19, steps per second: 443, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.084 [-1.957, 1.154], loss: 9.186044, mean_absolute_error: 12.482686, mean_q: 23.473341\n",
      " 3250/4500: episode: 159, duration: 0.057s, episode steps: 23, steps per second: 400, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.081 [-0.958, 0.634], loss: 10.344200, mean_absolute_error: 12.013327, mean_q: 22.508185\n",
      " 3275/4500: episode: 160, duration: 0.060s, episode steps: 25, steps per second: 419, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.073 [-0.753, 1.142], loss: 11.690453, mean_absolute_error: 12.684355, mean_q: 23.865819\n",
      " 3327/4500: episode: 161, duration: 0.125s, episode steps: 52, steps per second: 415, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: 0.020 [-1.535, 2.191], loss: 5.408960, mean_absolute_error: 11.971982, mean_q: 23.020701\n",
      " 3341/4500: episode: 162, duration: 0.032s, episode steps: 14, steps per second: 436, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.117 [-0.798, 1.557], loss: 17.939914, mean_absolute_error: 12.366055, mean_q: 22.646334\n",
      " 3365/4500: episode: 163, duration: 0.058s, episode steps: 24, steps per second: 413, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.399, 1.096], loss: 9.928556, mean_absolute_error: 12.257839, mean_q: 23.088214\n",
      " 3394/4500: episode: 164, duration: 0.072s, episode steps: 29, steps per second: 402, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.040 [-0.628, 0.938], loss: 10.468577, mean_absolute_error: 12.234621, mean_q: 23.077241\n",
      " 3418/4500: episode: 165, duration: 0.055s, episode steps: 24, steps per second: 433, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.900, 0.409], loss: 9.326188, mean_absolute_error: 11.391011, mean_q: 21.378894\n",
      " 3493/4500: episode: 166, duration: 0.176s, episode steps: 75, steps per second: 426, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.166 [-1.275, 0.695], loss: 4.145384, mean_absolute_error: 11.741681, mean_q: 22.690957\n",
      " 3526/4500: episode: 167, duration: 0.070s, episode steps: 33, steps per second: 469, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.023 [-1.345, 1.781], loss: 7.418816, mean_absolute_error: 12.031158, mean_q: 23.205352\n",
      " 3547/4500: episode: 168, duration: 0.046s, episode steps: 21, steps per second: 452, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.084 [-0.810, 1.274], loss: 13.074214, mean_absolute_error: 12.847999, mean_q: 24.097975\n",
      " 3599/4500: episode: 169, duration: 0.124s, episode steps: 52, steps per second: 420, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.114 [-1.128, 0.744], loss: 5.033267, mean_absolute_error: 12.021411, mean_q: 23.204563\n",
      " 3627/4500: episode: 170, duration: 0.068s, episode steps: 28, steps per second: 409, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.595, 1.184], loss: 9.025471, mean_absolute_error: 12.627999, mean_q: 23.958301\n",
      " 3691/4500: episode: 171, duration: 0.149s, episode steps: 64, steps per second: 429, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.154 [-0.449, 1.236], loss: 5.698699, mean_absolute_error: 13.347220, mean_q: 25.683773\n",
      " 3712/4500: episode: 172, duration: 0.052s, episode steps: 21, steps per second: 407, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.124 [-0.952, 0.609], loss: 13.505416, mean_absolute_error: 13.375476, mean_q: 25.399580\n",
      " 3771/4500: episode: 173, duration: 0.129s, episode steps: 59, steps per second: 456, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.161 [-0.575, 1.286], loss: 6.385696, mean_absolute_error: 13.694912, mean_q: 26.297236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3791/4500: episode: 174, duration: 0.048s, episode steps: 20, steps per second: 420, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.065 [-1.225, 0.622], loss: 13.408300, mean_absolute_error: 13.564146, mean_q: 25.727165\n",
      " 3826/4500: episode: 175, duration: 0.076s, episode steps: 35, steps per second: 462, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.096 [-1.147, 0.629], loss: 7.750299, mean_absolute_error: 12.678521, mean_q: 24.407891\n",
      " 3841/4500: episode: 176, duration: 0.061s, episode steps: 15, steps per second: 247, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.093 [-0.418, 1.007], loss: 18.974102, mean_absolute_error: 13.189904, mean_q: 24.509106\n",
      " 3911/4500: episode: 177, duration: 0.187s, episode steps: 70, steps per second: 374, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.213 [-0.760, 1.158], loss: 4.322142, mean_absolute_error: 12.769568, mean_q: 24.889394\n",
      " 3936/4500: episode: 178, duration: 0.057s, episode steps: 25, steps per second: 437, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.019 [-1.335, 0.801], loss: 10.228596, mean_absolute_error: 13.241332, mean_q: 25.406600\n",
      " 3962/4500: episode: 179, duration: 0.061s, episode steps: 26, steps per second: 424, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.101 [-1.181, 0.760], loss: 10.357280, mean_absolute_error: 12.638757, mean_q: 24.140449\n",
      " 4012/4500: episode: 180, duration: 0.124s, episode steps: 50, steps per second: 404, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.174 [-0.972, 1.393], loss: 6.959508, mean_absolute_error: 13.400056, mean_q: 25.926178\n",
      " 4036/4500: episode: 181, duration: 0.053s, episode steps: 24, steps per second: 456, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.571, 1.055], loss: 13.533474, mean_absolute_error: 13.433471, mean_q: 25.372022\n",
      " 4094/4500: episode: 182, duration: 0.127s, episode steps: 58, steps per second: 456, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-1.047, 0.923], loss: 4.556753, mean_absolute_error: 12.835524, mean_q: 25.069156\n",
      " 4126/4500: episode: 183, duration: 0.075s, episode steps: 32, steps per second: 429, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-0.972, 0.504], loss: 9.523470, mean_absolute_error: 13.418782, mean_q: 25.635619\n",
      " 4260/4500: episode: 184, duration: 0.306s, episode steps: 134, steps per second: 437, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.021 [-1.140, 1.173], loss: 3.365376, mean_absolute_error: 15.415260, mean_q: 30.371083\n",
      " 4297/4500: episode: 185, duration: 0.081s, episode steps: 37, steps per second: 456, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.174 [-1.150, 0.464], loss: 11.863466, mean_absolute_error: 15.118938, mean_q: 29.373314\n",
      " 4338/4500: episode: 186, duration: 0.087s, episode steps: 41, steps per second: 472, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.033 [-1.315, 0.988], loss: 8.815203, mean_absolute_error: 15.210214, mean_q: 29.601268\n",
      " 4352/4500: episode: 187, duration: 0.031s, episode steps: 14, steps per second: 452, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.082 [-1.596, 1.025], loss: 21.756719, mean_absolute_error: 15.044814, mean_q: 28.172160\n",
      " 4372/4500: episode: 188, duration: 0.052s, episode steps: 20, steps per second: 386, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.089 [-1.326, 0.594], loss: 13.188230, mean_absolute_error: 14.512942, mean_q: 27.084277\n",
      " 4390/4500: episode: 189, duration: 0.078s, episode steps: 18, steps per second: 230, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.069 [-1.160, 0.798], loss: 11.010380, mean_absolute_error: 12.749348, mean_q: 24.277851\n",
      " 4406/4500: episode: 190, duration: 0.041s, episode steps: 16, steps per second: 392, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.103 [-0.564, 1.116], loss: 25.516428, mean_absolute_error: 15.082650, mean_q: 28.193342\n",
      " 4458/4500: episode: 191, duration: 0.124s, episode steps: 52, steps per second: 421, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.036 [-0.644, 1.117], loss: 8.069915, mean_absolute_error: 13.847460, mean_q: 26.714055\n",
      " 4483/4500: episode: 192, duration: 0.054s, episode steps: 25, steps per second: 464, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.069 [-0.944, 1.445], loss: 16.388221, mean_absolute_error: 14.238371, mean_q: 27.024520\n",
      " 4498/4500: episode: 193, duration: 0.037s, episode steps: 15, steps per second: 407, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.106 [-0.580, 1.008], loss: 24.318955, mean_absolute_error: 14.220924, mean_q: 26.511375\n",
      "done, took 11.187 seconds\n",
      "TEMPO:  1561719341.822095\n",
      "\n",
      "\n",
      "DQN\n",
      "\n",
      "\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 184.000, steps: 184\n",
      "\n",
      "\\CEM\n",
      "\n",
      "\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 172.000, steps: 172\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 190.000, steps: 190\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "\n",
      "\\SARSA\n",
      "\n",
      "\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 24.000, steps: 24\n",
      "Episode 2: reward: 30.000, steps: 30\n",
      "Episode 3: reward: 23.000, steps: 23\n",
      "Episode 4: reward: 23.000, steps: 23\n",
      "Episode 5: reward: 22.000, steps: 22\n",
      "\n",
      "\\DUEL\n",
      "\n",
      "\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 160.000, steps: 160\n",
      "Episode 2: reward: 130.000, steps: 130\n",
      "Episode 3: reward: 166.000, steps: 166\n",
      "Episode 4: reward: 172.000, steps: 172\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2dc7fa550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import time\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# SARSA does not require a memory.\n",
    "policy = BoltzmannQPolicy()\n",
    "sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\n",
    "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "# Next, we build a very simple model regardless of the dueling architecture\n",
    "# if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically\n",
    "# Also, you can build a dueling network by yourself and turn off the dueling network in DQN.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "# enable the dueling network\n",
    "# you can specify the dueling_type to one of {'avg','max','naive'}\n",
    "duel = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "duel.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"TEMPO: \", time.time())\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "duel.fit(env, nb_steps=2900, visualize=True, verbose=2)\n",
    "\n",
    "print(\"TEMPO: \", time.time())\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=3200, visualize=True, verbose=2)\n",
    "\n",
    "print(\"TEMPO: \", time.time())\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=11000, visualize=True, verbose=2)\n",
    "\n",
    "print(\"TEMPO: \", time.time())\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "sarsa.fit(env, nb_steps=4500, visualize=True, verbose=2)\n",
    "print(\"TEMPO: \", time.time())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "\n",
    "print(\"\\n\\nDQN\\n\\n\")\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "print(\"\\n\\CEM\\n\\n\")\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)\n",
    "\n",
    "\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "print(\"\\n\\SARSA\\n\\n\")\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "sarsa.test(env, nb_episodes=5, visualize=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "duel.save_weights('duel_dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "print(\"\\n\\DUEL\\n\\n\")\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "duel.test(env, nb_episodes=5, visualize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
